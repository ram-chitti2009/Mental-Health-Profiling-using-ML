{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dca6c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing kaggle and opendatasets...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cgi'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopendatasets\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\venka\\Desktop\\mental-health-profiling\\venv\\Lib\\site-packages\\opendatasets\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopendatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnetwork\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_url, is_url\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopendatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgoogledrive\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_google_drive_url, download_google_drive\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\venka\\Desktop\\mental-health-profiling\\venv\\Lib\\site-packages\\opendatasets\\utils\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopendatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnetwork\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_url\n\u001b[32m      3\u001b[39m GITHUB_RAW_BASE_URL = \u001b[33m'\u001b[39m\u001b[33mhttps://raw.githubusercontent.com/JovianML/opendatasets/master/data/\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\venka\\Desktop\\mental-health-profiling\\venv\\Lib\\site-packages\\opendatasets\\utils\\network.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcgi\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cgi'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkaggle\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mopendatasets\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopendatasets\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\venka\\Desktop\\mental-health-profiling\\venv\\Lib\\site-packages\\opendatasets\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopendatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnetwork\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_url, is_url\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopendatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgoogledrive\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_google_drive_url, download_google_drive\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\venka\\Desktop\\mental-health-profiling\\venv\\Lib\\site-packages\\opendatasets\\utils\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopendatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnetwork\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_url\n\u001b[32m      3\u001b[39m GITHUB_RAW_BASE_URL = \u001b[33m'\u001b[39m\u001b[33mhttps://raw.githubusercontent.com/JovianML/opendatasets/master/data/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_raw_files\u001b[39m(dataset_id, data_dir, dry_run, files):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\venka\\Desktop\\mental-health-profiling\\venv\\Lib\\site-packages\\opendatasets\\utils\\network.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcgi\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cgi'"
     ]
    }
   ],
   "source": [
    "# Install required packages for working with Kaggle datasets\n",
    "# Note: Run this in terminal: pip install kaggle opendatasets\n",
    "# Or uncomment below to install programmatically:\n",
    "try:\n",
    "    import kaggle\n",
    "    import opendatasets\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    print(\"Installing kaggle and opendatasets...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kaggle\", \"opendatasets\"])\n",
    "    import kaggle\n",
    "    import opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b9658a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/thedevastator/medical-student-mental-health\n",
      "Dataset downloaded successfully!\n",
      "Dataset downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import os \n",
    "kaggle.api.dataset_download_files('osmi/mental-health-in-tech-survey', \n",
    "                                  path='./data', \n",
    "                                  unzip=True)\n",
    "\n",
    "print(\"Dataset downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5e2895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the data directory:\n",
      "- ./data\\D1-Swiss.csv\n",
      "- ./data\\D1_Swiss_processed.csv\n",
      "- ./data\\D2-Cultural.csv\n",
      "- ./data\\D3-Academic.csv\n",
      "- ./data\\D4-Tech.csv\n",
      "\n",
      "Trying to load: ./data\\D1-Swiss.csv\n",
      "Successfully loaded: ./data\\D1-Swiss.csv\n",
      "Shape: (886, 20)\n",
      "Columns: ['id', 'age', 'year', 'sex', 'glang', 'part', 'job', 'stud_h', 'health', 'psyt', 'jspe', 'qcae_cog', 'qcae_aff', 'amsp', 'erec_mean', 'cesd', 'stai_t', 'mbi_ex', 'mbi_cy', 'mbi_ea']\n",
      "\n",
      "Trying to load: ./data\\D1_Swiss_processed.csv\n",
      "Successfully loaded: ./data\\D1_Swiss_processed.csv\n",
      "Shape: (886, 56)\n",
      "Columns: ['id', 'age', 'year', 'sex', 'glang', 'part', 'job', 'stud_h', 'health', 'PSYT_Therapy_Use', 'jspe', 'qcae_cog', 'qcae_aff', 'amsp', 'erec_mean', 'Depression', 'Anxiety', 'Burnout', 'Stress', 'mbi_ea', 'Source_Group', 'Gender', 'Age', 'City', 'Profession', 'Work Pressure', 'CGPA', 'Job Satisfaction', 'Sleep Duration', 'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?', 'Work/Study Hours', 'Family History of Mental Illness', 'Timestamp', 'Country', 'state', 'self_employed', 'family_history', 'H3_Tech_Validation', 'no_employees', 'remote_work', 'tech_company', 'benefits', 'care_options', 'wellness_program', 'seek_help', 'anonymity', 'phys_health_consequence', 'coworkers', 'supervisor', 'mental_health_interview', 'phys_health_interview', 'mental_vs_physical', 'obs_consequence', 'comments']\n",
      "\n",
      "Trying to load: ./data\\D2-Cultural.csv\n",
      "Successfully loaded: ./data\\D2-Cultural.csv\n",
      "Shape: (101, 11)\n",
      "Columns: ['Timestamp', 'Choose your gender', 'Age', 'What is your course?', 'Your current year of Study', 'What is your CGPA?', 'Marital status', 'Do you have Depression?', 'Do you have Anxiety?', 'Do you have Panic attack?', 'Did you seek any specialist for a treatment?']\n",
      "\n",
      "Trying to load: ./data\\D3-Academic.csv\n",
      "Successfully loaded: ./data\\D3-Academic.csv\n",
      "Shape: (27901, 18)\n",
      "Columns: ['id', 'Gender', 'Age', 'City', 'Profession', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', 'Job Satisfaction', 'Sleep Duration', 'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?', 'Work/Study Hours', 'Financial Stress', 'Family History of Mental Illness', 'Depression']\n",
      "\n",
      "Trying to load: ./data\\D4-Tech.csv\n",
      "Successfully loaded: ./data\\D4-Tech.csv\n",
      "Shape: (1259, 27)\n",
      "Columns: ['Timestamp', 'Age', 'Gender', 'Country', 'state', 'self_employed', 'family_history', 'treatment', 'work_interfere', 'no_employees', 'remote_work', 'tech_company', 'benefits', 'care_options', 'wellness_program', 'seek_help', 'anonymity', 'leave', 'mental_health_consequence', 'phys_health_consequence', 'coworkers', 'supervisor', 'mental_health_interview', 'phys_health_interview', 'mental_vs_physical', 'obs_consequence', 'comments']\n",
      "\n",
      "First 5 rows:\n",
      "             Timestamp  Age  Gender         Country state self_employed  \\\n",
      "0  2014-08-27 11:29:31   37  Female   United States    IL           NaN   \n",
      "1  2014-08-27 11:29:37   44       M   United States    IN           NaN   \n",
      "2  2014-08-27 11:29:44   32    Male          Canada   NaN           NaN   \n",
      "3  2014-08-27 11:29:46   31    Male  United Kingdom   NaN           NaN   \n",
      "4  2014-08-27 11:30:22   31    Male   United States    TX           NaN   \n",
      "\n",
      "  family_history treatment work_interfere    no_employees  ...  \\\n",
      "0             No       Yes          Often            6-25  ...   \n",
      "1             No        No         Rarely  More than 1000  ...   \n",
      "2             No        No         Rarely            6-25  ...   \n",
      "3            Yes       Yes          Often          26-100  ...   \n",
      "4             No        No          Never         100-500  ...   \n",
      "\n",
      "                leave mental_health_consequence phys_health_consequence  \\\n",
      "0       Somewhat easy                        No                      No   \n",
      "1          Don't know                     Maybe                      No   \n",
      "2  Somewhat difficult                        No                      No   \n",
      "3  Somewhat difficult                       Yes                     Yes   \n",
      "4          Don't know                        No                      No   \n",
      "\n",
      "      coworkers supervisor mental_health_interview phys_health_interview  \\\n",
      "0  Some of them        Yes                      No                 Maybe   \n",
      "1            No         No                      No                    No   \n",
      "2           Yes        Yes                     Yes                   Yes   \n",
      "3  Some of them         No                   Maybe                 Maybe   \n",
      "4  Some of them        Yes                     Yes                   Yes   \n",
      "\n",
      "  mental_vs_physical obs_consequence comments  \n",
      "0                Yes              No      NaN  \n",
      "1         Don't know              No      NaN  \n",
      "2                 No              No      NaN  \n",
      "3                 No             Yes      NaN  \n",
      "4         Don't know              No      NaN  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os \n",
    "\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "data_files = glob.glob('./data/*')\n",
    "print(\"Files in the data directory:\")\n",
    "for file in data_files:\n",
    "    print(f\"- {file}\")\n",
    "\n",
    "df = None\n",
    "for file in data_files:\n",
    "    if file.endswith('.csv'):\n",
    "        try:\n",
    "            print(f\"\\nTrying to load: {file}\")\n",
    "            \n",
    "            if 'codebook' in file.lower():\n",
    "                print(\"Skipping codebook file\")\n",
    "                continue\n",
    "            elif 'data' in file.lower():\n",
    "                df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')\n",
    "                print(f\"Successfully loaded: {file}\")\n",
    "                print(f\"Shape: {df.shape}\")\n",
    "                print(f\"Columns: {df.columns.tolist()}\")\n",
    "                \n",
    "            else:\n",
    "                df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')\n",
    "                print(f\"Successfully loaded: {file}\")\n",
    "                print(f\"Shape: {df.shape}\")\n",
    "                print(f\"Columns: {df.columns.tolist()}\")\n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"No dataset could be loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d4532a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing D1-Swiss\n",
      "============================================================\n",
      "Loaded D1-Swiss dataset with shape: (886, 20)\n",
      "\n",
      "Outlier Detection for D1-Swiss...\n",
      "  Depression: Found 8 outlier(s)\n",
      "    Bounds: -15.00 to 49.00\n",
      "    Value 50: 1 row(s)\n",
      "    Value 51: 2 row(s)\n",
      "    Value 52: 1 row(s)\n",
      "    Value 53: 1 row(s)\n",
      "    Value 54: 2 row(s)\n",
      "    Value 56: 1 row(s)\n",
      "    → Removing 8 outlier row(s)\n",
      "  Anxiety: Found 4 outlier(s)\n",
      "    Bounds: 8.88 to 75.88\n",
      "    Value 76: 3 row(s)\n",
      "    Value 77: 1 row(s)\n",
      "    → Removing 4 outlier row(s)\n",
      "  Stress: Found 3 outlier(s)\n",
      "    Bounds: -4.50 to 23.50\n",
      "    Value 24: 3 row(s)\n",
      "    → Removing 3 outlier row(s)\n",
      "\n",
      "Data Cleaning: Handling missing values for D1-Swiss\n",
      "\n",
      "Normalizing D1-Swiss features using Z-score normalization (per-dataset)\n",
      "  Scaler saved as 'D1_Swiss_scaler.joblib'\n",
      "  Normalized feature statistics:\n",
      "    Depression: mean=0.000000, std=1.000575\n",
      "    Anxiety: mean=-0.000000, std=1.000575\n",
      "    Burnout: mean=0.000000, std=1.000575\n",
      "    Stress: mean=-0.000000, std=1.000575\n",
      "   D1-Swiss processed dataset saved as 'D1_Swiss_processed.csv'\n",
      "\n",
      "============================================================\n",
      "Processing D2-Cultural\n",
      "============================================================\n",
      "Loaded D2-Cultural dataset with shape: (101, 11)\n",
      "Converting categorical column Depression to numerical in D2-Cultural\n",
      "Converting categorical column Anxiety to numerical in D2-Cultural\n",
      "Converting categorical column Burnout to numerical in D2-Cultural\n",
      "Converting categorical column Stress to numerical in D2-Cultural\n",
      "\n",
      "Outlier Detection for D2-Cultural...\n",
      "\n",
      "Data Cleaning: Handling missing values for D2-Cultural\n",
      "\n",
      "Normalizing D2-Cultural features using Z-score normalization (per-dataset)\n",
      "  Scaler saved as 'D2_Cultural_scaler.joblib'\n",
      "  Normalized feature statistics:\n",
      "    Depression: mean=0.000000, std=1.004988\n",
      "    Anxiety: mean=0.000000, std=1.004988\n",
      "    Burnout: mean=-0.000000, std=1.004988\n",
      "    Stress: mean=0.000000, std=1.004988\n",
      "   D2-Cultural processed dataset saved as 'D2_Cultural_processed.csv'\n",
      "\n",
      "============================================================\n",
      "Processing D3-Academic\n",
      "============================================================\n",
      "Loaded D3-Academic dataset with shape: (27901, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venka\\AppData\\Local\\Temp\\ipykernel_35980\\360032249.py:112: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_selected[col] = df_selected[col].astype(str).str.lower().replace(\n",
      "C:\\Users\\venka\\AppData\\Local\\Temp\\ipykernel_35980\\360032249.py:112: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_selected[col] = df_selected[col].astype(str).str.lower().replace(\n",
      "C:\\Users\\venka\\AppData\\Local\\Temp\\ipykernel_35980\\360032249.py:112: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_selected[col] = df_selected[col].astype(str).str.lower().replace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting categorical column Stress to numerical in D3-Academic\n",
      "\n",
      "Outlier Detection for D3-Academic...\n",
      "\n",
      "Data Cleaning: Handling missing values for D3-Academic\n",
      "  Stress: Filling 3 missing values with mean=3.1399\n",
      "\n",
      "Normalizing D3-Academic features using Z-score normalization (per-dataset)\n",
      "  Scaler saved as 'D3_Academic_scaler.joblib'\n",
      "  Normalized feature statistics:\n",
      "    Depression: mean=0.000000, std=1.000018\n",
      "    Anxiety: mean=0.000000, std=1.000018\n",
      "    Burnout: mean=0.000000, std=1.000018\n",
      "    Stress: mean=-0.000000, std=1.000018\n",
      "   D3-Academic processed dataset saved as 'D3_Academic_processed.csv'\n",
      "\n",
      "============================================================\n",
      "Processing D4-Tech\n",
      "============================================================\n",
      "Loaded D4-Tech dataset with shape: (1259, 27)\n",
      "Converting categorical column Depression to numerical in D4-Tech\n",
      "Converting categorical column Anxiety to numerical in D4-Tech\n",
      "Converting categorical column Burnout to numerical in D4-Tech\n",
      "\n",
      "Outlier Detection for D4-Tech...\n",
      "  Stress: Found 40 outlier(s)\n",
      "    Bounds: 18.00 to 49.50\n",
      "    Value -1726: 1 row(s)\n",
      "    Value -29: 1 row(s)\n",
      "    Value -1: 1 row(s)\n",
      "    Value 5: 1 row(s)\n",
      "    Value 8: 1 row(s)\n",
      "    Value 11: 1 row(s)\n",
      "    Value 50: 6 row(s)\n",
      "    Value 51: 5 row(s)\n",
      "    Value 53: 1 row(s)\n",
      "    Value 54: 3 row(s)\n",
      "    Value 55: 3 row(s)\n",
      "    Value 56: 4 row(s)\n",
      "    Value 57: 3 row(s)\n",
      "    Value 58: 1 row(s)\n",
      "    Value 60: 2 row(s)\n",
      "    Value 61: 1 row(s)\n",
      "    Value 62: 1 row(s)\n",
      "    Value 65: 1 row(s)\n",
      "    Value 72: 1 row(s)\n",
      "    Value 329: 1 row(s)\n",
      "    Value 99999999999: 1 row(s)\n",
      "    → Removing 40 outlier row(s)\n",
      "\n",
      "Data Cleaning: Handling missing values for D4-Tech\n",
      "  Anxiety: Filling 257 missing values with mean=0.3805\n",
      "  Burnout: Filling 545 missing values with mean=0.4243\n",
      "\n",
      "Normalizing D4-Tech features using Z-score normalization (per-dataset)\n",
      "  Scaler saved as 'D4_Tech_scaler.joblib'\n",
      "  Normalized feature statistics:\n",
      "    Depression: mean=0.000000, std=1.000410\n",
      "    Anxiety: mean=-0.000000, std=1.000410\n",
      "    Burnout: mean=0.000000, std=1.000410\n",
      "    Stress: mean=0.000000, std=1.000410\n",
      "   D4-Tech processed dataset saved as 'D4_Tech_processed.csv'\n",
      "\n",
      "============================================================\n",
      "Combining all processed datasets\n",
      "============================================================\n",
      "Combined dataset shape: (30092, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venka\\AppData\\Local\\Temp\\ipykernel_35980\\360032249.py:112: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_selected[col] = df_selected[col].astype(str).str.lower().replace(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Fused dataset saved as 'fused_mental_health_dataset.csv'\n",
      "\n",
      "============================================================\n",
      "Data processing complete!\n",
      "============================================================\n",
      "\n",
      "Processed datasets:\n",
      "  ✓ D1-Swiss: D1_Swiss_processed.csv\n",
      "  ✓ D2-Cultural: D2_Cultural_processed.csv\n",
      "  ✓ D3-Academic: D3_Academic_processed.csv\n",
      "  ✓ D4-Tech: D4_Tech_processed.csv\n",
      "\n",
      "Note: Each dataset was normalized separately to preserve feature distributions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import joblib\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DATA_DIR = Path(\"./data\")\n",
    "\n",
    "# Fixed file paths to match actual filenames in data directory\n",
    "FILE_PATHS = {\n",
    "    \"D1-Swiss\": \"D1-Swiss.csv\",\n",
    "    \"D2-Cultural\": \"D2-Cultural.csv\",\n",
    "    \"D3-Academic\": \"D3-Academic.csv\",\n",
    "    \"D4-Tech\": \"D4-Tech.csv\",\n",
    "}\n",
    "\n",
    "OUTPUT_PATHS = {\n",
    "    \"D1-Swiss\": \"D1_Swiss_processed.csv\",\n",
    "    \"D2-Cultural\": \"D2_Cultural_processed.csv\",\n",
    "    \"D3-Academic\": \"D3_Academic_processed.csv\",\n",
    "    \"D4-Tech\": \"D4_Tech_processed.csv\",\n",
    "}\n",
    "\n",
    "COLUMN_MAPPING = {\n",
    "    \"D1-Swiss\": {\n",
    "        \"cesd\": \"Depression\",\n",
    "        \"stai_t\": \"Anxiety\",\n",
    "        \"mbi_ex\": \"Burnout\",\n",
    "        \"mbi_cy\": \"Stress\",\n",
    "        \"psyt\": \"PSYT_Therapy_Use\",\n",
    "    },\n",
    "    \"D2-Cultural\": {\n",
    "        \"Do you have Depression?\": \"Depression\",\n",
    "        \"Do you have Anxiety?\": \"Anxiety\",\n",
    "        \"Do you have Panic attack?\": \"Burnout\",\n",
    "        \"Your current year of Study\": \"Stress\",\n",
    "    },\n",
    "    \"D3-Academic\": {\n",
    "        \"Depression\": \"Depression\",\n",
    "        \"Academic Pressure\": \"Anxiety\",\n",
    "        \"Study Satisfaction\": \"Burnout\",\n",
    "        \"Financial Stress\": \"Stress\",\n",
    "    },\n",
    "    \"D4-Tech\": {\n",
    "        \"mental_health_consequence\": \"Depression\",\n",
    "        \"work_interfere\": \"Anxiety\",\n",
    "        \"leave\": \"Burnout\",\n",
    "        \"Age\": \"Stress\",\n",
    "        \"treatment\": \"H3_Tech_Validation\",\n",
    "    },\n",
    "}\n",
    "\n",
    "UNIVERSAL_FEATURES = [\"Depression\", \"Anxiety\", \"Burnout\", \"Stress\"]\n",
    "\n",
    "# Process each dataset separately with individual normalization\n",
    "processed_datasets = {}\n",
    "missing_sources = []\n",
    "\n",
    "for source_name, file_name in FILE_PATHS.items():\n",
    "    file_path = DATA_DIR / file_name\n",
    "\n",
    "    if not file_path.exists():\n",
    "        print(f\"File not found for {source_name}: {file_path}\")\n",
    "        missing_sources.append(source_name)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing {source_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        df_raw = pd.read_csv(file_path, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "        print(f\"Loaded {source_name} dataset with shape: {df_raw.shape}\")\n",
    "\n",
    "        current_mapping = COLUMN_MAPPING[source_name]\n",
    "        missing_cols = [src for src in current_mapping if src not in df_raw.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"ERROR: Missing crucial columns in {file_name}: {missing_cols}\")\n",
    "            missing_sources.append(source_name)\n",
    "            continue\n",
    "\n",
    "        df_named = df_raw.rename(columns=current_mapping)\n",
    "\n",
    "        selected_columns = UNIVERSAL_FEATURES.copy()\n",
    "        if \"PSYT_Therapy_Use\" in current_mapping.values():\n",
    "            selected_columns.append(\"PSYT_Therapy_Use\")\n",
    "        if \"H3_Tech_Validation\" in current_mapping.values():\n",
    "            selected_columns.append(\"H3_Tech_Validation\")\n",
    "\n",
    "        for col in selected_columns:\n",
    "            if col not in df_named.columns:\n",
    "                df_named[col] = np.nan\n",
    "\n",
    "        df_selected = df_named[selected_columns].copy()\n",
    "\n",
    "        # Convert categorical columns to numerical\n",
    "        for col in UNIVERSAL_FEATURES:\n",
    "            if df_selected[col].dtype == \"object\":\n",
    "                print(f\"Converting categorical column {col} to numerical in {source_name}\")\n",
    "                \n",
    "                # First, try to extract year numbers from strings like \"year 1\", \"Year 1\", etc.\n",
    "                if col == \"Stress\" and source_name == \"D2-Cultural\":\n",
    "                    # Extract year number from strings like \"year 1\", \"Year 1\", \"year 2\", etc.\n",
    "                    # Use regex to extract the number after \"year\"\n",
    "                    df_selected[col] = df_selected[col].apply(\n",
    "                        lambda x: re.search(r'year\\s*(\\d+)', str(x).lower())\n",
    "                    ).apply(lambda m: int(m.group(1)) if m else np.nan)\n",
    "                else:\n",
    "                    # Standard categorical conversion\n",
    "                    df_selected[col] = df_selected[col].astype(str).str.lower().replace(\n",
    "                        {\n",
    "                            \"yes\": 1,\n",
    "                            \"no\": 0,\n",
    "                            \"often\": 1,\n",
    "                            \"rarely\": 0,\n",
    "                            \"sometimes\": 0.5,\n",
    "                            \"maybe\": 0.5,\n",
    "                            \"most of the time\": 1,\n",
    "                            \"never\": 0,\n",
    "                            \"always\": 1,\n",
    "                            \"not sure\": 0.5,\n",
    "                            \"high\": 1,\n",
    "                            \"low\": 0,\n",
    "                            \"medium\": 0.5,\n",
    "                            \"somewhat easy\": 0.5,\n",
    "                            \"somewhat difficult\": 0.5,\n",
    "                            \"very difficult\": 1,\n",
    "                            \"very easy\": 0,\n",
    "                            # Year mappings (for D2-Cultural Stress column)\n",
    "                            \"year 1\": 1,\n",
    "                            \"year 2\": 2,\n",
    "                            \"year 3\": 3,\n",
    "                            \"year 4\": 4,\n",
    "                        }\n",
    "                    )\n",
    "                    df_selected[col] = pd.to_numeric(df_selected[col], errors=\"coerce\")\n",
    "\n",
    "        # OUTLIER DETECTION AND REMOVAL (IQR-based, per-dataset, per-feature)\n",
    "        print(f\"\\nOutlier Detection for {source_name}...\")\n",
    "\n",
    "        for feature in UNIVERSAL_FEATURES:\n",
    "            # Work on a numeric copy of the column\n",
    "            values = pd.to_numeric(df_selected[feature], errors=\"coerce\")\n",
    "\n",
    "            # Skip if all NaN or constant\n",
    "            if values.nunique(dropna=True) < 2:\n",
    "                continue\n",
    "\n",
    "            Q1 = values.quantile(0.25)\n",
    "            Q3 = values.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            if IQR == 0:\n",
    "                continue\n",
    "\n",
    "            # IQR bounds\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Domain-specific bounds for D4-Tech Age→Stress\n",
    "            if source_name == \"D4-Tech\" and feature == \"Stress\":\n",
    "                # Reasonable working range for tech workers\n",
    "                lower_bound = max(lower_bound, 18)\n",
    "                upper_bound = min(upper_bound, 80)\n",
    "\n",
    "            outliers = (values < lower_bound) | (values > upper_bound)\n",
    "            n_outliers = outliers.sum()\n",
    "\n",
    "            if n_outliers > 0:\n",
    "                print(f\"  {feature}: Found {n_outliers} outlier(s)\")\n",
    "                print(f\"    Bounds: {lower_bound:.2f} to {upper_bound:.2f}\")\n",
    "                unique_outliers = values[outliers].unique()\n",
    "                for val in sorted(unique_outliers):\n",
    "                    count_val = (values == val).sum()\n",
    "                    print(f\"    Value {val}: {count_val} row(s)\")\n",
    "                print(f\"    → Removing {n_outliers} outlier row(s)\")\n",
    "\n",
    "                # Apply mask back to df_selected\n",
    "                df_selected = df_selected[~outliers].reset_index(drop=True)\n",
    "\n",
    "        # Handle missing values (fill with mean of THIS dataset)\n",
    "        print(f\"\\nData Cleaning: Handling missing values for {source_name}\")\n",
    "        for feature in UNIVERSAL_FEATURES:\n",
    "            mean_value = df_selected[feature].mean(skipna=True)\n",
    "            missing_count = df_selected[feature].isna().sum()\n",
    "            if missing_count > 0:\n",
    "                print(f\"  {feature}: Filling {missing_count} missing values with mean={mean_value:.4f}\")\n",
    "            df_selected[feature] = df_selected[feature].fillna(mean_value)\n",
    "\n",
    "        # NORMALIZE EACH DATASET SEPARATELY (this will ensure the prevention of the Stress collapse issue)\n",
    "        print(f\"\\nNormalizing {source_name} features using Z-score normalization (per-dataset)\")\n",
    "        scaler = StandardScaler()\n",
    "        df_selected[UNIVERSAL_FEATURES] = scaler.fit_transform(df_selected[UNIVERSAL_FEATURES])\n",
    "        \n",
    "        # Save scaler for this dataset\n",
    "        scaler_path = f\"{source_name.replace('-', '_')}_scaler.joblib\"\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        print(f\"  Scaler saved as '{scaler_path}'\")\n",
    "        \n",
    "        # Print normalization stats\n",
    "        print(f\"  Normalized feature statistics:\")\n",
    "        for feat in UNIVERSAL_FEATURES:\n",
    "            mean_val = df_selected[feat].mean()\n",
    "            std_val = df_selected[feat].std()\n",
    "            print(f\"    {feat}: mean={mean_val:.6f}, std={std_val:.6f}\")\n",
    "        \n",
    "        df_selected[\"Source_Group\"] = source_name\n",
    "        processed_datasets[source_name] = df_selected\n",
    "        \n",
    "        # Save individual processed dataset\n",
    "        output_file = OUTPUT_PATHS[source_name]\n",
    "        df_selected.to_csv(output_file, index=False)\n",
    "        print(f\"   {source_name} processed dataset saved as '{output_file}'\")\n",
    "\n",
    "    except Exception as exc:\n",
    "        print(f\"Error loading {source_name} dataset: {exc}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        missing_sources.append(source_name)\n",
    "\n",
    "if not processed_datasets:\n",
    "    print(\"\\nNo datasets were processed. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if missing_sources:\n",
    "    print(\"\\n⚠ Datasets with missing files or errors:\")\n",
    "    for name in missing_sources:\n",
    "        print(f\"  - {name}\")\n",
    "\n",
    "# Combine all processed datasets (already normalized separately)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Combining all processed datasets\")\n",
    "print(f\"{'='*60}\")\n",
    "df_combined = pd.concat(list(processed_datasets.values()), ignore_index=True)\n",
    "print(f\"Combined dataset shape: {df_combined.shape}\")\n",
    "\n",
    "# Save fused dataset\n",
    "fused_output = \"fused_mental_health_dataset.csv\"\n",
    "df_combined.to_csv(fused_output, index=False)\n",
    "print(f\"\\n✓ Fused dataset saved as '{fused_output}'\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Data processing complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nProcessed datasets:\")\n",
    "for source_name in processed_datasets.keys():\n",
    "    print(f\"  ✓ {source_name}: {OUTPUT_PATHS[source_name]}\")\n",
    "print(f\"\\nNote: Each dataset was normalized separately to preserve feature distributions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a224ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
