{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 0: SETUP AND CONFIGURATION\n",
    "# ============================================================================\n",
    "# Purpose: Initialize environment, set reproducibility seeds, define constants\n",
    "# Key components:\n",
    "#   - Library imports\n",
    "#   - Random seed configuration for reproducibility\n",
    "#   - Dataset paths and feature definitions\n",
    "#   - Device selection (GPU/CPU)\n",
    "#   - Data preparation function\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from scipy.stats import chi2, chi2_contingency, pearsonr\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "FEATURE_COLUMNS = [\"Depression\", \"Anxiety\", \"Stress\", \"Burnout\"]\n",
    "DATASETS = {\n",
    "    \"D1-Swiss\": Path(\"D1_Swiss_processed.csv\"),\n",
    "    \"D2-Cultural\": Path(\"D2_Cultural_processed.csv\"),\n",
    "    \"D3-Academic\": Path(\"D3_Academic_processed.csv\"),\n",
    "    \"D4-Tech\": Path(\"D4_Tech_processed.csv\"),\n",
    "}\n",
    "PIPELINE_RESULTS = {}\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "def prepare_dataset(dataset_name: str):\n",
    "    if dataset_name not in DATASETS:\n",
    "        raise ValueError(f\"Unknown dataset '{dataset_name}'. Available: {list(DATASETS.keys())}\")\n",
    "\n",
    "    dataset_path = DATASETS[dataset_name]\n",
    "    print(f\"\\n=== Loading {dataset_name} dataset ===\")\n",
    "    print(f\"File: {dataset_path}\")\n",
    "    print(f\"Features: {FEATURE_COLUMNS}\")\n",
    "\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    feature_matrix = df[FEATURE_COLUMNS].values\n",
    "    print(f\"Dataset: {feature_matrix.shape[0]} samples, {feature_matrix.shape[1]} features\")\n",
    "\n",
    "    train_val_data, test_data = train_test_split(feature_matrix, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "    train_val_tensor = torch.tensor(train_val_data, dtype=torch.float32)\n",
    "    test_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    print(f\"Train+Val: {train_val_tensor.shape[0]} samples (80%)\")\n",
    "    print(f\"Test: {test_tensor.shape[0]} samples (20%)\")\n",
    "    print(f\"K-Fold: 10 folds, ~{train_val_tensor.shape[0]//10} samples per fold\\n\")\n",
    "\n",
    "    return (\n",
    "        df,\n",
    "        feature_matrix,\n",
    "        train_val_data,\n",
    "        test_data,\n",
    "        train_val_tensor,\n",
    "        test_tensor,\n",
    "        kfold,\n",
    "        dataset_path,\n",
    "    )\n",
    "\n",
    "INPUT_DIM = len(FEATURE_COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: AUTOENCODER ARCHITECTURE DEFINITION\n",
    "# ============================================================================\n",
    "# Purpose: Define the neural network that compresses 4D symptoms into\n",
    "#          lower-dimensional latent space for clustering\n",
    "# Architecture: Symmetric encoder-decoder with configurable dimensions\n",
    "# ============================================================================\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, activation_function):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        #Encoder - input -> hidden -> latent\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_function(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "\n",
    "        #Decoder - latent -> hidden -> output\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            activation_function(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: MAIN PIPELINE - Hyperparameter Tuning and Profile Extraction\n",
    "# ============================================================================\n",
    "# Purpose: Two-stage hyperparameter optimization followed by profile extraction\n",
    "# Stage 1: Architecture tuning (hidden_size, latent_dim, activation, optimizer)\n",
    "# Stage 2: Learning parameter tuning (batch_size, weight_decay, momentum)\n",
    "# Final: Train on all data, extract profiles, evaluate on test set\n",
    "# ============================================================================\n",
    "\n",
    "def run_autoencoder_pipeline(dataset_name: str, force_latent_dim: int = None, force_k: int = None):\n",
    "    (\n",
    "        all_data_df,\n",
    "        all_data,\n",
    "        train_val_data,\n",
    "        test_data,\n",
    "        train_val_tensor,\n",
    "        test_tensor,\n",
    "        kfold,\n",
    "        dataset_path,\n",
    "    ) = prepare_dataset(dataset_name)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # STAGE 1: Architecture Parameter Tuning\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Grid search over architecture parameters with 10-fold cross-validation\n",
    "    # Selection criterion: Consensus voting on clustering quality metrics\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"STAGE 1: Architecture Parameters Tuning (K-Fold CV)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    hidden_sizes = [3, 4, 5, 6, 8, 10]\n",
    "    latent_dims = [force_latent_dim] if force_latent_dim is not None else [2, 3]\n",
    "\n",
    "    activations = {\n",
    "        'ReLU': nn.ReLU,\n",
    "        'Tanh': nn.Tanh,\n",
    "        'Sigmoid': nn.Sigmoid\n",
    "    }\n",
    "\n",
    "    optimizers = {\n",
    "        'Adam': optim.Adam,\n",
    "        'SGD': optim.SGD\n",
    "    }\n",
    "\n",
    "    epochs_list = [20, 50, 100]\n",
    "    fixed_lr = 1e-3\n",
    "\n",
    "    n_folds = 10\n",
    "    criterion = nn.MSELoss()\n",
    "    results_stage1 = defaultdict(list)\n",
    "\n",
    "    total_experiments = len(hidden_sizes) * len(latent_dims) * len(activations) * len(optimizers) * len(epochs_list) * n_folds\n",
    "    experiment_count = 0\n",
    "\n",
    "    print(f\"Testing: hidden_size, latent_dim, activation, optimizer, epochs\")\n",
    "    print(f\"Fixed: learning_rate = {fixed_lr}\")\n",
    "    print(f\"\\nGrid sizes:\")\n",
    "    print(f\"  hidden_size: {len(hidden_sizes)} values {hidden_sizes}\")\n",
    "    print(f\"  latent_dim: {len(latent_dims)} values {latent_dims}\")\n",
    "    print(f\"  activation: {len(activations)} values {list(activations.keys())}\")\n",
    "    print(f\"  optimizer: {len(optimizers)} values {list(optimizers.keys())}\")\n",
    "    print(f\"  epochs: {len(epochs_list)} values {epochs_list}\")\n",
    "    print(f\"  CV folds: {n_folds}\")\n",
    "    print(f\"Total: {total_experiments} experiments ({total_experiments//n_folds} configs × {n_folds} folds)\\n\")\n",
    "\n",
    "    start_stage1 = time.time()\n",
    "\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for latent_dim in latent_dims:\n",
    "            for act_name, act_fn in activations.items():\n",
    "                for opt_name, opt_class in optimizers.items():\n",
    "                    for num_epochs in epochs_list:\n",
    "                        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_val_tensor)):\n",
    "                            experiment_count += 1\n",
    "                            if experiment_count % 50 == 0 or experiment_count == total_experiments:\n",
    "                                elapsed = time.time() - start_stage1\n",
    "                                print(f\"  [{experiment_count}/{total_experiments}] {100*experiment_count/total_experiments:.1f}% - {elapsed/60:.1f}min\")\n",
    "                        \n",
    "                            fold_seed = 42 + fold_idx\n",
    "                            torch.manual_seed(fold_seed)\n",
    "                            np.random.seed(fold_seed)\n",
    "                        \n",
    "                            train_fold = train_val_tensor[train_idx]\n",
    "                            val_fold = train_val_tensor[val_idx]\n",
    "                        \n",
    "                            train_dataset = TensorDataset(train_fold.cpu())\n",
    "                            val_dataset = TensorDataset(val_fold.cpu())\n",
    "                            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "                            val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "                        \n",
    "                            model = Autoencoder(INPUT_DIM, hidden_size, latent_dim, act_fn).to(device)\n",
    "                        \n",
    "                            if opt_name == 'Adam':\n",
    "                                optimizer = opt_class(model.parameters(), lr=fixed_lr)\n",
    "                            else:\n",
    "                                optimizer = opt_class(model.parameters(), lr=fixed_lr, momentum=0.9)\n",
    "                        \n",
    "                            train_losses, val_losses, optimal_k, best_sil_score, latent_vectors, validation_metrics = \\\n",
    "                                train_and_validate_model(model, train_loader, val_loader, optimizer, \n",
    "                                                        criterion, num_epochs, device)\n",
    "                        \n",
    "                            optimal_k_idx = validation_metrics['k_values'].index(optimal_k)\n",
    "                            best_ch_score = validation_metrics['calinski_harabasz_scores'][optimal_k_idx]\n",
    "                            best_db_score = validation_metrics['davies_bouldin_scores'][optimal_k_idx]\n",
    "                        \n",
    "                            results_stage1['hidden_size'].append(hidden_size)\n",
    "                            results_stage1['latent_dim'].append(latent_dim)\n",
    "                            results_stage1['activation'].append(act_name)\n",
    "                            results_stage1['optimizer'].append(opt_name)\n",
    "                            results_stage1['epochs'].append(num_epochs)\n",
    "                            results_stage1['fold'].append(fold_idx)\n",
    "                            results_stage1['optimal_k'].append(optimal_k)\n",
    "                            results_stage1['silhouette_score'].append(best_sil_score)\n",
    "                            results_stage1['calinski_harabasz_score'].append(best_ch_score)\n",
    "                            results_stage1['davies_bouldin_score'].append(best_db_score)\n",
    "                            results_stage1['reconstruction_loss'].append(val_losses[-1])\n",
    "                            results_stage1['consensus_reached'].append(validation_metrics['consensus_reached'])\n",
    "\n",
    "    print(f\"\\n✓ Stage 1 completed in {(time.time()-start_stage1)/60:.2f} minutes\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # STAGE 1: Results Aggregation and Best Configuration Selection\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Aggregate results across folds, rank by clustering metrics,\n",
    "    # use consensus voting to select best architecture\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"STAGE 1: Results Aggregation\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    stage1_df = pd.DataFrame(results_stage1)\n",
    "\n",
    "    aggregated_stage1 = stage1_df.groupby(['hidden_size', 'latent_dim', 'activation', 'optimizer', 'epochs']).agg({\n",
    "        'silhouette_score': ['mean', 'std'],\n",
    "        'calinski_harabasz_score': ['mean', 'std'],\n",
    "        'davies_bouldin_score': ['mean', 'std'],\n",
    "        'reconstruction_loss': ['mean', 'std'],\n",
    "        'optimal_k': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    }).reset_index()\n",
    "\n",
    "    aggregated_stage1.columns = ['hidden_size', 'latent_dim', 'activation', 'optimizer', 'epochs',\n",
    "                                  'mean_silhouette', 'std_silhouette',\n",
    "                                  'mean_ch', 'std_ch',\n",
    "                                  'mean_db', 'std_db',\n",
    "                                  'mean_recon_loss', 'std_recon_loss',\n",
    "                                  'most_common_k']\n",
    "\n",
    "    aggregated_stage1['rank_silhouette'] = aggregated_stage1['mean_silhouette'].rank(ascending=False, method='min')\n",
    "    aggregated_stage1['rank_ch'] = aggregated_stage1['mean_ch'].rank(ascending=False, method='min')\n",
    "    aggregated_stage1['rank_db'] = aggregated_stage1['mean_db'].rank(ascending=True, method='min')\n",
    "\n",
    "    top_by_sil = aggregated_stage1.loc[aggregated_stage1['rank_silhouette'] == 1]\n",
    "    top_by_ch = aggregated_stage1.loc[aggregated_stage1['rank_ch'] == 1]\n",
    "    top_by_db = aggregated_stage1.loc[aggregated_stage1['rank_db'] == 1]\n",
    "\n",
    "    aggregated_stage1['config_id'] = aggregated_stage1.apply(\n",
    "        lambda row: f\"{row['hidden_size']}_{row['latent_dim']}_{row['activation']}_{row['optimizer']}_{row['epochs']}\", \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    votes = []\n",
    "    if len(top_by_sil) > 0:\n",
    "        votes.extend(top_by_sil['config_id'].tolist())\n",
    "    if len(top_by_ch) > 0:\n",
    "        votes.extend(top_by_ch['config_id'].tolist())\n",
    "    if len(top_by_db) > 0:\n",
    "        votes.extend(top_by_db['config_id'].tolist())\n",
    "\n",
    "    vote_counts = Counter(votes)\n",
    "    if len(vote_counts) > 0:\n",
    "        most_voted_config, vote_count = vote_counts.most_common(1)[0]\n",
    "    \n",
    "        if vote_count >= 2:\n",
    "            best_config_stage1 = aggregated_stage1[aggregated_stage1['config_id'] == most_voted_config].iloc[0]\n",
    "            consensus_status = f\"Consensus: {vote_count} metrics agree\"\n",
    "        else:\n",
    "            best_config_stage1 = aggregated_stage1.loc[aggregated_stage1['rank_silhouette'] == 1].iloc[0]\n",
    "            consensus_status = f\"No consensus. Using Silhouette\"\n",
    "    else:\n",
    "        aggregated_stage1_sorted = aggregated_stage1.sort_values('mean_silhouette', ascending=False)\n",
    "        best_config_stage1 = aggregated_stage1_sorted.iloc[0]\n",
    "        consensus_status = \"Using Silhouette (fallback)\"\n",
    "\n",
    "    best_hidden_size = int(best_config_stage1['hidden_size'])\n",
    "    best_latent_dim = int(best_config_stage1['latent_dim'])\n",
    "    best_activation_name = best_config_stage1['activation']\n",
    "    best_optimizer_name = best_config_stage1['optimizer']\n",
    "    best_epochs = int(best_config_stage1['epochs'])\n",
    "    best_k = force_k if force_k is not None else int(best_config_stage1['most_common_k'])\n",
    "\n",
    "    print(f\"Top 5 configs (by mean silhouette score):\\n\")\n",
    "    aggregated_stage1_sorted = aggregated_stage1.sort_values('mean_silhouette', ascending=False)\n",
    "    print(aggregated_stage1_sorted.head(5)[['hidden_size', 'latent_dim', 'activation', 'optimizer', \n",
    "                                            'epochs', 'mean_silhouette', 'std_silhouette',\n",
    "                                            'mean_ch', 'mean_db', 'most_common_k']].to_string(index=False))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Best Architecture (Stage 1): {consensus_status}\")\n",
    "    print(f\"  hidden_size={best_hidden_size}, latent_dim={best_latent_dim}\")\n",
    "    print(f\"  activation={best_activation_name}, optimizer={best_optimizer_name}, epochs={best_epochs}\")\n",
    "    print(f\"  Silhouette: {best_config_stage1['mean_silhouette']:.6f} ± {best_config_stage1['std_silhouette']:.6f}\")\n",
    "    print(f\"  Calinski-Harabasz: {best_config_stage1['mean_ch']:.6f} ± {best_config_stage1['std_ch']:.6f}\")\n",
    "    print(f\"  Davies-Bouldin: {best_config_stage1['mean_db']:.6f} ± {best_config_stage1['std_db']:.6f}\")\n",
    "    print(f\"  Reconstruction loss: {best_config_stage1['mean_recon_loss']:.6f} ± {best_config_stage1['std_recon_loss']:.6f}\")\n",
    "    print(f\"  Most common optimal K: {best_k}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # STAGE 2: Learning Parameter Optimization\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Fine-tune training parameters using best architecture from Stage 1\n",
    "    # Learning rate determined via LR Range Test\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"STAGE 2: Learning Parameter Optimization (K-Fold CV)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Using best architecture from Stage 1:\")\n",
    "    print(f\"  hidden={best_hidden_size}, latent={best_latent_dim}, activation={best_activation_name}, optimizer={best_optimizer_name}\")\n",
    "\n",
    "    activation_map = {'ReLU': nn.ReLU, 'Tanh': nn.Tanh, 'Sigmoid': nn.Sigmoid}\n",
    "    best_activation_fn = activation_map[best_activation_name]\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # LR Range Test: Find Optimal Learning Rate\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Test learning rates from 1e-7 to 10 to find optimal value\n",
    "    # Uses subset of training data for efficiency\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LR RANGE TEST (Using Best Architecture from Stage 1)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    class AutoencoderDataWrapper:\n",
    "        def __init__(self, data_loader):\n",
    "            self.data_loader = data_loader\n",
    "        def __iter__(self):\n",
    "            for batch in self.data_loader:\n",
    "                batch_data = batch[0]\n",
    "                yield batch_data, batch_data\n",
    "\n",
    "    train_subset = TensorDataset(train_val_tensor[:500])\n",
    "    train_loader_lr = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    train_loader_wrapped = AutoencoderDataWrapper(train_loader_lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    print(f\"\\nLR Range Test: {best_optimizer_name} optimizer\")\n",
    "    model_lr = Autoencoder(INPUT_DIM, best_hidden_size, best_latent_dim, best_activation_fn).to(device)\n",
    "\n",
    "    if best_optimizer_name == 'Adam':\n",
    "        optimizer_lr = optim.Adam(model_lr.parameters(), lr=1e-7)\n",
    "    else:\n",
    "        optimizer_lr = optim.SGD(model_lr.parameters(), lr=1e-7, momentum=0.9)\n",
    "\n",
    "    lr_finder = LRFinder(model_lr, optimizer_lr, criterion, device=device)\n",
    "    lr_finder.range_test(train_loader_wrapped, end_lr=10, num_iter=100, step_mode=\"exp\")\n",
    "    lr_finder.plot()\n",
    "    plt.title(f'{best_optimizer_name} LR Range Test (Best Architecture)', fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "    history = lr_finder.history\n",
    "    lrs = np.array(history['lr'])\n",
    "    losses = np.array(history['loss'])\n",
    "    loss_diffs = np.diff(losses)\n",
    "    descending = np.where(loss_diffs < 0)[0]\n",
    "\n",
    "    if len(descending) > 0:\n",
    "        mid = (descending[0] + descending[-1]) // 2\n",
    "        optimal_lr = lrs[mid]\n",
    "    else:\n",
    "        optimal_lr = lrs[np.argmin(losses)]\n",
    "\n",
    "    print(f\"Optimal LR from range test: {optimal_lr:.2e}\\n\")\n",
    "\n",
    "    lr_finder.reset()\n",
    "\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    weight_decays = [0, 1e-4, 1e-3]\n",
    "\n",
    "    if best_optimizer_name == 'SGD':\n",
    "        momentum_values = [0.5, 0.9, 0.95]\n",
    "        print(f\"Testing: batch_size, weight_decay, momentum\")\n",
    "        print(f\"Fixed: learning_rate = {optimal_lr:.2e}\")\n",
    "        total_experiments = len(batch_sizes) * len(weight_decays) * len(momentum_values) * n_folds\n",
    "    else:\n",
    "        momentum_values = [None]\n",
    "        print(f\"Testing: batch_size, weight_decay\")\n",
    "        print(f\"Fixed: learning_rate = {optimal_lr:.2e}\")\n",
    "        total_experiments = len(batch_sizes) * len(weight_decays) * n_folds\n",
    "\n",
    "    results_stage2 = defaultdict(list)\n",
    "    experiment_count = 0\n",
    "\n",
    "    print(f\"\\nGrid sizes:\")\n",
    "    print(f\"  learning_rate: 1 value (optimal from LR range test)\")\n",
    "    print(f\"  batch_size: {len(batch_sizes)} values {batch_sizes}\")\n",
    "    print(f\"  weight_decay: {len(weight_decays)} values {weight_decays}\")\n",
    "    if best_optimizer_name == 'SGD':\n",
    "        print(f\"  momentum: {len(momentum_values)} values {momentum_values}\")\n",
    "    print(f\"  CV folds: {n_folds}\")\n",
    "    print(f\"Total: {total_experiments} experiments ({total_experiments//n_folds} configs × {n_folds} folds)\\n\")\n",
    "\n",
    "    start_stage2 = time.time()\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        for weight_decay in weight_decays:\n",
    "            for momentum in momentum_values:\n",
    "                for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_val_tensor)):\n",
    "                    experiment_count += 1\n",
    "                    if experiment_count % 50 == 0 or experiment_count == total_experiments:\n",
    "                        elapsed = time.time() - start_stage2\n",
    "                        print(f\"  [{experiment_count}/{total_experiments}] {100*experiment_count/total_experiments:.1f}% - {elapsed/60:.1f}min\")\n",
    "                \n",
    "                    fold_seed = 42 + fold_idx\n",
    "                    torch.manual_seed(fold_seed)\n",
    "                    np.random.seed(fold_seed)\n",
    "                \n",
    "                    train_fold = train_val_tensor[train_idx]\n",
    "                    val_fold = train_val_tensor[val_idx]\n",
    "                \n",
    "                    train_dataset = TensorDataset(train_fold.cpu())\n",
    "                    val_dataset = TensorDataset(val_fold.cpu())\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "                \n",
    "                    model = Autoencoder(INPUT_DIM, best_hidden_size, best_latent_dim, best_activation_fn).to(device)\n",
    "                \n",
    "                    if best_optimizer_name == 'SGD':\n",
    "                        optimizer = optim.SGD(model.parameters(), lr=optimal_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "                    else:\n",
    "                        optimizer = optim.Adam(model.parameters(), lr=optimal_lr, weight_decay=weight_decay)\n",
    "                \n",
    "                    train_losses, val_losses, optimal_k, best_sil_score, latent_vectors, validation_metrics = \\\n",
    "                        train_and_validate_model(model, train_loader, val_loader, optimizer, \n",
    "                                                criterion, best_epochs, device)\n",
    "                \n",
    "                    optimal_k_idx = validation_metrics['k_values'].index(optimal_k)\n",
    "                    best_ch_score = validation_metrics['calinski_harabasz_scores'][optimal_k_idx]\n",
    "                    best_db_score = validation_metrics['davies_bouldin_scores'][optimal_k_idx]\n",
    "                \n",
    "                    results_stage2['learning_rate'].append(optimal_lr)\n",
    "                    results_stage2['batch_size'].append(batch_size)\n",
    "                    results_stage2['weight_decay'].append(weight_decay)\n",
    "                    if best_optimizer_name == 'SGD':\n",
    "                        results_stage2['momentum'].append(momentum)\n",
    "                    results_stage2['fold'].append(fold_idx)\n",
    "                    results_stage2['silhouette_score'].append(best_sil_score)\n",
    "                    results_stage2['calinski_harabasz_score'].append(best_ch_score)\n",
    "                    results_stage2['davies_bouldin_score'].append(best_db_score)\n",
    "                    results_stage2['reconstruction_loss'].append(val_losses[-1])\n",
    "\n",
    "    print(f\"\\n✓ Stage 2 completed in {(time.time()-start_stage2)/60:.2f} minutes\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # STAGE 2: Results Aggregation and Best Configuration Selection\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Aggregate results across folds, select best learning parameters\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"STAGE 2: Results Aggregation\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    stage2_df = pd.DataFrame(results_stage2)\n",
    "\n",
    "    if 'momentum' in results_stage2:\n",
    "        groupby_cols = ['learning_rate', 'batch_size', 'weight_decay', 'momentum']\n",
    "    else:\n",
    "        groupby_cols = ['learning_rate', 'batch_size', 'weight_decay']\n",
    "\n",
    "    aggregated_stage2 = stage2_df.groupby(groupby_cols).agg({\n",
    "        'silhouette_score': ['mean', 'std'],\n",
    "        'calinski_harabasz_score': ['mean', 'std'],\n",
    "        'davies_bouldin_score': ['mean', 'std'],\n",
    "        'reconstruction_loss': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    col_names = groupby_cols + ['mean_silhouette', 'std_silhouette', 'mean_ch', 'std_ch', \n",
    "                                'mean_db', 'std_db', 'mean_recon_loss', 'std_recon_loss']\n",
    "    aggregated_stage2.columns = col_names\n",
    "\n",
    "    aggregated_stage2['rank_silhouette'] = aggregated_stage2['mean_silhouette'].rank(ascending=False, method='min')\n",
    "    aggregated_stage2['rank_ch'] = aggregated_stage2['mean_ch'].rank(ascending=False, method='min')\n",
    "    aggregated_stage2['rank_db'] = aggregated_stage2['mean_db'].rank(ascending=True, method='min')\n",
    "\n",
    "    top_by_sil = aggregated_stage2.loc[aggregated_stage2['rank_silhouette'] == 1]\n",
    "    top_by_ch = aggregated_stage2.loc[aggregated_stage2['rank_ch'] == 1]\n",
    "    top_by_db = aggregated_stage2.loc[aggregated_stage2['rank_db'] == 1]\n",
    "\n",
    "    aggregated_stage2['config_id'] = aggregated_stage2.apply(\n",
    "        lambda row: f\"{row['learning_rate']:.2e}_{row['batch_size']}_{row['weight_decay']:.2e}\" + \n",
    "                    (f\"_{row['momentum']}\" if 'momentum' in row else \"\"), \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    votes = []\n",
    "    if len(top_by_sil) > 0:\n",
    "        votes.extend(top_by_sil['config_id'].tolist())\n",
    "    if len(top_by_ch) > 0:\n",
    "        votes.extend(top_by_ch['config_id'].tolist())\n",
    "    if len(top_by_db) > 0:\n",
    "        votes.extend(top_by_db['config_id'].tolist())\n",
    "\n",
    "    vote_counts = Counter(votes)\n",
    "    if len(vote_counts) > 0:\n",
    "        most_voted_config, vote_count = vote_counts.most_common(1)[0]\n",
    "    \n",
    "        if vote_count >= 2:\n",
    "            best_config_stage2 = aggregated_stage2[aggregated_stage2['config_id'] == most_voted_config].iloc[0]\n",
    "            consensus_status = f\"Consensus: {vote_count} metrics agree\"\n",
    "        else:\n",
    "            best_config_stage2 = aggregated_stage2.loc[aggregated_stage2['rank_silhouette'] == 1].iloc[0]\n",
    "            consensus_status = f\"No consensus. Using Silhouette\"\n",
    "    else:\n",
    "        aggregated_stage2_sorted = aggregated_stage2.sort_values('mean_silhouette', ascending=False)\n",
    "        best_config_stage2 = aggregated_stage2_sorted.iloc[0]\n",
    "        consensus_status = \"Using Silhouette (fallback)\"\n",
    "\n",
    "    best_learning_rate = best_config_stage2['learning_rate']\n",
    "    best_batch_size = int(best_config_stage2['batch_size'])\n",
    "    best_weight_decay = best_config_stage2['weight_decay']\n",
    "    best_momentum = best_config_stage2.get('momentum', None)\n",
    "\n",
    "    print(f\"Top 5 configs (by mean silhouette score):\\n\")\n",
    "    aggregated_stage2_sorted = aggregated_stage2.sort_values('mean_silhouette', ascending=False)\n",
    "    print(aggregated_stage2_sorted.head(5)[groupby_cols + ['mean_silhouette', 'std_silhouette', \n",
    "                                                            'mean_ch', 'mean_db']].to_string(index=False))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Best Overall Configuration: {consensus_status}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Architecture (from Stage 1):\")\n",
    "    print(f\"  hidden_size={best_hidden_size}, latent_dim={best_latent_dim}\")\n",
    "    print(f\"  activation={best_activation_name}, optimizer={best_optimizer_name}, epochs={best_epochs}\")\n",
    "    print(f\"\\nLearning Parameters (from Stage 2):\")\n",
    "    print(f\"  learning_rate={best_learning_rate:.2e}\")\n",
    "    print(f\"  batch_size={best_batch_size}\")\n",
    "    print(f\"  weight_decay={best_weight_decay:.2e}\", end=\"\")\n",
    "    if best_momentum is not None:\n",
    "        print(f\", momentum={best_momentum}\")\n",
    "    else:\n",
    "        print()\n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  Silhouette: {best_config_stage2['mean_silhouette']:.6f} ± {best_config_stage2['std_silhouette']:.6f}\")\n",
    "    print(f\"  Calinski-Harabasz: {best_config_stage2['mean_ch']:.6f} ± {best_config_stage2['std_ch']:.6f}\")\n",
    "    print(f\"  Davies-Bouldin: {best_config_stage2['mean_db']:.6f} ± {best_config_stage2['std_db']:.6f}\")\n",
    "    print(f\"  Reconstruction loss: {best_config_stage2['mean_recon_loss']:.6f} ± {best_config_stage2['std_recon_loss']:.6f}\")\n",
    "    print(f\"  Optimal K: {best_k}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # FINAL MODEL TRAINING AND LATENT PROFILE EXTRACTION\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Train final model on all train+val data with best hyperparameters\n",
    "    # Extract latent vectors and perform K-means clustering\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"Final Model Training and Latent Profile Extraction\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Training final model on all {dataset_name} train+val data with best hyperparameters:\")\n",
    "    print(f\"  Architecture: hidden={best_hidden_size}, latent={best_latent_dim}, activation={best_activation_name}\")\n",
    "    print(f\"  Optimizer: {best_optimizer_name}, lr={best_learning_rate:.2e}, batch_size={best_batch_size}\")\n",
    "    print(f\"  Epochs: {best_epochs}, Optimal K: {best_k}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    final_dataset = TensorDataset(train_val_tensor.cpu())\n",
    "    final_loader = DataLoader(final_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "    activation_map = {'ReLU': nn.ReLU, 'Tanh': nn.Tanh, 'Sigmoid': nn.Sigmoid}\n",
    "    best_activation_fn = activation_map[best_activation_name]\n",
    "    final_model = Autoencoder(INPUT_DIM, best_hidden_size, best_latent_dim, best_activation_fn).to(device)\n",
    "\n",
    "    if best_optimizer_name == 'SGD':\n",
    "        final_optimizer = optim.SGD(final_model.parameters(), lr=best_learning_rate, \n",
    "                                    momentum=best_momentum if best_momentum is not None else 0.9, \n",
    "                                    weight_decay=best_weight_decay)\n",
    "    else:\n",
    "        final_optimizer = optim.Adam(final_model.parameters(), lr=best_learning_rate, \n",
    "                                    weight_decay=best_weight_decay)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    print(f\"\\nTraining final model...\")\n",
    "    final_model.train()\n",
    "    final_losses = []\n",
    "\n",
    "    for epoch in range(best_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_data in final_loader:\n",
    "            batch_data = batch_data[0].to(device)\n",
    "            final_optimizer.zero_grad()\n",
    "            reconstructed = final_model(batch_data)\n",
    "            loss = criterion(reconstructed, batch_data)\n",
    "            loss.backward()\n",
    "            final_optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(final_loader)\n",
    "        final_losses.append(avg_loss)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{best_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nFinal model training complete. Saving model...\")\n",
    "    final_model.eval()\n",
    "    all_latent_vectors_batches = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in final_loader:\n",
    "            data = batch_data[0].to(device)\n",
    "            latent = final_model.encoder(data)\n",
    "            all_latent_vectors_batches.append(latent.cpu())\n",
    "    latent_vectors_all = np.vstack(all_latent_vectors_batches)\n",
    "    print(f\"Latent vectors shape: {latent_vectors_all.shape}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # K-Means Clustering: Extract Mental Health Profiles\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Cluster latent vectors to identify distinct mental health profiles\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"Running K-means clustering with optimal k... {best_k}\")\n",
    "    final_kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_SEED, n_init=10)\n",
    "    cluster_labels_all = final_kmeans.fit_predict(latent_vectors_all)\n",
    "    cluster_centroids = final_kmeans.cluster_centers_\n",
    "\n",
    "    print(f\"Cluster Assignments: {cluster_labels_all}\")\n",
    "    print(f\"Cluster Centroids: {cluster_centroids.shape}\")\n",
    "\n",
    "    final_sil_score = silhouette_score(latent_vectors_all, cluster_labels_all)\n",
    "    print(f\"Final Silhouette Score: {final_sil_score:.4f}\")\n",
    "    final_ch_score = calinski_harabasz_score(latent_vectors_all, cluster_labels_all)\n",
    "    print(f\"Final Calinski-Harabasz Score: {final_ch_score:.4f}\")\n",
    "    final_db_score = davies_bouldin_score(latent_vectors_all, cluster_labels_all)\n",
    "    print(f\"Final Davies-Bouldin Score: {final_db_score:.4f}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # PROFILE CHARACTERISTICS EXTRACTION\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Map cluster labels back to original symptom space to interpret profiles\n",
    "    # Compute mean symptom levels (Depression, Anxiety, Stress, Burnout) per cluster\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\nProfile Characteristics (mean feature values per cluster):\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # VERIFICATION: Check that column order matches FEATURE_COLUMNS\n",
    "    print(\"Verifying feature column order...\")\n",
    "    sample_0 = train_val_data[0]\n",
    "    print(f\"FEATURE_COLUMNS: {FEATURE_COLUMNS}\")\n",
    "    print(f\"First sample values:\")\n",
    "    for i, feature_name in enumerate(FEATURE_COLUMNS):\n",
    "        print(f\"  train_val_data[0, {i}] = {sample_0[i]:.4f} → {feature_name}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    profile_summary = []\n",
    "\n",
    "    for k in range(best_k):\n",
    "        cluster_mask = cluster_labels_all == k\n",
    "        # CRITICAL: Use original 4D symptom data, NOT latent vectors as they are more intellgible to understand  \n",
    "        cluster_data = train_val_data[cluster_mask]  # Original 4D: [Depression, Anxiety, Stress, Burnout]\n",
    "        cluster_size = np.sum(cluster_mask)\n",
    "    \n",
    "        # Method 1: Using array indexing (for verification)\n",
    "        feature_means_array = cluster_data.mean(axis=0)\n",
    "    \n",
    "        # Method 2: Using DataFrame for explicit mapping (SAFER)\n",
    "        cluster_df = pd.DataFrame(cluster_data, columns=FEATURE_COLUMNS)\n",
    "        feature_means_dict = cluster_df.mean().to_dict()\n",
    "    \n",
    "        # Verify they match\n",
    "        print(f\"Cluster {k} (N={cluster_size}):\")\n",
    "        for i, feature_name in enumerate(FEATURE_COLUMNS):\n",
    "            array_val = feature_means_array[i]\n",
    "            dict_val = feature_means_dict[feature_name]\n",
    "            match = \" MATCH\" if abs(array_val - dict_val) < 1e-10 else \" MISMATCH\"\n",
    "            print(f\"  Index {i} ({feature_name}): array[{i}]={array_val:.6f}, dict['{feature_name}']={dict_val:.6f} {match}\")\n",
    "    \n",
    "        # Use dictionary approach (explicit, no index guessing)\n",
    "        profile_summary.append({\n",
    "            'Profile': f'P{k+1}',\n",
    "            'N': cluster_size,\n",
    "            'Depression': feature_means_dict['Depression'],\n",
    "            'Anxiety': feature_means_dict['Anxiety'],\n",
    "            'Stress': feature_means_dict['Stress'],\n",
    "            'Burnout': feature_means_dict['Burnout']\n",
    "        })\n",
    "        print()\n",
    "\n",
    "    profile_df = pd.DataFrame(profile_summary)\n",
    "    print(\"Profile Summary Table:\")\n",
    "    print(profile_df.to_string(index=False))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Final Model Results Saved:\")\n",
    "    print(f\"  - {dataset_name} latent vectors: {latent_vectors_all.shape}\")\n",
    "    print(f\"  - {dataset_name} cluster assignments: {cluster_labels_all.shape}\")\n",
    "    print(f\"  - {dataset_name} cluster centroids: {cluster_centroids.shape}\")\n",
    "    print(f\"  - Profile summary: {len(profile_summary)} profiles\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # PROFILE INTERPRETATION\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Classify profiles based on symptom levels relative to global thresholds\n",
    "    # Assign meaningful names (e.g., \"Severe Comorbid\", \"Low Symptom\")\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"Interpretation of the profiles:\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "    #It is important to compute global threshold values for each profile based on the entire dataset not just the cluster data\n",
    "    #This ensures consistency and comparability across different datasets\n",
    "\n",
    "\n",
    "    global_depression_threshold_high = np.percentile(train_val_data[:, 0], 75)  \n",
    "    global_depression_threshold_low = np.percentile(train_val_data[:, 0], 25)  \n",
    "    global_anxiety_threshold_high = np.percentile(train_val_data[:, 1], 75)\n",
    "    global_anxiety_threshold_low = np.percentile(train_val_data[:, 1], 25)\n",
    "    global_stress_threshold_high = np.percentile(train_val_data[:, 2], 75)\n",
    "    global_stress_threshold_low = np.percentile(train_val_data[:, 2], 25)\n",
    "    global_burnout_threshold_high = np.percentile(train_val_data[:, 3], 75)\n",
    "    global_burnout_threshold_low = np.percentile(train_val_data[:, 3], 25)\n",
    "\n",
    "    def interpret_profile(depression, anxiety, stress, burnout):\n",
    "        \"\"\"\n",
    "        Interpret a single profile based on global thresholds\n",
    "        returns a string description of the profile\n",
    "        \"\"\"\n",
    "\n",
    "        high_symptoms = []\n",
    "        low_symptoms = []\n",
    "\n",
    "\n",
    "        # Compare to global thresholds not just cluster centroids\n",
    "        if depression > global_depression_threshold_high:\n",
    "            high_symptoms.append(\"Depression\")\n",
    "        elif depression < global_depression_threshold_low:\n",
    "            low_symptoms.append(\"Depression\")\n",
    "\n",
    "        if anxiety > global_anxiety_threshold_high:\n",
    "            high_symptoms.append(\"Anxiety\")\n",
    "        elif anxiety < global_anxiety_threshold_low:\n",
    "            low_symptoms.append(\"Anxiety\")\n",
    "\n",
    "        if stress > global_stress_threshold_high:\n",
    "            high_symptoms.append(\"Stress\")\n",
    "        elif stress < global_stress_threshold_low:\n",
    "            low_symptoms.append(\"Stress\")\n",
    "\n",
    "        if burnout > global_burnout_threshold_high:\n",
    "            high_symptoms.append(\"Burnout\")\n",
    "        elif burnout < global_burnout_threshold_low:\n",
    "            low_symptoms.append(\"Burnout\")\n",
    "    \n",
    "        if len(high_symptoms) >= 3:\n",
    "            return \"Severe Comorbid Profile\", \"High levels across multiple dimensions\"\n",
    "        elif \"Depression\" in high_symptoms and \"Anxiety\" in high_symptoms:\n",
    "            return \"Depression-Anxiety Comorbidity Profile\", \"High Depression and Anxiety, typical of internalizing disorders\"\n",
    "        elif \"Stress\" in high_symptoms and \"Burnout\" in high_symptoms:\n",
    "            return \"Stress-Burnout Profile\", \"High Stress and Burnout, typical of work-related distress\"\n",
    "        elif len(high_symptoms) == 1:\n",
    "            return f\"High {high_symptoms[0]} Profile\", f\"Elevated {high_symptoms[0]} with other symptoms in normal range\"\n",
    "        elif len(low_symptoms) >= 3:\n",
    "            return \"Low Symptom Profile\", \"Low levels across most dimensions\"\n",
    "        else:\n",
    "            return \"Moderate/Mixed Profile\", \"Moderate levels across dimensions\"\n",
    "\n",
    "    for i, profile in enumerate(profile_summary):\n",
    "        profile_name, description = interpret_profile(\n",
    "            profile['Depression'], \n",
    "            profile['Anxiety'], \n",
    "            profile['Stress'], \n",
    "            profile['Burnout']\n",
    "        )\n",
    "        profile_summary[i]['Profile_Name'] = profile_name\n",
    "        profile_summary[i]['Description'] = description\n",
    "\n",
    "    # Display interpreted profiles\n",
    "    print(\"Interpreted Profiles:\")\n",
    "    interpreted_df = pd.DataFrame(profile_summary)\n",
    "    print(interpreted_df[['Profile', 'Profile_Name', 'N', 'Depression', 'Anxiety', 'Stress', 'Burnout', 'Description']].to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # TEST SET EVALUATION\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Evaluate model generalization on held-out test data\n",
    "    # Test set was never used during hyperparameter tuning or training\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"Test-Set - 20% of the data was kept aside and used for testing\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Test set was held out during hyperparameter tuning - now evaluating final model\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"Encoding test data with trained autoencoder...\")\n",
    "    test_dataset = TensorDataset(test_tensor.cpu())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "    final_model.eval()\n",
    "    test_latent_vectors = []\n",
    "    test_reconstructions = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            data = batch_data[0].to(device)\n",
    "            latent = final_model.encoder(data)\n",
    "            reconstructed = final_model(data)\n",
    "            test_latent_vectors.append(latent.cpu().numpy())\n",
    "            test_reconstructions.append(reconstructed.cpu().numpy())\n",
    "    test_latent = np.vstack(test_latent_vectors)\n",
    "    test_recon = np.vstack(test_reconstructions)\n",
    "\n",
    "\n",
    "    print(f\"  Test latent vectors: {test_latent.shape}\")\n",
    "    print(f\"  Test reconstructions: {test_recon.shape}\")\n",
    "\n",
    "    # Compute test reconstruction error\n",
    "    test_data_np = test_tensor.cpu().numpy()\n",
    "    test_recon_loss = np.mean((test_data_np - test_recon) ** 2)\n",
    "    print(f\"  Test reconstruction loss (MSE): {test_recon_loss:.6f}\")\n",
    "\n",
    "    # Assign test samples to clusters using trained centroids\n",
    "    # CRITICAL: Use centroids from train+val, don't retrain K-means\n",
    "    # This tests if cluster structure generalizes to new data\n",
    "    print(f\"\\nAssigning test samples to clusters...\")\n",
    "    from scipy.spatial.distance import cdist\n",
    "    test_distances = cdist(test_latent, cluster_centroids, metric='euclidean')\n",
    "    test_cluster_assignments = np.argmin(test_distances, axis=1)\n",
    "\n",
    "    print(f\"  Test cluster assignments: {Counter(test_cluster_assignments)}\")\n",
    "\n",
    "    # Evaluate clustering quality on test set\n",
    "    test_sil_score = silhouette_score(test_latent, test_cluster_assignments)\n",
    "    test_ch_score = calinski_harabasz_score(test_latent, test_cluster_assignments)\n",
    "    test_db_score = davies_bouldin_score(test_latent, test_cluster_assignments)\n",
    "\n",
    "    print(f\"\\nTest Set Clustering Quality:\")\n",
    "    print(f\"  Silhouette Score: {test_sil_score:.6f}\")\n",
    "    print(f\"  Calinski-Harabasz: {test_ch_score:.6f}\")\n",
    "    print(f\"  Davies-Bouldin: {test_db_score:.6f}\")\n",
    "\n",
    "    # Check for overfitting: Compare train+val vs test performance\n",
    "    if final_sil_score != 0:\n",
    "        sil_diff_pct = abs(final_sil_score - test_sil_score) / final_sil_score * 100\n",
    "    else:\n",
    "        sil_diff_pct = float('inf')  # Handle edge case where final_sil_score is 0\n",
    "    if sil_diff_pct < 5:\n",
    "        print(f\"\\n✓ Good generalization: Test performance within {sil_diff_pct:.2f}% of train+val\")\n",
    "    elif sil_diff_pct < 10:\n",
    "        print(f\"\\n⚠ Moderate generalization gap: Test performance {sil_diff_pct:.2f}% different from train+val\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Potential overfitting: Test performance {sil_diff_pct:.2f}% different from train+val\")\n",
    "\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # VISUALIZATIONS\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Create visualizations of latent space and profile characteristics\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"Visualizing Latent Space and Profile Characteristics\")\n",
    "\n",
    "    #1. Latent Space Visualization\n",
    "    print(\"\\n1. Latent Space Viz\")\n",
    "\n",
    "    if best_latent_dim == 2:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        scatter = ax.scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], \n",
    "                            c=cluster_labels_all, cmap='viridis', alpha=0.6, s=30)\n",
    "        ax.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], c='red', marker='x', \n",
    "                  s=300, linewidths=4, label='Cluster Centroids', zorder=5)\n",
    "        #Add profile labels to centroids\n",
    "\n",
    "        for i, (x,y) in enumerate(cluster_centroids):\n",
    "            profile_name = profile_summary[i].get('Profile_Name', f'P{i+1}')\n",
    "            ax.annotate(profile_name, (x,y), xytext=(5,5), textcoords='offset points',\n",
    "                       fontsize=10, fontweight='bold', bbox=dict(boxstyle='round,pad=0.3',\n",
    "                       facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "        ax.set_xlabel('Latent Dimension 1', fontsize=12)\n",
    "        ax.set_ylabel('Latent Dimension 2', fontsize=12)\n",
    "        ax.set_title('Latent Space Visualization (Colored by Profile)', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=ax, label='Profile')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    elif best_latent_dim == 3:\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        scatter = ax.scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], latent_vectors_all[:, 2],\n",
    "                            c=cluster_labels_all, cmap='viridis', alpha=0.6, s=30)\n",
    "        ax.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], cluster_centroids[:, 2],\n",
    "                  c='red', marker='x', s=300, linewidths=4, label='Centroids')\n",
    "    \n",
    "        ax.set_xlabel('Latent Dim 1', fontsize=12)\n",
    "        ax.set_ylabel('Latent Dim 2', fontsize=12)\n",
    "        ax.set_zlabel('Latent Dim 3', fontsize=12)\n",
    "        ax.set_title('Latent Space Visualization (3D)', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        plt.colorbar(scatter, ax=ax, label='Profile')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    #2. Profile Characteristics Heatmap\n",
    "    print(\"\\n2. Profile Characteristics Heatmap:\")\n",
    "    profile_matrix = pd.DataFrame(profile_summary)[['Profile', 'Depression', 'Anxiety', 'Stress', 'Burnout']].set_index('Profile')\n",
    "    profile_matrix_normalized = (profile_matrix - profile_matrix.min()) / (profile_matrix.max() - profile_matrix.min())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(profile_matrix_normalized.T, annot=profile_matrix.T, fmt='.3f', cmap='RdYlGn_r',\n",
    "               cbar_kws={'label': 'Normalized Symptom Level'}, linewidths=0.5, linecolor='black')\n",
    "    plt.title('Profile Characteristics Heatmap\\n(Normalized Symptom Levels)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Profile', fontsize=12)\n",
    "    plt.ylabel('Symptom', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #Profile Bar Chart\n",
    "    print(\"\\n3. Profile Symptom Levels (Bar Chart):\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(len(profile_summary))\n",
    "    width = 0.2\n",
    "\n",
    "    for i, profile in enumerate(profile_summary):\n",
    "        ax.bar(x[i] - 1.5*width, profile['Depression'], width, label='Depression' if i == 0 else '', color='#ff6b6b')\n",
    "        ax.bar(x[i] - 0.5*width, profile['Anxiety'], width, label='Anxiety' if i == 0 else '', color='#4ecdc4')\n",
    "        ax.bar(x[i] + 0.5*width, profile['Stress'], width, label='Stress' if i == 0 else '', color='#45b7d1')\n",
    "        ax.bar(x[i] + 1.5*width, profile['Burnout'], width, label='Burnout' if i == 0 else '', color='#f9ca24')\n",
    "\n",
    "    ax.set_xlabel('Profile', fontsize=12)\n",
    "    ax.set_ylabel('Symptom Level', fontsize=12)\n",
    "    ax.set_title('Symptom Levels by Profile', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([p['Profile'] for p in profile_summary])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #Cluster Size Distribution\n",
    "    print(\"\\n4. Cluster Size Distribution:\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cluster_sizes = [p['N'] for p in profile_summary]\n",
    "    cluster_labels_viz = [p['Profile'] for p in profile_summary]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(cluster_sizes)))\n",
    "\n",
    "    bars = ax.bar(cluster_labels_viz, cluster_sizes, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    ax.set_xlabel('Profile', fontsize=12)\n",
    "    ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "    ax.set_title('Profile Size Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # H3 VALIDATION: Clinical Utility Testing\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Test if profile membership predicts therapy utilization\n",
    "    # Only available for D1-Swiss dataset\n",
    "    # ------------------------------------------------------------------------\n",
    "    if dataset_name == \"D1-Swiss\" and \"PSYT_Therapy_Use\" in all_data_df.columns:\n",
    "        print(\"\\nH3 VALIDATION: Testing Clinical Utility of Profiles\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Hypothesis H3: Profile membership is associated with therapy utilization\")\n",
    "        print(\"Using FULL dataset (train+val+test) for maximum statistical power\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        y_therapy = all_data_df[\"PSYT_Therapy_Use\"].values\n",
    "        train_val_therapy, test_therapy = train_test_split(\n",
    "            y_therapy,\n",
    "            test_size=0.2,\n",
    "            random_state=RANDOM_SEED,\n",
    "        )\n",
    "        y_therapy_aligned = np.concatenate([train_val_therapy, test_therapy])\n",
    "        all_cluster_labels = np.concatenate([cluster_labels_all, test_cluster_assignments])\n",
    "\n",
    "        assert len(y_therapy_aligned) == len(all_cluster_labels), \"Misalignment\"\n",
    "        print(f\"\\n✓ Data aligned: {len(all_cluster_labels)} samples\")\n",
    "        print(f\"  Therapy use rate: {y_therapy_aligned.mean():.2%} ({y_therapy_aligned.sum()}/{len(y_therapy_aligned)})\")\n",
    "\n",
    "        print(\"\\nChi-Square Test for Independence:\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        contingency = pd.crosstab(all_cluster_labels, y_therapy_aligned)\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "        print(\"   Contingency Table:\")\n",
    "        print(contingency)\n",
    "        print(f\"\\n   Chi-square statistic: χ² = {chi2:.4f}\")\n",
    "        print(f\"   Degrees of freedom: df = {dof}\")\n",
    "        print(f\"   p-value: p = {p:.6f}\")\n",
    "\n",
    "        alpha = 0.05\n",
    "        if p < alpha:\n",
    "            print(f\"\\n   ✓ SIGNIFICANT (p < {alpha}): Profile membership IS associated with therapy utilization\")\n",
    "            print(\"   → H3 VALIDATED: Profiles have clinical utility\")\n",
    "        else:\n",
    "            print(f\"\\n   ✗ NOT SIGNIFICANT (p >= {alpha}): No association detected\")\n",
    "            print(\"   → H3 NOT VALIDATED\")\n",
    "\n",
    "        print(\"\\nCramer V Effect Size:\")\n",
    "        print(\"=\"*70)\n",
    "        n = contingency.values.sum()\n",
    "        min_dim = min(contingency.shape)\n",
    "        cramers_v = np.sqrt(chi2 / (n * (min_dim - 1)))\n",
    "        print(f\"   Cramer's V: {cramers_v:.4f}\")\n",
    "\n",
    "        if cramers_v < 0.10:\n",
    "            effect_size = \"negligible\"\n",
    "        elif cramers_v < 0.30:\n",
    "            effect_size = \"small\"\n",
    "        elif cramers_v < 0.50:\n",
    "            effect_size = \"medium\"\n",
    "        else:\n",
    "            effect_size = \"large\"\n",
    "        print(f\"   Effect size: {effect_size}\")\n",
    "\n",
    "        print(\"\\n3. Post-Hoc Analysis: Standardized Residuals:\")\n",
    "        print(\"-\"*70)\n",
    "        print(\"   (Values > |2| indicate significant deviation from expected)\")\n",
    "        residuals = (contingency.values - expected) / np.sqrt(expected + 1e-10)\n",
    "        residuals_df = pd.DataFrame(\n",
    "            residuals,\n",
    "            index=[f'P{k+1}' for k in range(best_k)],\n",
    "            columns=['No Therapy', 'Therapy']\n",
    "        )\n",
    "        print(residuals_df.round(3))\n",
    "\n",
    "        print(\"\\n   Significant deviations:\")\n",
    "        for i in range(best_k):\n",
    "            for j in range(2):\n",
    "                if abs(residuals[i, j]) > 2:\n",
    "                    profile_name = profile_summary[i].get('Profile_Name', f'P{i+1}')\n",
    "                    therapy_status = 'Therapy' if j == 1 else 'No Therapy'\n",
    "                    direction = 'Higher' if residuals[i, j] > 0 else 'Lower'\n",
    "                    print(f\"      • {profile_name} - {therapy_status}: {direction} than expected (residual = {residuals[i, j]:.2f})\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"H3 VALIDATION SUMMARY:\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Dataset: {dataset_name} (N={len(all_cluster_labels)})\")\n",
    "        print(f\"Chi-square: χ² = {chi2:.4f}, p = {p:.6f}, df = {dof}\")\n",
    "        print(f\"Cramér's V = {cramers_v:.4f} ({effect_size} effect)\")\n",
    "        print(f\"H3 Status: {' VALIDATED' if p < alpha else '✗ NOT VALIDATED'}\")\n",
    "\n",
    "        if p < alpha:\n",
    "            print(\"\\nConclusion:\")\n",
    "            print(\"  Profiles demonstrate clinical utility by predicting therapy utilization.\")\n",
    "            print(\"  This supports the use of these profiles for targeted mental health interventions.\")\n",
    "\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "    else:\n",
    "        print(\"\\nSkipping H3 validation (only available for D1-Swiss with PSYT_Therapy_Use)\")\n",
    "\n",
    "    result = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'train_val_data': train_val_data,\n",
    "        'latent_vectors_all': latent_vectors_all,\n",
    "        'cluster_labels_all': cluster_labels_all,\n",
    "        'cluster_centroids': cluster_centroids,\n",
    "        'best_k': best_k,\n",
    "        'best_latent_dim': best_latent_dim,\n",
    "        'final_sil_score': final_sil_score,\n",
    "        'final_ch_score': final_ch_score,\n",
    "        'final_db_score': final_db_score,\n",
    "        'reconstruction_loss': test_recon_loss,  # Store test reconstruction loss for comparison\n",
    "    }\n",
    "    PIPELINE_RESULTS[dataset_name] = result\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: TRAINING AND VALIDATION FUNCTION\n",
    "# ============================================================================\n",
    "# Purpose: Train autoencoder and evaluate clustering quality in latent space\n",
    "# Key feature: Model quality evaluated by clustering metrics, not just\n",
    "#              reconstruction loss, because goal is finding distinct profiles\n",
    "# ============================================================================\n",
    "\n",
    "def train_and_validate_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Train and validate the autoencoder model and evaluate reconstruction accuracy and latent quality.\n",
    "    \n",
    "    Uses multiple validation methods for K selection with consensus/voting approach:\n",
    "    - Silhouette score (primary, tiebreaker)\n",
    "    - Calinski-Harabasz index\n",
    "    - Davies-Bouldin index\n",
    "    - Elbow method (WCSS-based knee detection)\n",
    "    \n",
    "    K selection: Uses consensus voting - if 2+ methods agree on K, that K is selected.\n",
    "    If no consensus, falls back to silhouette score (most interpretable).\n",
    "    \n",
    "    Returns:\n",
    "        train_losses: list of training loss values\n",
    "        val_losses: list of validation loss values\n",
    "        optimal_k: optimal number of clusters (consensus or silhouette-based)\n",
    "        best_silhouette_score: best silhouette score achieved\n",
    "        latent_vectors: latent representations (train+val combined)\n",
    "        validation_metrics: dict with all K validation metrics and consensus info\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        epoch_training_loss = 0.0\n",
    "        for batch_data in train_loader:\n",
    "            batch_data = batch_data[0].to(device)  # Unpack tuple from DataLoader\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(batch_data)\n",
    "            loss = criterion(reconstructed, batch_data)  # MSE Reconstruction Loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.item()\n",
    "        avg_train_loss = epoch_training_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        epoch_validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_data in val_loader:\n",
    "                batch_data = batch_data[0].to(device)\n",
    "                reconstructed = model(batch_data)\n",
    "                loss = criterion(reconstructed, batch_data)\n",
    "                epoch_validation_loss += loss.item()\n",
    "        avg_val_loss = epoch_validation_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        model.train()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # EXTRACT LATENT VECTORS AND EVALUATE CLUSTERING QUALITY\n",
    "    # ------------------------------------------------------------------------\n",
    "    # After training, extract latent representations and evaluate clustering\n",
    "    # ------------------------------------------------------------------------\n",
    "    model.eval()\n",
    "    all_latent_vectors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract latent vectors from training data\n",
    "        for batch_data in train_loader:\n",
    "            data = batch_data[0].to(device)\n",
    "            latent = model.encoder(data)\n",
    "            all_latent_vectors.append(latent.cpu().numpy())\n",
    "        \n",
    "        # Extract latent vectors from validation data\n",
    "        for batch_data in val_loader:\n",
    "            data = batch_data[0].to(device)\n",
    "            latent = model.encoder(data)\n",
    "            all_latent_vectors.append(latent.cpu().numpy())\n",
    "    \n",
    "    # Combine all latent vectors\n",
    "    latent_vectors = np.vstack(all_latent_vectors)\n",
    "\n",
    "    # =========================================================================\n",
    "    # K Selection via Multiple Validation Methods (Convergent Validity)\n",
    "    # =========================================================================\n",
    "    k_range = range(2, 7)  # K = 2, 3, 4, 5, 6\n",
    "    \n",
    "    # Initialize metric lists\n",
    "    silhouette_scores_list = []\n",
    "    calinski_harabasz_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "    wcss_values = []  # Within-cluster sum of squares (for elbow method)\n",
    "    \n",
    "    best_silhouette_score = -1\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=RANDOM_SEED, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(latent_vectors)\n",
    "        \n",
    "        # Compute all validation metrics\n",
    "        sil_score = silhouette_score(latent_vectors, cluster_labels)\n",
    "        ch_score = calinski_harabasz_score(latent_vectors, cluster_labels)\n",
    "        db_score = davies_bouldin_score(latent_vectors, cluster_labels)\n",
    "        wcss = kmeans.inertia_  # Within-cluster sum of squares\n",
    "        \n",
    "        # Store metrics\n",
    "        silhouette_scores_list.append(sil_score)\n",
    "        calinski_harabasz_scores.append(ch_score)\n",
    "        davies_bouldin_scores.append(db_score)\n",
    "        wcss_values.append(wcss)\n",
    "        \n",
    "        # Track best silhouette score\n",
    "        if sil_score > best_silhouette_score:\n",
    "            best_silhouette_score = sil_score\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Consensus/Voting Approach for K Selection\n",
    "    # =========================================================================\n",
    "    # Determine optimal K from each method\n",
    "    optimal_k_silhouette = k_range[np.argmax(silhouette_scores_list)]\n",
    "    optimal_k_ch = k_range[np.argmax(calinski_harabasz_scores)]  # Highest CH = best\n",
    "    optimal_k_db = k_range[np.argmin(davies_bouldin_scores)]  # Lowest DB = best\n",
    "    \n",
    "    # Elbow Method: Find the \"knee\" where WCSS decrease rate slows down\n",
    "    # Compute percentage decrease in WCSS for each K\n",
    "    wcss_decreases = []\n",
    "    for i in range(1, len(wcss_values)):\n",
    "        if wcss_values[i-1] > 0:  # Avoid division by zero\n",
    "            pct_decrease = ((wcss_values[i-1] - wcss_values[i]) / wcss_values[i-1]) * 100\n",
    "            wcss_decreases.append(pct_decrease)\n",
    "        else:\n",
    "            wcss_decreases.append(0)\n",
    "    \n",
    "    # Find elbow: K where decrease rate drops most (knee point)\n",
    "    # The elbow is where adding more clusters doesn't significantly reduce WCSS\n",
    "    if len(wcss_decreases) > 0:\n",
    "        decrease_rates = np.array(wcss_decreases)\n",
    "        # Method: Find where the decrease rate drops most (elbow detection)\n",
    "        # Compute the rate of change of decrease rates (second derivative of WCSS)\n",
    "        if len(decrease_rates) > 1:\n",
    "            # Rate changes: how much the decrease rate changes between consecutive K\n",
    "            rate_changes = np.diff(decrease_rates)\n",
    "            # Elbow is where rate change is most negative (biggest drop in decrease rate)\n",
    "            # This means: decrease rate was high, then dropped significantly\n",
    "            elbow_idx = np.argmin(rate_changes) + 1  # +1 because diff reduces length by 1\n",
    "            # Ensure index is within valid range\n",
    "            elbow_idx = min(elbow_idx, len(k_range) - 1)\n",
    "            optimal_k_elbow = k_range[elbow_idx]\n",
    "        else:\n",
    "            # Fallback: use K with smallest decrease (conservative)\n",
    "            min_decrease_idx = np.argmin(wcss_decreases) + 1\n",
    "            min_decrease_idx = min(min_decrease_idx, len(k_range) - 1)\n",
    "            optimal_k_elbow = k_range[min_decrease_idx]\n",
    "    else:\n",
    "        # Fallback to silhouette if WCSS calculation fails\n",
    "        optimal_k_elbow = optimal_k_silhouette\n",
    "    \n",
    "    # Consensus voting: If 2+ methods agree, use that K\n",
    "    # Note: Multiple comparisons across 4 metrics and 5 K values (K=2-6) are exploratory\n",
    "    # We use consensus voting rather to find the optimal K\n",
    "  \n",
    "    k_votes = [optimal_k_silhouette, optimal_k_ch, optimal_k_db, optimal_k_elbow]\n",
    "    k_counts = Counter(k_votes)\n",
    "    most_common_k, consensus_count = k_counts.most_common(1)[0]\n",
    "    \n",
    "    if consensus_count >= 2:\n",
    "        # Consensus reached: 2+ methods agree\n",
    "        optimal_k = most_common_k\n",
    "        consensus_status = f\"Consensus: {consensus_count} methods agree on K={optimal_k}\"\n",
    "        consensus_reached = True\n",
    "    else:\n",
    "        # No consensus: fallback to silhouette (most interpretable)\n",
    "        optimal_k = optimal_k_silhouette\n",
    "        consensus_status = f\"No consensus (Sil={optimal_k_silhouette}, CH={optimal_k_ch}, DB={optimal_k_db}, Elbow={optimal_k_elbow}). Using silhouette K={optimal_k}\"\n",
    "        consensus_reached = False\n",
    "    \n",
    "    # Package all validation metrics for analysis\n",
    "    validation_metrics = {\n",
    "        'k_values': list(k_range),\n",
    "        'silhouette_scores': silhouette_scores_list,\n",
    "        'calinski_harabasz_scores': calinski_harabasz_scores,\n",
    "        'davies_bouldin_scores': davies_bouldin_scores,\n",
    "        'wcss_values': wcss_values,\n",
    "        'wcss_decreases': wcss_decreases if len(wcss_decreases) > 0 else [],\n",
    "        'optimal_k_silhouette': optimal_k_silhouette,\n",
    "        'optimal_k_ch': optimal_k_ch,\n",
    "        'optimal_k_db': optimal_k_db,\n",
    "        'optimal_k_elbow': optimal_k_elbow,\n",
    "        'optimal_k_consensus': optimal_k,\n",
    "        'consensus_reached': consensus_reached,\n",
    "        'consensus_status': consensus_status,\n",
    "        'k_votes': k_votes\n",
    "    }\n",
    "    \n",
    "    return train_losses, val_losses, optimal_k, best_silhouette_score, latent_vectors, validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef2e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOTE: This cell previously contained duplicate hyperparameter tuning code\n",
    "# that was causing NameError because kfold and train_val_tensor are only\n",
    "# defined inside run_autoencoder_pipeline() function scope.\n",
    "#\n",
    "# The hyperparameter tuning code is now properly contained within \n",
    "# run_autoencoder_pipeline() function.\n",
    "#\n",
    "# To run the pipeline, use:\n",
    "#   run_autoencoder_pipeline(\"D1-Swiss\")\n",
    "# ============================================================================\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# PCA Comparison: Justify Autoencoder Choice\n",
    "# =========================================================================\n",
    "\n",
    "# LEGACY VERSION (kept for reference):\n",
    "\"\"\"\n",
    "SWISS_DATASET = \"D1-Swiss\"\n",
    "\n",
    "if SWISS_DATASET not in PIPELINE_RESULTS:\n",
    "    print(f\"{SWISS_DATASET} not yet processed. Running autoencoder pipeline once for PCA comparison...\")\n",
    "    run_autoencoder_pipeline(SWISS_DATASET)\n",
    "\n",
    "swiss_results = PIPELINE_RESULTS[SWISS_DATASET]\n",
    "train_val_data = swiss_results['train_val_data']\n",
    "latent_vectors_all = swiss_results['latent_vectors_all']\n",
    "cluster_labels_all = swiss_results['cluster_labels_all']\n",
    "cluster_centroids = swiss_results['cluster_centroids']\n",
    "best_k = swiss_results['best_k']\n",
    "best_latent_dim = swiss_results['best_latent_dim']\n",
    "final_sil_score = swiss_results['final_sil_score']\n",
    "final_ch_score = swiss_results['final_ch_score']\n",
    "final_db_score = swiss_results['final_db_score']\n",
    "\n",
    "print(\"METHOD VALIDATION: PCA vs Autoencoder Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing if autoencoder captures nonlinear patterns better than PCA\")\n",
    "print(\"Testing PCA with all possible dimensions (1-4) to find optimal PCA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "scaler = StandardScaler()\n",
    "train_val_scaled = scaler.fit_transform(train_val_data)\n",
    "\n",
    "# Test PCA with all possible dimensions (1, 2, 3, 4)\n",
    "print(\"\\n1. Testing PCA with all dimensions:\")\n",
    "pca_results = []\n",
    "\n",
    "for pca_dim in range(1, 5):  # 1, 2, 3, 4\n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    pca_latent = pca.fit_transform(train_val_scaled)\n",
    "    explained_var = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "    # Cluster with optimal K (same as autoencoder)\n",
    "    pca_kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_SEED, n_init=10)\n",
    "    pca_cluster_labels = pca_kmeans.fit_predict(pca_latent)\n",
    "\n",
    "    # Evaluate clustering quality\n",
    "    pca_sil_score = silhouette_score(pca_latent, pca_cluster_labels)\n",
    "    pca_ch_score = calinski_harabasz_score(pca_latent, pca_cluster_labels)\n",
    "    pca_db_score = davies_bouldin_score(pca_latent, pca_cluster_labels)\n",
    "\n",
    "    pca_results.append({\n",
    "        'dim': pca_dim,\n",
    "        'explained_var': explained_var,\n",
    "        'silhouette': pca_sil_score,\n",
    "        'calinski_harabasz': pca_ch_score,\n",
    "        'davies_bouldin': pca_db_score\n",
    "    })\n",
    "\n",
    "    print(f\"   PCA dim={pca_dim}: Sil={pca_sil_score:.6f}, CH={pca_ch_score:.6f}, DB={pca_db_score:.6f}, Var={explained_var:.4f}\")\n",
    "\n",
    "# Find best PCA configuration (by silhouette score)\n",
    "pca_results_df = pd.DataFrame(pca_results)\n",
    "best_pca_idx = pca_results_df['silhouette'].idxmax()\n",
    "best_pca_config = pca_results_df.iloc[best_pca_idx]\n",
    "\n",
    "print(f\"\\n   Best PCA: dim={int(best_pca_config['dim'])}, Sil={best_pca_config['silhouette']:.6f}\")\n",
    "\n",
    "# Get best PCA latent vectors for visualization\n",
    "best_pca_dim = int(best_pca_config['dim'])\n",
    "best_pca = PCA(n_components=best_pca_dim)\n",
    "pca_latent_best = best_pca.fit_transform(train_val_scaled)\n",
    "pca_kmeans_best = KMeans(n_clusters=best_k, random_state=RANDOM_SEED, n_init=10)\n",
    "pca_cluster_labels_best = pca_kmeans_best.fit_predict(pca_latent_best)\n",
    "pca_centroids_best = pca_kmeans_best.cluster_centers_\n",
    "\n",
    "# Autoencoder results (from Cell 7)\n",
    "print(\"\\n2. Autoencoder (Nonlinear Method):\")\n",
    "print(f\"   AE latent dim: {best_latent_dim}\")\n",
    "print(f\"   AE latent vectors shape: {latent_vectors_all.shape}\")\n",
    "print(f\"   AE Clustering Quality:\")\n",
    "print(f\"     Silhouette Score: {final_sil_score:.6f}\")\n",
    "print(f\"     Calinski-Harabasz: {final_ch_score:.6f}\")\n",
    "print(f\"     Davies-Bouldin: {final_db_score:.6f}\")\n",
    "\n",
    "# Comparison: Best PCA vs Autoencoder\n",
    "print(\"\\n3. Comparison (Best PCA vs Autoencoder):\")\n",
    "best_pca_sil = best_pca_config['silhouette']\n",
    "best_pca_ch = best_pca_config['calinski_harabasz']\n",
    "best_pca_db = best_pca_config['davies_bouldin']\n",
    "\n",
    "sil_improvement = ((final_sil_score - best_pca_sil) / best_pca_sil) * 100\n",
    "ch_improvement = ((final_ch_score - best_pca_ch) / best_pca_ch) * 100\n",
    "db_improvement = ((best_pca_db - final_db_score) / best_pca_db) * 100  # DB: lower is better\n",
    "\n",
    "print(f\"   Silhouette: AE {final_sil_score:.6f} vs Best PCA {best_pca_sil:.6f} ({sil_improvement:+.2f}%)\")\n",
    "print(f\"   Calinski-Harabasz: AE {final_ch_score:.6f} vs Best PCA {best_pca_ch:.6f} ({ch_improvement:+.2f}%)\")\n",
    "print(f\"   Davies-Bouldin: AE {final_db_score:.6f} vs Best PCA {best_pca_db:.6f} ({db_improvement:+.2f}% improvement)\")\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\n4. Conclusion:\")\n",
    "if final_sil_score > best_pca_sil:\n",
    "    print(f\"   ✓ Autoencoder achieves {sil_improvement:.2f}% better silhouette score than best PCA\")\n",
    "    print(f\"   → Suggests latent structure contains nonlinear patterns\")\n",
    "    print(f\"   → Justifies use of autoencoder over linear PCA\")\n",
    "else:\n",
    "    print(f\"   Note: Best PCA (dim={best_pca_dim}) performs similarly ({best_pca_sil:.6f} vs {final_sil_score:.6f})\")\n",
    "    print(f\"   → Linear patterns may be sufficient, but AE provides flexibility\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Visualization: Compare latent spaces\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "if best_pca_dim == 2:\n",
    "    scatter1 = axes[0].scatter(pca_latent_best[:, 0], pca_latent_best[:, 1], c=pca_cluster_labels_best, \n",
    "                              cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[0].scatter(pca_centroids_best[:, 0], pca_centroids_best[:, 1], c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[0].set_xlabel('PC1', fontsize=12)\n",
    "    axes[0].set_ylabel('PC2', fontsize=12)\n",
    "    axes[0].set_title(f'Best PCA (dim={best_pca_dim}, Linear)\\nSilhouette: {best_pca_sil:.4f}', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    scatter2 = axes[1].scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], c=cluster_labels_all,\n",
    "                              cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[1].scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[1].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "    axes[1].set_ylabel('Latent Dim 2', fontsize=12)\n",
    "    axes[1].set_title(f'Autoencoder (dim={best_latent_dim}, Nonlinear)\\nSilhouette: {final_sil_score:.4f}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "elif best_pca_dim == 3 or best_latent_dim == 3:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "    if best_pca_dim >= 3:\n",
    "        ax1.scatter(pca_latent_best[:, 0], pca_latent_best[:, 1], pca_latent_best[:, 2], \n",
    "                   c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax1.scatter(pca_centroids_best[:, 0], pca_centroids_best[:, 1], pca_centroids_best[:, 2], \n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    else:\n",
    "        pca_3d = np.zeros((len(pca_latent_best), 3))\n",
    "        pca_3d[:, :best_pca_dim] = pca_latent_best\n",
    "        ax1.scatter(pca_3d[:, 0], pca_3d[:, 1], pca_3d[:, 2], \n",
    "                   c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "    ax1.set_title(f'Best PCA (dim={best_pca_dim})')\n",
    "\n",
    "    if best_latent_dim >= 3:\n",
    "        ax2.scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], latent_vectors_all[:, 2], \n",
    "                   c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax2.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], cluster_centroids[:, 2], \n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    else:\n",
    "        ae_3d = np.zeros((len(latent_vectors_all), 3))\n",
    "        ae_3d[:, :best_latent_dim] = latent_vectors_all\n",
    "        ax2.scatter(ae_3d[:, 0], ae_3d[:, 1], ae_3d[:, 2], \n",
    "                   c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax2.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], np.zeros(len(cluster_centroids)), \n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    ax2.set_xlabel('Latent Dim 1')\n",
    "    ax2.set_ylabel('Latent Dim 2')\n",
    "    ax2.set_zlabel('Latent Dim 3')\n",
    "    ax2.set_title(f'Autoencoder (dim={best_latent_dim})\\nSil: {final_sil_score:.4f} vs PCA: {best_pca_sil:.4f}')\n",
    "else:\n",
    "    scatter1 = axes[0].scatter(pca_latent_best[:, 0], np.zeros(len(pca_latent_best)), \n",
    "                              c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[0].scatter(pca_centroids_best[:, 0], np.zeros(len(pca_centroids_best)), \n",
    "                   c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[0].set_xlabel('PC1', fontsize=12)\n",
    "    axes[0].set_ylabel('(Projected)', fontsize=12)\n",
    "    axes[0].set_title(f'Best PCA (dim={best_pca_dim})', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    scatter2 = axes[1].scatter(latent_vectors_all[:, 0], np.zeros(len(latent_vectors_all)) if best_latent_dim == 1 else latent_vectors_all[:, 1], \n",
    "                              c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[1].scatter(cluster_centroids[:, 0], np.zeros(len(cluster_centroids)) if best_latent_dim == 1 else cluster_centroids[:, 1], \n",
    "                   c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[1].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "    axes[1].set_ylabel('Latent Dim 2' if best_latent_dim >= 2 else '(Projected)', fontsize=12)\n",
    "    axes[1].set_title(f'Autoencoder (dim={best_latent_dim})', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n5. Summary Table:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Best PCA', 'Autoencoder'],\n",
    "    'Latent_Dim': [best_pca_dim, best_latent_dim],\n",
    "    'Silhouette': [best_pca_sil, final_sil_score],\n",
    "    'Calinski_Harabasz': [best_pca_ch, final_ch_score],\n",
    "    'Davies_Bouldin': [best_pca_db, final_db_score]\n",
    "})\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "# ACTIVE VERSION (executes):\n",
    "SWISS_DATASET = \"D1-Swiss\"\n",
    "\n",
    "if SWISS_DATASET not in PIPELINE_RESULTS:\n",
    "    print(f\"{SWISS_DATASET} not yet processed. Running autoencoder pipeline once for PCA comparison...\")\n",
    "    run_autoencoder_pipeline(SWISS_DATASET)\n",
    "\n",
    "swiss_results = PIPELINE_RESULTS[SWISS_DATASET]\n",
    "train_val_data = swiss_results['train_val_data']\n",
    "latent_vectors_all = swiss_results['latent_vectors_all']\n",
    "cluster_labels_all = swiss_results['cluster_labels_all']\n",
    "cluster_centroids = swiss_results['cluster_centroids']\n",
    "best_k = swiss_results['best_k']\n",
    "best_latent_dim = swiss_results['best_latent_dim']\n",
    "final_sil_score = swiss_results['final_sil_score']\n",
    "final_ch_score = swiss_results['final_ch_score']\n",
    "final_db_score = swiss_results['final_db_score']\n",
    "\n",
    "print(\"METHOD VALIDATION: PCA vs Autoencoder Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing if autoencoder captures nonlinear patterns better than PCA\")\n",
    "print(\"Testing PCA with all possible dimensions (1-4) to find optimal PCA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "scaler = StandardScaler()\n",
    "train_val_scaled = scaler.fit_transform(train_val_data)\n",
    "\n",
    "# Test PCA with all possible dimensions (1, 2, 3, 4)\n",
    "print(\"\\n1. Testing PCA with all dimensions:\")\n",
    "pca_results = []\n",
    "\n",
    "for pca_dim in range(1, 5):  # 1, 2, 3, 4\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    pca_latent = pca.fit_transform(train_val_scaled)\n",
    "    explained_var = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "    pca_kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_SEED, n_init=10)\n",
    "    pca_cluster_labels = pca_kmeans.fit_predict(pca_latent)\n",
    "\n",
    "    pca_sil_score = silhouette_score(pca_latent, pca_cluster_labels)\n",
    "    pca_ch_score = calinski_harabasz_score(pca_latent, pca_cluster_labels)\n",
    "    pca_db_score = davies_bouldin_score(pca_latent, pca_cluster_labels)\n",
    "\n",
    "    pca_results.append({\n",
    "        'dim': pca_dim,\n",
    "        'explained_var': explained_var,\n",
    "        'silhouette': pca_sil_score,\n",
    "        'calinski_harabasz': pca_ch_score,\n",
    "        'davies_bouldin': pca_db_score\n",
    "    })\n",
    "\n",
    "    print(f\"   PCA dim={pca_dim}: Sil={pca_sil_score:.6f}, CH={pca_ch_score:.6f}, DB={pca_db_score:.6f}, Var={explained_var:.4f}\")\n",
    "\n",
    "pca_results_df = pd.DataFrame(pca_results)\n",
    "best_pca_idx = pca_results_df['silhouette'].idxmax()\n",
    "best_pca_config = pca_results_df.iloc[best_pca_idx]\n",
    "\n",
    "print(f\"\\n   Best PCA: dim={int(best_pca_config['dim'])}, Sil={best_pca_config['silhouette']:.6f}\")\n",
    "\n",
    "best_pca_dim = int(best_pca_config['dim'])\n",
    "best_pca = PCA(n_components=best_pca_dim)\n",
    "pca_latent_best = best_pca.fit_transform(train_val_scaled)\n",
    "pca_kmeans_best = KMeans(n_clusters=best_k, random_state=RANDOM_SEED, n_init=10)\n",
    "pca_cluster_labels_best = pca_kmeans_best.fit_predict(pca_latent_best)\n",
    "pca_centroids_best = pca_kmeans_best.cluster_centers_\n",
    "\n",
    "print(\"\\n2. Autoencoder (Nonlinear Method):\")\n",
    "print(f\"   AE latent dim: {best_latent_dim}\")\n",
    "print(f\"   AE latent vectors shape: {latent_vectors_all.shape}\")\n",
    "print(\"   AE Clustering Quality:\")\n",
    "print(f\"     Silhouette Score: {final_sil_score:.6f}\")\n",
    "print(f\"     Calinski-Harabasz: {final_ch_score:.6f}\")\n",
    "print(f\"     Davies-Bouldin: {final_db_score:.6f}\")\n",
    "\n",
    "print(\"\\n3. Comparison (Best PCA vs Autoencoder):\")\n",
    "best_pca_sil = best_pca_config['silhouette']\n",
    "best_pca_ch = best_pca_config['calinski_harabasz']\n",
    "best_pca_db = best_pca_config['davies_bouldin']\n",
    "\n",
    "sil_improvement = ((final_sil_score - best_pca_sil) / best_pca_sil) * 100\n",
    "ch_improvement = ((final_ch_score - best_pca_ch) / best_pca_ch) * 100\n",
    "db_improvement = ((best_pca_db - final_db_score) / best_pca_db) * 100\n",
    "\n",
    "print(f\"   Silhouette: AE {final_sil_score:.6f} vs Best PCA {best_pca_sil:.6f} ({sil_improvement:+.2f}%)\")\n",
    "print(f\"   Calinski-Harabasz: AE {final_ch_score:.6f} vs Best PCA {best_pca_ch:.6f} ({ch_improvement:+.2f}%)\")\n",
    "print(f\"   Davies-Bouldin: AE {final_db_score:.6f} vs Best PCA {best_pca_db:.6f} ({db_improvement:+.2f}% improvement)\")\n",
    "\n",
    "print(\"\\n4. Conclusion:\")\n",
    "if final_sil_score > best_pca_sil:\n",
    "    print(f\"   ✓ Autoencoder achieves {sil_improvement:.2f}% better silhouette score than best PCA\")\n",
    "    print(f\"   → Suggests latent structure contains nonlinear patterns\")\n",
    "    print(f\"   → Justifies use of autoencoder over linear PCA\")\n",
    "else:\n",
    "    print(f\"   Note: Best PCA (dim={best_pca_dim}) performs similarly ({best_pca_sil:.6f} vs {final_sil_score:.6f})\")\n",
    "    print(f\"   → Linear patterns may be sufficient, but AE provides flexibility\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "if best_pca_dim == 2:\n",
    "    scatter1 = axes[0].scatter(pca_latent_best[:, 0], pca_latent_best[:, 1], c=pca_cluster_labels_best,\n",
    "                              cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[0].scatter(pca_centroids_best[:, 0], pca_centroids_best[:, 1], c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[0].set_xlabel('PC1', fontsize=12)\n",
    "    axes[0].set_ylabel('PC2', fontsize=12)\n",
    "    axes[0].set_title(f'Best PCA (dim={best_pca_dim}, Linear)\\nSilhouette: {best_pca_sil:.4f}', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    scatter2 = axes[1].scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], c=cluster_labels_all,\n",
    "                              cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[1].scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[1].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "    axes[1].set_ylabel('Latent Dim 2', fontsize=12)\n",
    "    axes[1].set_title(f'Autoencoder (dim={best_latent_dim}, Nonlinear)\\nSilhouette: {final_sil_score:.4f}', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "elif best_pca_dim == 3 or best_latent_dim == 3:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "    if best_pca_dim >= 3:\n",
    "        ax1.scatter(pca_latent_best[:, 0], pca_latent_best[:, 1], pca_latent_best[:, 2],\n",
    "                   c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax1.scatter(pca_centroids_best[:, 0], pca_centroids_best[:, 1], pca_centroids_best[:, 2],\n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    else:\n",
    "        pca_3d = np.zeros((len(pca_latent_best), 3))\n",
    "        pca_3d[:, :best_pca_dim] = pca_latent_best\n",
    "        ax1.scatter(pca_3d[:, 0], pca_3d[:, 1], pca_3d[:, 2],\n",
    "                   c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "    ax1.set_title(f'Best PCA (dim={best_pca_dim})')\n",
    "\n",
    "    if best_latent_dim >= 3:\n",
    "        ax2.scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], latent_vectors_all[:, 2],\n",
    "                   c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax2.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], cluster_centroids[:, 2],\n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    else:\n",
    "        ae_3d = np.zeros((len(latent_vectors_all), 3))\n",
    "        ae_3d[:, :best_latent_dim] = latent_vectors_all\n",
    "        ax2.scatter(ae_3d[:, 0], ae_3d[:, 1], ae_3d[:, 2],\n",
    "                   c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax2.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], np.zeros(len(cluster_centroids)),\n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    ax2.set_xlabel('Latent Dim 1')\n",
    "    ax2.set_ylabel('Latent Dim 2')\n",
    "    ax2.set_zlabel('Latent Dim 3')\n",
    "    ax2.set_title(f'Autoencoder (dim={best_latent_dim})\\nSil: {final_sil_score:.4f} vs PCA: {best_pca_sil:.4f}')\n",
    "else:\n",
    "    scatter1 = axes[0].scatter(pca_latent_best[:, 0], np.zeros(len(pca_latent_best)),\n",
    "                              c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[0].scatter(pca_centroids_best[:, 0], np.zeros(len(pca_centroids_best)),\n",
    "                   c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[0].set_xlabel('PC1', fontsize=12)\n",
    "    axes[0].set_ylabel('(Projected)', fontsize=12)\n",
    "    axes[0].set_title(f'Best PCA (dim={best_pca_dim})', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    scatter2 = axes[1].scatter(latent_vectors_all[:, 0], np.zeros(len(latent_vectors_all)) if best_latent_dim == 1 else latent_vectors_all[:, 1],\n",
    "                              c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[1].scatter(cluster_centroids[:, 0], np.zeros(len(cluster_centroids)) if best_latent_dim == 1 else cluster_centroids[:, 1],\n",
    "                   c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[1].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "    axes[1].set_ylabel('Latent Dim 2' if best_latent_dim >= 2 else '(Projected)', fontsize=12)\n",
    "    axes[1].set_title(f'Autoencoder (dim={best_latent_dim})', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n5. Summary Table:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Best PCA', 'Autoencoder'],\n",
    "    'Latent_Dim': [best_pca_dim, best_latent_dim],\n",
    "    'Silhouette': [best_pca_sil, final_sil_score],\n",
    "    'Calinski_Harabasz': [best_pca_ch, final_ch_score],\n",
    "    'Davies_Bouldin': [best_pca_db, final_db_score]\n",
    "})\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# REPLICATION TESTING: H1/H2 Hypotheses (D1-Swiss as Reference)\n",
    "# =========================================================================\n",
    "# Only matches latent_dim (required for comparison), K can differ\n",
    "# Reuses existing run_autoencoder_pipeline() function\n",
    "# =========================================================================\n",
    "\n",
    "# Additional imports (already imported in Cell 0, but included for clarity)\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Configuration constants\n",
    "EXTERNAL_DATASETS = [\"D2-Cultural\", \"D3-Academic\", \"D4-Tech\"]\n",
    "REPLICATION_THRESHOLD_H1 = 0.70\n",
    "REPLICATION_THRESHOLD_H2 = 0.50\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REPLICATION TESTING: Testing Profile Generalizability\")\n",
    "print(\"=\"*80)\n",
    "print(\"Hypothesis H1: Universal profiles (r > 0.70)\")\n",
    "print(\"Hypothesis H2: Context-specific profiles (r < 0.50)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Reference: D1-Swiss (only latent_dim must match for comparison)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 1: PROCESS D1-SWISS (REFERENCE DATASET)\n",
    "# ========================================================================\n",
    "print(\"\\nStep 1: Processing D1-Swiss (reference dataset)...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if \"D1-Swiss\" not in PIPELINE_RESULTS:\n",
    "    print(\"Running pipeline on D1-Swiss...\")\n",
    "    run_autoencoder_pipeline(\"D1-Swiss\")\n",
    "else:\n",
    "    print(\"D1-Swiss already processed ✓\")\n",
    "\n",
    "d1_results = PIPELINE_RESULTS[\"D1-Swiss\"]\n",
    "d1_latent_dim = d1_results['best_latent_dim']\n",
    "d1_k = d1_results['best_k']\n",
    "d1_centroids = d1_results['cluster_centroids']\n",
    "\n",
    "print(f\"\\nD1-Swiss Reference Configuration:\")\n",
    "print(f\"  Optimal K: {d1_k}\")\n",
    "print(f\"  Latent dimension: {d1_latent_dim}D\")\n",
    "print(f\"  Centroids shape: {d1_centroids.shape}\")\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 2: PROCESS EXTERNAL DATASETS AND RETUNE IF NEEDED\n",
    "# ========================================================================\n",
    "print(f\"\\nStep 2: Processing external datasets and retuning to {d1_latent_dim}D if needed...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "standardized_results = {}\n",
    "\n",
    "for ext_dataset in EXTERNAL_DATASETS:\n",
    "    print(f\"\\nProcessing {ext_dataset}...\")\n",
    "    \n",
    "    # Run pipeline if not already processed\n",
    "    if ext_dataset not in PIPELINE_RESULTS:\n",
    "        print(f\"  Running pipeline on {ext_dataset}...\")\n",
    "        run_autoencoder_pipeline(ext_dataset)\n",
    "    \n",
    "    ext_result = PIPELINE_RESULTS[ext_dataset]\n",
    "    ext_latent_dim = ext_result['best_latent_dim']\n",
    "    \n",
    "    print(f\"  Original latent dimensions: {ext_latent_dim}D\")\n",
    "    \n",
    "    if ext_latent_dim == d1_latent_dim:\n",
    "        print(f\"  ✓ Latent dimension matches D1-Swiss ({d1_latent_dim}D)\")\n",
    "        standardized_results[ext_dataset] = ext_result.copy()\n",
    "    else:\n",
    "        print(f\"  ⚠ Dimension mismatch: {ext_latent_dim}D ≠ {d1_latent_dim}D\")\n",
    "        print(f\"  → Retuning to match D1-Swiss latent_dim ({d1_latent_dim}D)...\")\n",
    "        print(f\"    (K can differ - only latent_dim needs to match for comparison)\")\n",
    "        \n",
    "        retrained_result = run_autoencoder_pipeline(\n",
    "            ext_dataset, \n",
    "            force_latent_dim=d1_latent_dim\n",
    "        )\n",
    "        \n",
    "        original_recon_loss = ext_result.get('reconstruction_loss', None)\n",
    "        \n",
    "        if original_recon_loss is not None:\n",
    "            retrained_recon_loss = retrained_result.get('reconstruction_loss', None)\n",
    "            \n",
    "            if retrained_recon_loss is not None:\n",
    "                if original_recon_loss != 0:\n",
    "                    info_loss_pct = (\n",
    "                        (retrained_recon_loss - original_recon_loss) / original_recon_loss\n",
    "                    ) * 100\n",
    "                    \n",
    "                    print(f\"\\n  Information Loss Analysis ({ext_latent_dim}D → {d1_latent_dim}D):\")\n",
    "                    print(f\"    Original recon loss ({ext_latent_dim}D): {original_recon_loss:.6f}\")\n",
    "                    print(f\"    Retuned recon loss ({d1_latent_dim}D): {retrained_recon_loss:.6f}\")\n",
    "                    print(f\"    Increase: {info_loss_pct:+.2f}% (higher = more information lost)\")\n",
    "                    \n",
    "                    retrained_result['info_loss_pct'] = info_loss_pct\n",
    "                    retrained_result['original_recon_loss'] = original_recon_loss\n",
    "                else:\n",
    "                    print(f\"\\n  Information Loss Analysis ({ext_latent_dim}D → {d1_latent_dim}D):\")\n",
    "                    print(f\"    Original recon loss ({ext_latent_dim}D): {original_recon_loss:.6f}\")\n",
    "                    print(f\"    Retuned recon loss ({d1_latent_dim}D): {retrained_recon_loss:.6f}\")\n",
    "                    print(f\"    Note: Cannot calculate percentage change (original loss is 0)\")\n",
    "        \n",
    "        standardized_results[ext_dataset] = retrained_result\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 3: COMPARE CENTROIDS ACROSS DATASETS\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Step 3: Comparing profiles across datasets (all in {d1_latent_dim}D space)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "replication_results = []\n",
    "\n",
    "for ext_dataset in EXTERNAL_DATASETS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Comparing {ext_dataset} profiles to D1-Swiss\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    ext_result = standardized_results[ext_dataset]\n",
    "    ext_centroids = ext_result['cluster_centroids']\n",
    "    ext_k = ext_result['best_k']\n",
    "    \n",
    "    print(f\"  K: {ext_k} (D1-Swiss K: {d1_k})\")\n",
    "    print(f\"  Centroids shape: {ext_centroids.shape}\")\n",
    "    \n",
    "    # Re-cluster if K differs\n",
    "    if ext_k != d1_k:\n",
    "        print(f\"  Re-clustering with K={d1_k} to match D1-Swiss for comparison...\")\n",
    "        ext_latent_vectors = ext_result['latent_vectors_all']\n",
    "        ext_kmeans = KMeans(n_clusters=d1_k, random_state=RANDOM_SEED, n_init=10)\n",
    "        ext_cluster_labels = ext_kmeans.fit_predict(ext_latent_vectors)\n",
    "        ext_centroids = ext_kmeans.cluster_centers_\n",
    "        ext_k = d1_k\n",
    "    \n",
    "    # Match centroids using Hungarian Algorithm\n",
    "    distance_matrix = cdist(d1_centroids, ext_centroids, metric='euclidean')\n",
    "    row_indices, col_indices = linear_sum_assignment(distance_matrix)\n",
    "    ext_centroids_matched = ext_centroids[col_indices]\n",
    "    \n",
    "    # Calculate dimension-wise correlations\n",
    "    # NOTE: Sample size is K (number of clusters), typically 2-6, which is small for correlation\n",
    "    # P-values should be interpreted with caution. Correlation coefficient is the primary metric.\n",
    "    dim_correlations = []\n",
    "    dim_p_values = []\n",
    "    \n",
    "    for dim in range(d1_latent_dim):\n",
    "        d1_dim = d1_centroids[:, dim]\n",
    "        ext_dim = ext_centroids_matched[:, dim]\n",
    "        correlation, p_value = pearsonr(d1_dim, ext_dim)\n",
    "        dim_correlations.append(correlation)\n",
    "        dim_p_values.append(p_value)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_correlation = np.mean(dim_correlations)\n",
    "    \n",
    "    # Use pearsonr for overall correlation (consistent with dimension-wise calculation)\n",
    "    # NOTE: Sample size = K × latent_dim, still small but better than dimension-wise\n",
    "    d1_flat = d1_centroids.flatten()\n",
    "    ext_flat = ext_centroids_matched.flatten()\n",
    "    overall_correlation, overall_p_value = pearsonr(d1_flat, ext_flat)\n",
    "    \n",
    "    print(f\"\\n  Replication scores:\")\n",
    "    print(f\"    ⚠ Note: Correlation computed on {d1_k} centroids (n={d1_k})\")\n",
    "    print(f\"    ⚠ P-values should be interpreted with caution due to small sample size\")\n",
    "    for dim in range(d1_latent_dim):\n",
    "        print(f\"    Dim {dim+1} correlation: {dim_correlations[dim]:.4f} (p={dim_p_values[dim]:.6f})\")\n",
    "    print(f\"    Average correlation: {avg_correlation:.4f}\")\n",
    "    print(f\"    Overall correlation: {overall_correlation:.4f} (p={overall_p_value:.6f}, n={len(d1_flat)})\")\n",
    "    \n",
    "    # Test hypotheses\n",
    "    h1_supported = overall_correlation > REPLICATION_THRESHOLD_H1\n",
    "    h2_supported = overall_correlation < REPLICATION_THRESHOLD_H2\n",
    "    \n",
    "    print(f\"\\n  Hypothesis Testing:\")\n",
    "    print(f\"    H1: {'Supported' if h1_supported else 'Not Supported'}\")\n",
    "    print(f\"    H2: {'Supported' if h2_supported else 'Not Supported'}\")\n",
    "    \n",
    "    if h1_supported:\n",
    "        print(f\"    → Conclusion: Profiles are UNIVERSAL across D1-Swiss and {ext_dataset}\")\n",
    "    elif h2_supported:\n",
    "        print(f\"    → Conclusion: Profiles are CONTEXT-SPECIFIC (different patterns)\")\n",
    "    else:\n",
    "        print(f\"    → Conclusion: Moderate replication (0.50 ≤ r ≤ 0.70)\")\n",
    "    \n",
    "    replication_results.append({\n",
    "        'dataset': ext_dataset,\n",
    "        'replication_score': overall_correlation,\n",
    "        'avg_dim_correlation': avg_correlation,\n",
    "        'p_value': overall_p_value,\n",
    "        'h1_supported': h1_supported,\n",
    "        'h2_supported': h2_supported,\n",
    "        'info_loss_pct': ext_result.get('info_loss_pct', None),\n",
    "    })\n",
    "    \n",
    "    # ====================================================================\n",
    "    # VISUALIZATION\n",
    "    # ====================================================================\n",
    "    if d1_latent_dim == 2:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # D1-Swiss centroids\n",
    "        axes[0].scatter(\n",
    "            d1_centroids[:, 0],\n",
    "            d1_centroids[:, 1],\n",
    "            c=range(d1_k),\n",
    "            cmap='viridis',\n",
    "            s=200,\n",
    "            edgecolors='black',\n",
    "            linewidth=2,\n",
    "            marker='o',\n",
    "            label='D1-Swiss'\n",
    "        )\n",
    "        \n",
    "        for i, (x, y) in enumerate(d1_centroids):\n",
    "            axes[0].annotate(\n",
    "                f'P{i+1}',\n",
    "                (x, y),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontweight='bold'\n",
    "            )\n",
    "        \n",
    "        axes[0].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "        axes[0].set_ylabel('Latent Dim 2', fontsize=12)\n",
    "        axes[0].set_title('D1-Swiss Centroids (Reference)', fontsize=14, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # External dataset centroids\n",
    "        axes[1].scatter(\n",
    "            ext_centroids_matched[:, 0],\n",
    "            ext_centroids_matched[:, 1],\n",
    "            c=range(ext_k),\n",
    "            cmap='viridis',\n",
    "            s=200,\n",
    "            edgecolors='red',\n",
    "            linewidth=2,\n",
    "            marker='s',\n",
    "            label=ext_dataset\n",
    "        )\n",
    "        \n",
    "        for i, (x, y) in enumerate(ext_centroids_matched):\n",
    "            axes[1].annotate(\n",
    "                f'P{i+1}',\n",
    "                (x, y),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontweight='bold'\n",
    "            )\n",
    "        \n",
    "        axes[1].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "        axes[1].set_ylabel('Latent Dim 2', fontsize=12)\n",
    "        axes[1].set_title(\n",
    "            f'{ext_dataset} Centroids (Matched)\\nr = {overall_correlation:.4f}',\n",
    "            fontsize=14,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    elif d1_latent_dim == 3:\n",
    "        fig = plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        ax1 = fig.add_subplot(121, projection='3d')\n",
    "        ax1.scatter(\n",
    "            d1_centroids[:, 0],\n",
    "            d1_centroids[:, 1],\n",
    "            d1_centroids[:, 2],\n",
    "            c=range(d1_k),\n",
    "            cmap='viridis',\n",
    "            s=200,\n",
    "            edgecolors='black',\n",
    "            linewidth=2\n",
    "        )\n",
    "        ax1.set_xlabel('Latent Dim 1')\n",
    "        ax1.set_ylabel('Latent Dim 2')\n",
    "        ax1.set_zlabel('Latent Dim 3')\n",
    "        ax1.set_title('D1-Swiss Centroids (Reference)')\n",
    "        \n",
    "        ax2 = fig.add_subplot(122, projection='3d')\n",
    "        ax2.scatter(\n",
    "            ext_centroids_matched[:, 0],\n",
    "            ext_centroids_matched[:, 1],\n",
    "            ext_centroids_matched[:, 2],\n",
    "            c=range(ext_k),\n",
    "            cmap='viridis',\n",
    "            s=200,\n",
    "            edgecolors='red',\n",
    "            linewidth=2\n",
    "        )\n",
    "        ax2.set_xlabel('Latent Dim 1')\n",
    "        ax2.set_ylabel('Latent Dim 2')\n",
    "        ax2.set_zlabel('Latent Dim 3')\n",
    "        ax2.set_title(f'{ext_dataset} Centroids\\nr = {overall_correlation:.4f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ========================================================================\n",
    "# SUMMARY AND CONCLUSIONS\n",
    "# ========================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"REPLICATION TESTING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n⚠ METHODOLOGICAL NOTES:\")\n",
    "print(\"  - Correlation computed on centroids (n=K, typically 2-6)\")\n",
    "print(\"  - P-values should be interpreted with caution due to small sample size\")\n",
    "print(\"  - Correlation coefficient (r) is the primary metric for hypothesis testing\")\n",
    "print(\"  - Forced K matching: External datasets re-clustered with D1's K for comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "replication_df = pd.DataFrame(replication_results)\n",
    "\n",
    "display_cols = [\n",
    "    'dataset',\n",
    "    'replication_score',\n",
    "    'avg_dim_correlation',\n",
    "    'h1_supported',\n",
    "    'h2_supported'\n",
    "]\n",
    "\n",
    "if any(replication_df['info_loss_pct'].notna()):\n",
    "    display_cols.append('info_loss_pct')\n",
    "\n",
    "print(\"\\nResults Table:\")\n",
    "print(replication_df[display_cols].to_string(index=False))\n",
    "\n",
    "h1_count = sum(replication_df['h1_supported'] == True)\n",
    "h2_count = sum(replication_df['h2_supported'] == True)\n",
    "total_tested = len(replication_df)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OVERALL CONCLUSION:\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Reference: D1-Swiss ({d1_latent_dim}D, K={d1_k})\")\n",
    "print(f\"All datasets standardized to: {d1_latent_dim}D (K can differ)\")\n",
    "print(f\"Datasets tested: {total_tested}\")\n",
    "print(f\"H1 (Universal) supported: {h1_count}/{total_tested}\")\n",
    "print(f\"H2 (Contextual) supported: {h2_count}/{total_tested}\")\n",
    "\n",
    "if h1_count >= 2:\n",
    "    print(\"\\n✓ H1 VALIDATED: Profiles appear to be UNIVERSAL across datasets\")\n",
    "    print(\"  → Mental health profiles are consistent across different populations\")\n",
    "elif h2_count >= 2:\n",
    "    print(\"\\n✓ H2 VALIDATED: Profiles appear to be CONTEXT-SPECIFIC\")\n",
    "    print(\"  → Mental health profiles vary by population/culture\")\n",
    "else:\n",
    "    print(\"\\n→ Mixed results: Profiles show moderate replication\")\n",
    "    print(\"  → May depend on specific dataset characteristics\")\n",
    "\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# OPTIONAL: Process All Datasets Automatically\n",
    "# =========================================================================\n",
    "# Uncomment the function call below to process all datasets through\n",
    "# the autoencoder pipeline automatically\n",
    "# =========================================================================\n",
    "\n",
    "def run_all_datasets():\n",
    "    \"\"\"\n",
    "    Process all datasets through the autoencoder pipeline.\n",
    "    \n",
    "    This function iterates through all datasets defined in DATASETS\n",
    "    and runs the complete pipeline (hyperparameter tuning + profile extraction)\n",
    "    for each one.\n",
    "    \"\"\"\n",
    "    for dataset_name in DATASETS:\n",
    "        print(f\"\\n{'#'*80}\\nProcessing {dataset_name}\\n{'#'*80}\\n\")\n",
    "        run_autoencoder_pipeline(dataset_name)\n",
    "\n",
    "# Uncomment the line below to run:\n",
    "# run_all_datasets()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
