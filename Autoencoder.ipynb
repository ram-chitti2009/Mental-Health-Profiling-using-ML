{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 0: SETUP AND CONFIGURATION\n",
    "# ============================================================================\n",
    "# Purpose: Initialize environment, set reproducibility seeds, define constants\n",
    "# Key components:\n",
    "#   - Library imports\n",
    "#   - Random seed configuration for reproducibility\n",
    "#   - Dataset paths and feature definitions\n",
    "#   - Device selection (GPU/CPU)\n",
    "#   - Data preparation function\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from scipy.stats import chi2, chi2_contingency, pearsonr\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "FEATURE_COLUMNS = [\"Depression\", \"Anxiety\", \"Stress\", \"Burnout\"]\n",
    "DATASETS = {\n",
    "    \"D1-Swiss\": Path(\"D1_Swiss_processed.csv\"),\n",
    "    \"D2-Cultural\": Path(\"D2_Cultural_processed.csv\"),\n",
    "    \"D3-Academic\": Path(\"D3_Academic_processed.csv\"),\n",
    "    \"D4-Tech\": Path(\"D4_Tech_processed.csv\"),\n",
    "}\n",
    "# Optimal bin numbers for stratified splitting (from grid search)\n",
    "STRATIFICATION_BINS = {\n",
    "    \"D1-Swiss\": 2,\n",
    "    \"D2-Cultural\": 2,\n",
    "    \"D3-Academic\": 4,\n",
    "    \"D4-Tech\": 2,\n",
    "}\n",
    "PIPELINE_RESULTS = {}\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "def prepare_dataset(dataset_name: str):\n",
    "    if dataset_name not in DATASETS:\n",
    "        raise ValueError(f\"Unknown dataset '{dataset_name}'. Available: {list(DATASETS.keys())}\")\n",
    "\n",
    "    dataset_path = DATASETS[dataset_name]\n",
    "    print(f\"\\n=== Loading {dataset_name} dataset ===\")\n",
    "    print(f\"File: {dataset_path}\")\n",
    "    print(f\"Features: {FEATURE_COLUMNS}\")\n",
    "\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    missing_cols = [col for col in FEATURE_COLUMNS if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns in {dataset_path}: {missing_cols}\")\n",
    "    \n",
    "    # Check if dataset is empty\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(f\"Dataset {dataset_path} is empty.\")\n",
    "    \n",
    "    feature_matrix = df[FEATURE_COLUMNS].values\n",
    "    print(f\"Dataset: {feature_matrix.shape[0]} samples, {feature_matrix.shape[1]} features\")\n",
    "    \n",
    "    # Check if we have enough samples for train/test split\n",
    "    if len(feature_matrix) < 10:\n",
    "        raise ValueError(f\"Dataset too small ({len(feature_matrix)} samples). Need at least 10 samples.\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STRATIFIED SPLIT: Use binned features for stratification\n",
    "    # ========================================================================\n",
    "    # Create bins for each continuous feature to enable stratification\n",
    "    # This ensures train/test sets have similar distributions\n",
    "    # ========================================================================\n",
    "    try:\n",
    "        n_bins = STRATIFICATION_BINS.get(dataset_name, 2)  # Default to 2 bins if not specified\n",
    "        \n",
    "        # Create bins for each feature (using quantiles)\n",
    "        df_binned = df.copy()\n",
    "        for col in FEATURE_COLUMNS:\n",
    "            # Use quantiles to create bins, handle duplicates\n",
    "            df_binned[f'{col}_bin'] = pd.qcut(df[col], q=n_bins, labels=False, duplicates='drop')\n",
    "        \n",
    "        # Create stratification label (combination of all feature bins)\n",
    "        df_binned['stratify_label'] = df_binned[[f'{col}_bin' for col in FEATURE_COLUMNS]].apply(\n",
    "            lambda x: '_'.join(x.astype(str)), axis=1\n",
    "        )\n",
    "        \n",
    "        # Check if we have enough samples per stratum for stratification\n",
    "        stratum_counts = df_binned['stratify_label'].value_counts()\n",
    "        min_stratum_size = stratum_counts.min()\n",
    "        \n",
    "        if min_stratum_size < 2:\n",
    "            raise ValueError(f\"Some strata have < 2 samples (min = {min_stratum_size}), cannot stratify\")\n",
    "        \n",
    "        # Perform stratified split\n",
    "        train_val_data, test_data = train_test_split(\n",
    "            feature_matrix, \n",
    "            test_size=0.2, \n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=df_binned['stratify_label']\n",
    "        )\n",
    "        print(f\"✓ Using STRATIFIED split with {n_bins} bins per feature (ensures balanced train/test distributions)\")\n",
    "        \n",
    "    except (ValueError, KeyError) as e:\n",
    "        # Fallback to regular split if stratification fails\n",
    "        print(f\"⚠ Stratification failed ({str(e)}), using regular split\")\n",
    "        train_val_data, test_data = train_test_split(\n",
    "            feature_matrix, \n",
    "            test_size=0.2, \n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "\n",
    "    train_val_tensor = torch.tensor(train_val_data, dtype=torch.float32)\n",
    "    test_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    print(f\"Train+Val: {train_val_tensor.shape[0]} samples (80%)\")\n",
    "    print(f\"Test: {test_tensor.shape[0]} samples (20%)\")\n",
    "    print(f\"K-Fold: 10 folds, ~{train_val_tensor.shape[0]//10} samples per fold\\n\")\n",
    "\n",
    "    return (\n",
    "        df,\n",
    "        feature_matrix,\n",
    "        train_val_data,\n",
    "        test_data,\n",
    "        train_val_tensor,\n",
    "        test_tensor,\n",
    "        kfold,\n",
    "        dataset_path,\n",
    "    )\n",
    "\n",
    "INPUT_DIM = len(FEATURE_COLUMNS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7d410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: AUTOENCODER ARCHITECTURE DEFINITION\n",
    "# ============================================================================\n",
    "# Purpose: Define the neural network that compresses 4D symptoms into\n",
    "#          lower-dimensional latent space for clustering\n",
    "# Architecture: Symmetric encoder-decoder with configurable dimensions\n",
    "# ============================================================================\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, activation_function):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        #Encoder - input -> hidden -> latent\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_function(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "\n",
    "        #Decoder - latent -> hidden -> output\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            activation_function(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f87bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: MAIN PIPELINE - Hyperparameter Tuning and Profile Extraction\n",
    "# ============================================================================\n",
    "# Purpose: Two-stage hyperparameter optimization followed by profile extraction\n",
    "# Stage 1: Architecture tuning (hidden_size, latent_dim, activation, optimizer)\n",
    "# Stage 2: Learning parameter tuning (batch_size, weight_decay, momentum)\n",
    "# Final: Train on all data, extract profiles, evaluate on test set\n",
    "# ============================================================================\n",
    "\n",
    "def run_autoencoder_pipeline(dataset_name: str, force_latent_dim: int = None, force_k: int = None, narrowed_params: dict = None, n_folds: int = 5):\n",
    "    (\n",
    "        all_data_df,\n",
    "        all_data,\n",
    "        train_val_data,\n",
    "        test_data,\n",
    "        train_val_tensor,\n",
    "        test_tensor,\n",
    "        kfold,\n",
    "        dataset_path,\n",
    "    ) = prepare_dataset(dataset_name)\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # STAGE 1: Architecture Parameter Tuning\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Grid search over architecture parameters with 10-fold cross-validation\n",
    "    # Selection criterion: Consensus voting on clustering quality metrics\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"STAGE 1: Architecture Parameters Tuning (K-Fold CV)\")\n",
    "    \n",
    "    # Require narrowed params from Optuna scout phase\n",
    "    if not narrowed_params:\n",
    "        raise ValueError(\n",
    "            \"narrowed_params is required! Run the Optuna scout phase first to generate narrowed parameters.\\n\"\n",
    "            \"Usage: run_scout_phase() → extract_narrowed_ranges() → run_autoencoder_pipeline(narrowed_params=...)\"\n",
    "        )\n",
    "    \n",
    "    print(\"Using NARROWED parameter ranges from Optuna scout phase\")\n",
    "    hidden_sizes = narrowed_params.get(\"hidden_sizes\", [2, 3, 4, 5, 6, 8, 10])\n",
    "    latent_dims = narrowed_params.get(\"latent_dims\", [2, 3, 4])\n",
    "    \n",
    "    all_activations = {\"ReLU\": nn.ReLU, \"Tanh\": nn.Tanh, \"Sigmoid\": nn.Sigmoid}\n",
    "    activations = {name: all_activations[name] for name in narrowed_params.get(\"activations\", [\"ReLU\", \"Tanh\", \"Sigmoid\"])}\n",
    "    \n",
    "    all_optimizers = {\"Adam\": optim.Adam, \"SGD\": optim.SGD}\n",
    "    optimizers = {name: all_optimizers[name] for name in narrowed_params.get(\"optimizers\", [\"Adam\", \"SGD\"])}\n",
    "    \n",
    "    epochs_list = narrowed_params.get(\"epochs\", [50, 75, 100])\n",
    "    fixed_lr = 1e-3\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    results_stage1 = defaultdict(list)\n",
    "\n",
    "    total_experiments = len(hidden_sizes) * len(latent_dims) * len(activations) * len(optimizers) * len(epochs_list) * n_folds\n",
    "    experiment_count = 0\n",
    "\n",
    "    print(f\"Testing: hidden_size, latent_dim, activation, optimizer, epochs\")\n",
    "    print(f\"Fixed: learning_rate = {fixed_lr}\")\n",
    "    print(f\"\\nGrid sizes:\")\n",
    "    print(f\"  hidden_size: {len(hidden_sizes)} values {hidden_sizes}\")\n",
    "    print(f\"  latent_dim: {len(latent_dims)} values {latent_dims}\")\n",
    "    print(f\"  activation: {len(activations)} values {list(activations.keys())}\")\n",
    "    print(f\"  optimizer: {len(optimizers)} values {list(optimizers.keys())}\")\n",
    "    print(f\"  epochs: {len(epochs_list)} values {epochs_list}\")\n",
    "    print(f\"  CV folds: {n_folds}\")\n",
    "    print(f\"Total: {total_experiments} experiments ({total_experiments//n_folds} configs × {n_folds} folds)\\n\")\n",
    "\n",
    "    start_stage1 = time.time()\n",
    "\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for latent_dim in latent_dims:\n",
    "            for act_name, act_fn in activations.items():\n",
    "                for opt_name, opt_class in optimizers.items():\n",
    "                    for num_epochs in epochs_list:\n",
    "                        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_val_tensor)):\n",
    "                            experiment_count += 1\n",
    "                            if experiment_count % 50 == 0 or experiment_count == total_experiments:\n",
    "                                elapsed = time.time() - start_stage1\n",
    "                                print(f\"  [{experiment_count}/{total_experiments}] {100*experiment_count/total_experiments:.1f}% - {elapsed/60:.1f}min\")\n",
    "                        \n",
    "                            fold_seed = 42 + fold_idx\n",
    "                            torch.manual_seed(fold_seed)\n",
    "                            np.random.seed(fold_seed)\n",
    "                        \n",
    "                            train_fold = train_val_tensor[train_idx]\n",
    "                            val_fold = train_val_tensor[val_idx]\n",
    "                        \n",
    "                            train_dataset = TensorDataset(train_fold.cpu())\n",
    "                            val_dataset = TensorDataset(val_fold.cpu())\n",
    "                            train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "                            val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "                        \n",
    "                            model = Autoencoder(INPUT_DIM, hidden_size, latent_dim, act_fn).to(device)\n",
    "                        \n",
    "                            if opt_name == 'Adam':\n",
    "                                optimizer = opt_class(model.parameters(), lr=fixed_lr)\n",
    "                            else:\n",
    "                                optimizer = opt_class(model.parameters(), lr=fixed_lr, momentum=0.9)\n",
    "                        \n",
    "                            train_losses, val_losses, optimal_k, best_sil_score, latent_vectors, validation_metrics = \\\n",
    "                                train_and_validate_model(model, train_loader, val_loader, optimizer, \n",
    "                                                        criterion, num_epochs, device)\n",
    "                        \n",
    "                            # Safely get index for optimal_k\n",
    "                            if optimal_k in validation_metrics['k_values']:\n",
    "                                optimal_k_idx = validation_metrics['k_values'].index(optimal_k)\n",
    "                                best_ch_score = validation_metrics['calinski_harabasz_scores'][optimal_k_idx]\n",
    "                                best_db_score = validation_metrics['davies_bouldin_scores'][optimal_k_idx]\n",
    "                            else:\n",
    "                                # Fallback: use first available K\n",
    "                                optimal_k_idx = 0\n",
    "                                best_ch_score = validation_metrics['calinski_harabasz_scores'][0] if len(validation_metrics['calinski_harabasz_scores']) > 0 else 0\n",
    "                                best_db_score = validation_metrics['davies_bouldin_scores'][0] if len(validation_metrics['davies_bouldin_scores']) > 0 else float('inf')\n",
    "                        \n",
    "                            results_stage1['hidden_size'].append(hidden_size)\n",
    "                            results_stage1['latent_dim'].append(latent_dim)\n",
    "                            results_stage1['activation'].append(act_name)\n",
    "                            results_stage1['optimizer'].append(opt_name)\n",
    "                            results_stage1['epochs'].append(num_epochs)\n",
    "                            results_stage1['fold'].append(fold_idx)\n",
    "                            results_stage1['optimal_k'].append(optimal_k)\n",
    "                            results_stage1['silhouette_score'].append(best_sil_score)\n",
    "                            results_stage1['calinski_harabasz_score'].append(best_ch_score)\n",
    "                            results_stage1['davies_bouldin_score'].append(best_db_score)\n",
    "                            results_stage1['reconstruction_loss'].append(val_losses[-1])\n",
    "                            results_stage1['consensus_reached'].append(validation_metrics['consensus_reached'])\n",
    "\n",
    "    print(f\"\\n✓ Stage 1 completed in {(time.time()-start_stage1)/60:.2f} minutes\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # STAGE 1: Results Aggregation and Best Configuration Selection\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Aggregate results across folds, rank by clustering metrics,\n",
    "    # use consensus voting to select best architecture\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"STAGE 1: Results Aggregation\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    stage1_df = pd.DataFrame(results_stage1)\n",
    "    \n",
    "    # Check if we have any results\n",
    "    if len(stage1_df) == 0:\n",
    "        raise ValueError(\"Stage 1 produced no results. Check training loop.\")\n",
    "\n",
    "    aggregated_stage1 = stage1_df.groupby(['hidden_size', 'latent_dim', 'activation', 'optimizer', 'epochs']).agg({\n",
    "        'silhouette_score': ['mean', 'std'],\n",
    "        'calinski_harabasz_score': ['mean', 'std'],\n",
    "        'davies_bouldin_score': ['mean', 'std'],\n",
    "        'reconstruction_loss': ['mean', 'std'],\n",
    "        'optimal_k': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    }).reset_index()\n",
    "\n",
    "    aggregated_stage1.columns = ['hidden_size', 'latent_dim', 'activation', 'optimizer', 'epochs',\n",
    "                                  'mean_silhouette', 'std_silhouette',\n",
    "                                  'mean_ch', 'std_ch',\n",
    "                                  'mean_db', 'std_db',\n",
    "                                  'mean_recon_loss', 'std_recon_loss',\n",
    "                                  'most_common_k']\n",
    "\n",
    "    # Create config_id BEFORE filtering\n",
    "    aggregated_stage1['config_id'] = aggregated_stage1.apply(\n",
    "        lambda row: f\"{row['hidden_size']}_{row['latent_dim']}_{row['activation']}_{row['optimizer']}_{row['epochs']}\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    aggregated_stage1['rank_silhouette'] = aggregated_stage1['mean_silhouette'].rank(ascending=False, method='min')\n",
    "    aggregated_stage1['rank_ch'] = aggregated_stage1['mean_ch'].rank(ascending=False, method='min')\n",
    "    aggregated_stage1['rank_db'] = aggregated_stage1['mean_db'].rank(ascending=True, method='min')\n",
    "\n",
    "    top_by_sil = aggregated_stage1.loc[aggregated_stage1['rank_silhouette'] == 1]\n",
    "    top_by_ch = aggregated_stage1.loc[aggregated_stage1['rank_ch'] == 1]\n",
    "    top_by_db = aggregated_stage1.loc[aggregated_stage1['rank_db'] == 1]\n",
    "\n",
    "    votes = []\n",
    "    if len(top_by_sil) > 0:\n",
    "        votes.extend(top_by_sil['config_id'].tolist())\n",
    "    if len(top_by_ch) > 0:\n",
    "        votes.extend(top_by_ch['config_id'].tolist())\n",
    "    if len(top_by_db) > 0:\n",
    "        votes.extend(top_by_db['config_id'].tolist())\n",
    "\n",
    "    # Check if aggregated_stage1 is empty\n",
    "    if len(aggregated_stage1) == 0:\n",
    "        raise ValueError(\"Stage 1 aggregation produced no results. Check groupby operation.\")\n",
    "    \n",
    "    vote_counts = Counter(votes)\n",
    "    if len(vote_counts) > 0:\n",
    "        most_voted_config, vote_count = vote_counts.most_common(1)[0]\n",
    "    \n",
    "        if vote_count >= 2:\n",
    "            matching_configs = aggregated_stage1[aggregated_stage1['config_id'] == most_voted_config]\n",
    "            if len(matching_configs) == 0:\n",
    "                raise ValueError(f\"Config {most_voted_config} not found in aggregated results.\")\n",
    "            best_config_stage1 = matching_configs.iloc[0]\n",
    "            consensus_status = f\"Consensus: {vote_count} metrics agree\"\n",
    "        else:\n",
    "            top_sil = aggregated_stage1.loc[aggregated_stage1['rank_silhouette'] == 1]\n",
    "            if len(top_sil) == 0:\n",
    "                raise ValueError(\"No top silhouette config found.\")\n",
    "            best_config_stage1 = top_sil.iloc[0]\n",
    "            consensus_status = f\"No consensus. Using Silhouette\"\n",
    "    else:\n",
    "        aggregated_stage1_sorted = aggregated_stage1.sort_values('mean_silhouette', ascending=False)\n",
    "        if len(aggregated_stage1_sorted) == 0:\n",
    "            raise ValueError(\"No configurations to select from.\")\n",
    "        best_config_stage1 = aggregated_stage1_sorted.iloc[0]\n",
    "        consensus_status = \"Using Silhouette (fallback)\"\n",
    "\n",
    "    best_hidden_size = int(best_config_stage1['hidden_size'])\n",
    "    best_latent_dim = int(best_config_stage1['latent_dim'])\n",
    "    best_activation_name = best_config_stage1['activation']\n",
    "    best_optimizer_name = best_config_stage1['optimizer']\n",
    "    best_epochs = int(best_config_stage1['epochs'])\n",
    "    best_k = force_k if force_k is not None else int(best_config_stage1['most_common_k'])\n",
    "\n",
    "    print(f\"Top 5 configs (by mean silhouette score):\\n\")\n",
    "    aggregated_stage1_sorted = aggregated_stage1.sort_values('mean_silhouette', ascending=False)\n",
    "    print(aggregated_stage1_sorted.head(5)[['hidden_size', 'latent_dim', 'activation', 'optimizer', \n",
    "                                            'epochs', 'mean_silhouette', 'std_silhouette',\n",
    "                                            'mean_ch', 'mean_db', 'most_common_k']].to_string(index=False))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Best Architecture (Stage 1): {consensus_status}\")\n",
    "    print(f\"  hidden_size={best_hidden_size}, latent_dim={best_latent_dim}\")\n",
    "    print(f\"  activation={best_activation_name}, optimizer={best_optimizer_name}, epochs={best_epochs}\")\n",
    "    print(f\"  Silhouette: {best_config_stage1['mean_silhouette']:.6f} ± {best_config_stage1['std_silhouette']:.6f}\")\n",
    "    print(f\"  Calinski-Harabasz: {best_config_stage1['mean_ch']:.6f} ± {best_config_stage1['std_ch']:.6f}\")\n",
    "    print(f\"  Davies-Bouldin: {best_config_stage1['mean_db']:.6f} ± {best_config_stage1['std_db']:.6f}\")\n",
    "    print(f\"  Reconstruction loss: {best_config_stage1['mean_recon_loss']:.6f} ± {best_config_stage1['std_recon_loss']:.6f}\")\n",
    "    print(f\"  Most common optimal K: {best_k}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # STAGE 2: Learning Parameter Optimization\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Fine-tune training parameters using best architecture from Stage 1\n",
    "    # Learning rate determined via LR Range Test\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"STAGE 2: Learning Parameter Optimization (K-Fold CV)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Using best architecture from Stage 1:\")\n",
    "    print(f\"  hidden={best_hidden_size}, latent={best_latent_dim}, activation={best_activation_name}, optimizer={best_optimizer_name}\")\n",
    "\n",
    "    activation_map = {'ReLU': nn.ReLU, 'Tanh': nn.Tanh, 'Sigmoid': nn.Sigmoid}\n",
    "    best_activation_fn = activation_map[best_activation_name]\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # LR Range Test: Find Optimal Learning Rate\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Test learning rates from 1e-7 to 10 to find optimal value\n",
    "    # Uses subset of training data for efficiency\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LR RANGE TEST (Using Best Architecture from Stage 1)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Create DataLoader that returns (input, target) pairs for LR finder\n",
    "    # For autoencoder, target = input (reconstruction task)\n",
    "    train_subset_data = train_val_tensor[:500]\n",
    "    train_subset = TensorDataset(train_subset_data, train_subset_data)  # (input, target) where target=input\n",
    "    train_loader_lr = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    print(f\"\\nLR Range Test: {best_optimizer_name} optimizer\")\n",
    "    model_lr = Autoencoder(INPUT_DIM, best_hidden_size, best_latent_dim, best_activation_fn).to(device)\n",
    "\n",
    "    if best_optimizer_name == 'Adam':\n",
    "        optimizer_lr = optim.Adam(model_lr.parameters(), lr=1e-7)\n",
    "    else:\n",
    "        optimizer_lr = optim.SGD(model_lr.parameters(), lr=1e-7, momentum=0.9)\n",
    "\n",
    "    lr_finder = LRFinder(model_lr, optimizer_lr, criterion, device=device)\n",
    "    lr_finder.range_test(train_loader_lr, end_lr=10, num_iter=100, step_mode=\"exp\")\n",
    "    lr_finder.plot()\n",
    "    plt.title(f'{best_optimizer_name} LR Range Test (Best Architecture)', fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "    history = lr_finder.history\n",
    "    lrs = np.array(history['lr'])\n",
    "    losses = np.array(history['loss'])\n",
    "    loss_diffs = np.diff(losses)\n",
    "    descending = np.where(loss_diffs < 0)[0]\n",
    "\n",
    "    if len(descending) > 0:\n",
    "        mid = (descending[0] + descending[-1]) // 2\n",
    "        optimal_lr = lrs[mid]\n",
    "    else:\n",
    "        optimal_lr = lrs[np.argmin(losses)]\n",
    "\n",
    "    print(f\"Optimal LR from range test: {optimal_lr:.2e}\\n\")\n",
    "\n",
    "    lr_finder.reset()\n",
    "\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    weight_decays = [0, 1e-4, 1e-3]\n",
    "\n",
    "    if best_optimizer_name == 'SGD':\n",
    "        momentum_values = [0.5, 0.9, 0.95]\n",
    "        print(f\"Testing: batch_size, weight_decay, momentum\")\n",
    "        print(f\"Fixed: learning_rate = {optimal_lr:.2e}\")\n",
    "        total_experiments = len(batch_sizes) * len(weight_decays) * len(momentum_values) * n_folds\n",
    "    else:\n",
    "        momentum_values = [None]\n",
    "        print(f\"Testing: batch_size, weight_decay\")\n",
    "        print(f\"Fixed: learning_rate = {optimal_lr:.2e}\")\n",
    "        total_experiments = len(batch_sizes) * len(weight_decays) * n_folds\n",
    "\n",
    "    results_stage2 = defaultdict(list)\n",
    "    experiment_count = 0\n",
    "\n",
    "    print(f\"\\nGrid sizes:\")\n",
    "    print(f\"  learning_rate: 1 value (optimal from LR range test)\")\n",
    "    print(f\"  batch_size: {len(batch_sizes)} values {batch_sizes}\")\n",
    "    print(f\"  weight_decay: {len(weight_decays)} values {weight_decays}\")\n",
    "    if best_optimizer_name == 'SGD':\n",
    "        print(f\"  momentum: {len(momentum_values)} values {momentum_values}\")\n",
    "    print(f\"  CV folds: {n_folds}\")\n",
    "    print(f\"Total: {total_experiments} experiments ({total_experiments//n_folds} configs × {n_folds} folds)\\n\")\n",
    "\n",
    "    start_stage2 = time.time()\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        for weight_decay in weight_decays:\n",
    "            for momentum in momentum_values:\n",
    "                for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_val_tensor)):\n",
    "                    experiment_count += 1\n",
    "                    if experiment_count % 50 == 0 or experiment_count == total_experiments:\n",
    "                        elapsed = time.time() - start_stage2\n",
    "                        print(f\"  [{experiment_count}/{total_experiments}] {100*experiment_count/total_experiments:.1f}% - {elapsed/60:.1f}min\")\n",
    "                \n",
    "                    fold_seed = 42 + fold_idx\n",
    "                    torch.manual_seed(fold_seed)\n",
    "                    np.random.seed(fold_seed)\n",
    "                \n",
    "                    train_fold = train_val_tensor[train_idx]\n",
    "                    val_fold = train_val_tensor[val_idx]\n",
    "                \n",
    "                    train_dataset = TensorDataset(train_fold.cpu())\n",
    "                    val_dataset = TensorDataset(val_fold.cpu())\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "                \n",
    "                    model = Autoencoder(INPUT_DIM, best_hidden_size, best_latent_dim, best_activation_fn).to(device)\n",
    "                \n",
    "                    if best_optimizer_name == 'SGD':\n",
    "                        optimizer = optim.SGD(model.parameters(), lr=optimal_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "                    else:\n",
    "                        optimizer = optim.Adam(model.parameters(), lr=optimal_lr, weight_decay=weight_decay)\n",
    "                \n",
    "                    train_losses, val_losses, optimal_k, best_sil_score, latent_vectors, validation_metrics = \\\n",
    "                        train_and_validate_model(model, train_loader, val_loader, optimizer, \n",
    "                                                criterion, best_epochs, device)\n",
    "                \n",
    "                    # Safely get index for optimal_k\n",
    "                    if optimal_k in validation_metrics['k_values']:\n",
    "                        optimal_k_idx = validation_metrics['k_values'].index(optimal_k)\n",
    "                        best_ch_score = validation_metrics['calinski_harabasz_scores'][optimal_k_idx]\n",
    "                        best_db_score = validation_metrics['davies_bouldin_scores'][optimal_k_idx]\n",
    "                    else:\n",
    "                        # Fallback: use first available K\n",
    "                        optimal_k_idx = 0\n",
    "                        best_ch_score = validation_metrics['calinski_harabasz_scores'][0] if len(validation_metrics['calinski_harabasz_scores']) > 0 else 0\n",
    "                        best_db_score = validation_metrics['davies_bouldin_scores'][0] if len(validation_metrics['davies_bouldin_scores']) > 0 else float('inf')\n",
    "                \n",
    "                    results_stage2['learning_rate'].append(optimal_lr)\n",
    "                    results_stage2['batch_size'].append(batch_size)\n",
    "                    results_stage2['weight_decay'].append(weight_decay)\n",
    "                    if best_optimizer_name == 'SGD':\n",
    "                        results_stage2['momentum'].append(momentum)\n",
    "                    results_stage2['fold'].append(fold_idx)\n",
    "                    results_stage2['silhouette_score'].append(best_sil_score)\n",
    "                    results_stage2['calinski_harabasz_score'].append(best_ch_score)\n",
    "                    results_stage2['davies_bouldin_score'].append(best_db_score)\n",
    "                    results_stage2['reconstruction_loss'].append(val_losses[-1])\n",
    "\n",
    "    print(f\"\\n✓ Stage 2 completed in {(time.time()-start_stage2)/60:.2f} minutes\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # STAGE 2: Results Aggregation and Best Configuration Selection\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Aggregate results across folds, select best learning parameters\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"STAGE 2: Results Aggregation\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    stage2_df = pd.DataFrame(results_stage2)\n",
    "    \n",
    "    # Check if we have any results\n",
    "    if len(stage2_df) == 0:\n",
    "        raise ValueError(\"Stage 2 produced no results. Check training loop.\")\n",
    "\n",
    "    if 'momentum' in results_stage2:\n",
    "        groupby_cols = ['learning_rate', 'batch_size', 'weight_decay', 'momentum']\n",
    "    else:\n",
    "        groupby_cols = ['learning_rate', 'batch_size', 'weight_decay']\n",
    "\n",
    "    aggregated_stage2 = stage2_df.groupby(groupby_cols).agg({\n",
    "        'silhouette_score': ['mean', 'std'],\n",
    "        'calinski_harabasz_score': ['mean', 'std'],\n",
    "        'davies_bouldin_score': ['mean', 'std'],\n",
    "        'reconstruction_loss': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    col_names = groupby_cols + ['mean_silhouette', 'std_silhouette', 'mean_ch', 'std_ch', \n",
    "                                'mean_db', 'std_db', 'mean_recon_loss', 'std_recon_loss']\n",
    "    aggregated_stage2.columns = col_names\n",
    "\n",
    "    # Create config_id BEFORE filtering\n",
    "    # Check if aggregated_stage2 is empty\n",
    "    if len(aggregated_stage2) == 0:\n",
    "        raise ValueError(\"Stage 2 aggregation produced no results. Check groupby operation.\")\n",
    "    \n",
    "    aggregated_stage2['config_id'] = aggregated_stage2.apply(\n",
    "        lambda row: f\"{row['learning_rate']:.2e}_{row['batch_size']}_{row['weight_decay']:.2e}\" + \n",
    "                    (f\"_{row['momentum']}\" if 'momentum' in row.index else \"\"), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    aggregated_stage2['rank_silhouette'] = aggregated_stage2['mean_silhouette'].rank(ascending=False, method='min')\n",
    "    aggregated_stage2['rank_ch'] = aggregated_stage2['mean_ch'].rank(ascending=False, method='min')\n",
    "    aggregated_stage2['rank_db'] = aggregated_stage2['mean_db'].rank(ascending=True, method='min')\n",
    "\n",
    "    top_by_sil = aggregated_stage2.loc[aggregated_stage2['rank_silhouette'] == 1]\n",
    "    top_by_ch = aggregated_stage2.loc[aggregated_stage2['rank_ch'] == 1]\n",
    "    top_by_db = aggregated_stage2.loc[aggregated_stage2['rank_db'] == 1]\n",
    "\n",
    "    votes = []\n",
    "    if len(top_by_sil) > 0:\n",
    "        votes.extend(top_by_sil['config_id'].tolist())\n",
    "    if len(top_by_ch) > 0:\n",
    "        votes.extend(top_by_ch['config_id'].tolist())\n",
    "    if len(top_by_db) > 0:\n",
    "        votes.extend(top_by_db['config_id'].tolist())\n",
    "\n",
    "    vote_counts = Counter(votes)\n",
    "    if len(vote_counts) > 0:\n",
    "        most_voted_config, vote_count = vote_counts.most_common(1)[0]\n",
    "    \n",
    "        if vote_count >= 2:\n",
    "            matching_configs = aggregated_stage2[aggregated_stage2['config_id'] == most_voted_config]\n",
    "            if len(matching_configs) == 0:\n",
    "                raise ValueError(f\"Config {most_voted_config} not found in aggregated results.\")\n",
    "            best_config_stage2 = matching_configs.iloc[0]\n",
    "            consensus_status = f\"Consensus: {vote_count} metrics agree\"\n",
    "        else:\n",
    "            top_sil = aggregated_stage2.loc[aggregated_stage2['rank_silhouette'] == 1]\n",
    "            if len(top_sil) == 0:\n",
    "                raise ValueError(\"No top silhouette config found.\")\n",
    "            best_config_stage2 = top_sil.iloc[0]\n",
    "            consensus_status = f\"No consensus. Using Silhouette\"\n",
    "    else:\n",
    "        aggregated_stage2_sorted = aggregated_stage2.sort_values('mean_silhouette', ascending=False)\n",
    "        if len(aggregated_stage2_sorted) == 0:\n",
    "            raise ValueError(\"No configurations to select from.\")\n",
    "        best_config_stage2 = aggregated_stage2_sorted.iloc[0]\n",
    "        consensus_status = \"Using Silhouette (fallback)\"\n",
    "\n",
    "    best_learning_rate = best_config_stage2['learning_rate']\n",
    "    best_batch_size = int(best_config_stage2['batch_size'])\n",
    "    best_weight_decay = best_config_stage2['weight_decay']\n",
    "    best_momentum = best_config_stage2.get('momentum', None)\n",
    "\n",
    "    print(f\"Top 5 configs (by mean silhouette score):\\n\")\n",
    "    aggregated_stage2_sorted = aggregated_stage2.sort_values('mean_silhouette', ascending=False)\n",
    "    print(aggregated_stage2_sorted.head(5)[groupby_cols + ['mean_silhouette', 'std_silhouette', \n",
    "                                                            'mean_ch', 'mean_db']].to_string(index=False))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Best Overall Configuration: {consensus_status}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Architecture (from Stage 1):\")\n",
    "    print(f\"  hidden_size={best_hidden_size}, latent_dim={best_latent_dim}\")\n",
    "    print(f\"  activation={best_activation_name}, optimizer={best_optimizer_name}, epochs={best_epochs}\")\n",
    "    print(f\"\\nLearning Parameters (from Stage 2):\")\n",
    "    print(f\"  learning_rate={best_learning_rate:.2e}\")\n",
    "    print(f\"  batch_size={best_batch_size}\")\n",
    "    print(f\"  weight_decay={best_weight_decay:.2e}\", end=\"\")\n",
    "    if best_momentum is not None:\n",
    "        print(f\", momentum={best_momentum}\")\n",
    "    else:\n",
    "        print()\n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"  Silhouette: {best_config_stage2['mean_silhouette']:.6f} ± {best_config_stage2['std_silhouette']:.6f}\")\n",
    "    print(f\"  Calinski-Harabasz: {best_config_stage2['mean_ch']:.6f} ± {best_config_stage2['std_ch']:.6f}\")\n",
    "    print(f\"  Davies-Bouldin: {best_config_stage2['mean_db']:.6f} ± {best_config_stage2['std_db']:.6f}\")\n",
    "    print(f\"  Reconstruction loss: {best_config_stage2['mean_recon_loss']:.6f} ± {best_config_stage2['std_recon_loss']:.6f}\")\n",
    "    print(f\"  Optimal K: {best_k}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # FINAL MODEL TRAINING AND LATENT PROFILE EXTRACTION\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Train final model on all train+val data with best hyperparameters\n",
    "    # Extract latent vectors and perform K-means clustering\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"Final Model Training and Latent Profile Extraction\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Training final model on all {dataset_name} train+val data with best hyperparameters:\")\n",
    "    print(f\"  Architecture: hidden={best_hidden_size}, latent={best_latent_dim}, activation={best_activation_name}\")\n",
    "    print(f\"  Optimizer: {best_optimizer_name}, lr={best_learning_rate:.2e}, batch_size={best_batch_size}\")\n",
    "    print(f\"  Epochs: {best_epochs}, Optimal K: {best_k}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    final_dataset = TensorDataset(train_val_tensor.cpu())\n",
    "    final_loader = DataLoader(final_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "\n",
    "    activation_map = {'ReLU': nn.ReLU, 'Tanh': nn.Tanh, 'Sigmoid': nn.Sigmoid}\n",
    "    best_activation_fn = activation_map[best_activation_name]\n",
    "    final_model = Autoencoder(INPUT_DIM, best_hidden_size, best_latent_dim, best_activation_fn).to(device)\n",
    "\n",
    "    if best_optimizer_name == 'SGD':\n",
    "        final_optimizer = optim.SGD(final_model.parameters(), lr=best_learning_rate, \n",
    "                                    momentum=best_momentum if best_momentum is not None else 0.9, \n",
    "                                    weight_decay=best_weight_decay)\n",
    "    else:\n",
    "        final_optimizer = optim.Adam(final_model.parameters(), lr=best_learning_rate, \n",
    "                                    weight_decay=best_weight_decay)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    print(f\"\\nTraining final model...\")\n",
    "    final_model.train()\n",
    "    final_losses = []\n",
    "\n",
    "    for epoch in range(best_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_data in final_loader:\n",
    "            batch_data = batch_data[0].to(device)\n",
    "            final_optimizer.zero_grad()\n",
    "            reconstructed = final_model(batch_data)\n",
    "            loss = criterion(reconstructed, batch_data)\n",
    "            loss.backward()\n",
    "            final_optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(final_loader)\n",
    "        final_losses.append(avg_loss)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{best_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nFinal model training complete. Saving model...\")\n",
    "    final_model.eval()\n",
    "    all_latent_vectors_batches = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in final_loader:\n",
    "            data = batch_data[0].to(device)\n",
    "            latent = final_model.encoder(data)\n",
    "            all_latent_vectors_batches.append(latent.cpu())\n",
    "    latent_vectors_all = np.vstack(all_latent_vectors_batches)\n",
    "    print(f\"Latent vectors shape: {latent_vectors_all.shape}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # K-Means Clustering: Extract Mental Health Profiles\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Cluster latent vectors to identify distinct mental health profiles\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"Running K-means clustering with optimal k... {best_k}\")\n",
    "    final_kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_SEED, n_init=10)\n",
    "    cluster_labels_all = final_kmeans.fit_predict(latent_vectors_all)\n",
    "    cluster_centroids = final_kmeans.cluster_centers_\n",
    "\n",
    "    print(f\"Cluster Assignments: {cluster_labels_all}\")\n",
    "    print(f\"Cluster Centroids: {cluster_centroids.shape}\")\n",
    "\n",
    "    # Check if we have at least 2 clusters (required for silhouette score)\n",
    "    n_unique_clusters = len(np.unique(cluster_labels_all))\n",
    "    if n_unique_clusters < 2:\n",
    "        print(f\"⚠️ ERROR: Final clustering produced only {n_unique_clusters} cluster(s). Cannot compute metrics.\")\n",
    "        final_sil_score = -1\n",
    "        final_ch_score = 0\n",
    "        final_db_score = float('inf')\n",
    "    else:\n",
    "        final_sil_score = silhouette_score(latent_vectors_all, cluster_labels_all)\n",
    "        print(f\"Final Silhouette Score: {final_sil_score:.4f}\")\n",
    "        final_ch_score = calinski_harabasz_score(latent_vectors_all, cluster_labels_all)\n",
    "        print(f\"Final Calinski-Harabasz Score: {final_ch_score:.4f}\")\n",
    "        final_db_score = davies_bouldin_score(latent_vectors_all, cluster_labels_all)\n",
    "        print(f\"Final Davies-Bouldin Score: {final_db_score:.4f}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # PROFILE CHARACTERISTICS EXTRACTION\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Map cluster labels back to original symptom space to interpret profiles\n",
    "    # Compute mean symptom levels (Depression, Anxiety, Stress, Burnout) per cluster\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(f\"\\nProfile Characteristics (mean feature values per cluster):\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # VERIFICATION: Check that column order matches FEATURE_COLUMNS\n",
    "    print(\"Verifying feature column order...\")\n",
    "    sample_0 = train_val_data[0]\n",
    "    print(f\"FEATURE_COLUMNS: {FEATURE_COLUMNS}\")\n",
    "    print(f\"First sample values:\")\n",
    "    for i, feature_name in enumerate(FEATURE_COLUMNS):\n",
    "        print(f\"  train_val_data[0, {i}] = {sample_0[i]:.4f} → {feature_name}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    profile_summary = []\n",
    "\n",
    "    for k in range(best_k):\n",
    "        cluster_mask = cluster_labels_all == k\n",
    "        # CRITICAL: Use original 4D symptom data, NOT latent vectors as they are more intellgible to understand  \n",
    "        cluster_data = train_val_data[cluster_mask]  # Original 4D: [Depression, Anxiety, Stress, Burnout]\n",
    "        cluster_size = np.sum(cluster_mask)\n",
    "    \n",
    "        # Method 1: Using array indexing (for verification)\n",
    "        feature_means_array = cluster_data.mean(axis=0)\n",
    "    \n",
    "        # Method 2: Using DataFrame for explicit mapping (SAFER)\n",
    "        cluster_df = pd.DataFrame(cluster_data, columns=FEATURE_COLUMNS)\n",
    "        feature_means_dict = cluster_df.mean().to_dict()\n",
    "    \n",
    "        # Verify they match\n",
    "        print(f\"Cluster {k} (N={cluster_size}):\")\n",
    "        for i, feature_name in enumerate(FEATURE_COLUMNS):\n",
    "            array_val = feature_means_array[i]\n",
    "            dict_val = feature_means_dict[feature_name]\n",
    "            match = \" MATCH\" if abs(array_val - dict_val) < 1e-10 else \" MISMATCH\"\n",
    "            print(f\"  Index {i} ({feature_name}): array[{i}]={array_val:.6f}, dict['{feature_name}']={dict_val:.6f} {match}\")\n",
    "    \n",
    "        # Use dictionary approach (explicit, no index guessing)\n",
    "        profile_summary.append({\n",
    "            'Profile': f'P{k+1}',\n",
    "            'N': cluster_size,\n",
    "            'Depression': feature_means_dict['Depression'],\n",
    "            'Anxiety': feature_means_dict['Anxiety'],\n",
    "            'Stress': feature_means_dict['Stress'],\n",
    "            'Burnout': feature_means_dict['Burnout']\n",
    "        })\n",
    "        print()\n",
    "\n",
    "    profile_df = pd.DataFrame(profile_summary)\n",
    "    print(\"Profile Summary Table:\")\n",
    "    print(profile_df.to_string(index=False))\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Final Model Results Saved:\")\n",
    "    print(f\"  - {dataset_name} latent vectors: {latent_vectors_all.shape}\")\n",
    "    print(f\"  - {dataset_name} cluster assignments: {cluster_labels_all.shape}\")\n",
    "    print(f\"  - {dataset_name} cluster centroids: {cluster_centroids.shape}\")\n",
    "    print(f\"  - Profile summary: {len(profile_summary)} profiles\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # PROFILE INTERPRETATION\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Classify profiles based on symptom levels relative to global thresholds\n",
    "    # Assign meaningful names (e.g., \"Severe Comorbid\", \"Low Symptom\")\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"Interpretation of the profiles:\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "    #It is important to compute global threshold values for each profile based on the entire dataset not just the cluster data\n",
    "    #This ensures consistency and comparability across different datasets\n",
    "\n",
    "\n",
    "    global_depression_threshold_high = np.percentile(train_val_data[:, 0], 75)  \n",
    "    global_depression_threshold_low = np.percentile(train_val_data[:, 0], 25)  \n",
    "    global_anxiety_threshold_high = np.percentile(train_val_data[:, 1], 75)\n",
    "    global_anxiety_threshold_low = np.percentile(train_val_data[:, 1], 25)\n",
    "    global_stress_threshold_high = np.percentile(train_val_data[:, 2], 75)\n",
    "    global_stress_threshold_low = np.percentile(train_val_data[:, 2], 25)\n",
    "    global_burnout_threshold_high = np.percentile(train_val_data[:, 3], 75)\n",
    "    global_burnout_threshold_low = np.percentile(train_val_data[:, 3], 25)\n",
    "\n",
    "    def interpret_profile(depression, anxiety, stress, burnout):\n",
    "        \"\"\"\n",
    "        Interpret a single profile based on global thresholds\n",
    "        returns a string description of the profile\n",
    "        \"\"\"\n",
    "\n",
    "        high_symptoms = []\n",
    "        low_symptoms = []\n",
    "\n",
    "\n",
    "        # Compare to global thresholds not just cluster centroids\n",
    "        if depression > global_depression_threshold_high:\n",
    "            high_symptoms.append(\"Depression\")\n",
    "        elif depression < global_depression_threshold_low:\n",
    "            low_symptoms.append(\"Depression\")\n",
    "\n",
    "        if anxiety > global_anxiety_threshold_high:\n",
    "            high_symptoms.append(\"Anxiety\")\n",
    "        elif anxiety < global_anxiety_threshold_low:\n",
    "            low_symptoms.append(\"Anxiety\")\n",
    "\n",
    "        if stress > global_stress_threshold_high:\n",
    "            high_symptoms.append(\"Stress\")\n",
    "        elif stress < global_stress_threshold_low:\n",
    "            low_symptoms.append(\"Stress\")\n",
    "\n",
    "        if burnout > global_burnout_threshold_high:\n",
    "            high_symptoms.append(\"Burnout\")\n",
    "        elif burnout < global_burnout_threshold_low:\n",
    "            low_symptoms.append(\"Burnout\")\n",
    "    \n",
    "        if len(high_symptoms) >= 3:\n",
    "            return \"Severe Comorbid Profile\", \"High levels across multiple dimensions\"\n",
    "        elif \"Depression\" in high_symptoms and \"Anxiety\" in high_symptoms:\n",
    "            return \"Depression-Anxiety Comorbidity Profile\", \"High Depression and Anxiety, typical of internalizing disorders\"\n",
    "        elif \"Stress\" in high_symptoms and \"Burnout\" in high_symptoms:\n",
    "            return \"Stress-Burnout Profile\", \"High Stress and Burnout, typical of work-related distress\"\n",
    "        elif len(high_symptoms) == 1:\n",
    "            return f\"High {high_symptoms[0]} Profile\", f\"Elevated {high_symptoms[0]} with other symptoms in normal range\"\n",
    "        elif len(low_symptoms) >= 3:\n",
    "            return \"Low Symptom Profile\", \"Low levels across most dimensions\"\n",
    "        else:\n",
    "            return \"Moderate/Mixed Profile\", \"Moderate levels across dimensions\"\n",
    "\n",
    "    for i, profile in enumerate(profile_summary):\n",
    "        profile_name, description = interpret_profile(\n",
    "            profile['Depression'], \n",
    "            profile['Anxiety'], \n",
    "            profile['Stress'], \n",
    "            profile['Burnout']\n",
    "        )\n",
    "        profile_summary[i]['Profile_Name'] = profile_name\n",
    "        profile_summary[i]['Description'] = description\n",
    "\n",
    "    # Display interpreted profiles\n",
    "    print(\"Interpreted Profiles:\")\n",
    "    interpreted_df = pd.DataFrame(profile_summary)\n",
    "    print(interpreted_df[['Profile', 'Profile_Name', 'N', 'Depression', 'Anxiety', 'Stress', 'Burnout', 'Description']].to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # TEST SET EVALUATION\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Evaluate model generalization on held-out test data\n",
    "    # Test set was never used during hyperparameter tuning or training\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"Test-Set - 20% of the data was kept aside and used for testing\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Test set was held out during hyperparameter tuning - now evaluating final model\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"Encoding test data with trained autoencoder...\")\n",
    "    test_dataset = TensorDataset(test_tensor.cpu())\n",
    "    test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "    final_model.eval()\n",
    "    test_latent_vectors = []\n",
    "    test_reconstructions = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            data = batch_data[0].to(device)\n",
    "            latent = final_model.encoder(data)\n",
    "            reconstructed = final_model(data)\n",
    "            test_latent_vectors.append(latent.cpu().numpy())\n",
    "            test_reconstructions.append(reconstructed.cpu().numpy())\n",
    "    test_latent = np.vstack(test_latent_vectors)\n",
    "    test_recon = np.vstack(test_reconstructions)\n",
    "\n",
    "\n",
    "    print(f\"  Test latent vectors: {test_latent.shape}\")\n",
    "    print(f\"  Test reconstructions: {test_recon.shape}\")\n",
    "\n",
    "    # Compute test reconstruction error\n",
    "    test_data_np = test_tensor.cpu().numpy()\n",
    "    test_recon_loss = np.mean((test_data_np - test_recon) ** 2)\n",
    "    print(f\"  Test reconstruction loss (MSE): {test_recon_loss:.6f}\")\n",
    "\n",
    "    # Assign test samples to clusters using trained centroids\n",
    "    # CRITICAL: Use centroids from train+val, don't retrain K-means\n",
    "    # This tests if cluster structure generalizes to new data\n",
    "    print(f\"\\nAssigning test samples to clusters...\")\n",
    "    from scipy.spatial.distance import cdist\n",
    "    test_distances = cdist(test_latent, cluster_centroids, metric='euclidean')\n",
    "    test_cluster_assignments = np.argmin(test_distances, axis=1)\n",
    "\n",
    "    print(f\"  Test cluster assignments: {Counter(test_cluster_assignments)}\")\n",
    "\n",
    "    # Evaluate clustering quality on test set\n",
    "    # Check if we have at least 2 clusters (required for silhouette score)\n",
    "    n_unique_test_clusters = len(np.unique(test_cluster_assignments))\n",
    "    if n_unique_test_clusters < 2:\n",
    "        print(f\"⚠️ ERROR: Test clustering produced only {n_unique_test_clusters} cluster(s). Cannot compute metrics.\")\n",
    "        test_sil_score = -1\n",
    "        test_ch_score = 0\n",
    "        test_db_score = float('inf')\n",
    "    else:\n",
    "        test_sil_score = silhouette_score(test_latent, test_cluster_assignments)\n",
    "        test_ch_score = calinski_harabasz_score(test_latent, test_cluster_assignments)\n",
    "        test_db_score = davies_bouldin_score(test_latent, test_cluster_assignments)\n",
    "\n",
    "    print(f\"\\nTest Set Clustering Quality:\")\n",
    "    print(f\"  Silhouette Score: {test_sil_score:.6f}\")\n",
    "    print(f\"  Calinski-Harabasz: {test_ch_score:.6f}\")\n",
    "    print(f\"  Davies-Bouldin: {test_db_score:.6f}\")\n",
    "\n",
    "    # Check for overfitting: Compare train+val vs test performance\n",
    "    if final_sil_score != 0:\n",
    "        sil_diff_pct = abs(final_sil_score - test_sil_score) / final_sil_score * 100\n",
    "    else:\n",
    "        sil_diff_pct = float('inf')  # Handle edge case where final_sil_score is 0\n",
    "    if sil_diff_pct < 5:\n",
    "        print(f\"\\n✓ Good generalization: Test performance within {sil_diff_pct:.2f}% of train+val\")\n",
    "    elif sil_diff_pct < 10:\n",
    "        print(f\"\\n⚠ Moderate generalization gap: Test performance {sil_diff_pct:.2f}% different from train+val\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Potential overfitting: Test performance {sil_diff_pct:.2f}% different from train+val\")\n",
    "\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # VISUALIZATIONS\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Create visualizations of latent space and profile characteristics\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"Visualizing Latent Space and Profile Characteristics\")\n",
    "\n",
    "    #1. Latent Space Visualization\n",
    "    print(\"\\n1. Latent Space Viz\")\n",
    "\n",
    "    if best_latent_dim == 2:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        scatter = ax.scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], \n",
    "                            c=cluster_labels_all, cmap='viridis', alpha=0.6, s=30)\n",
    "        ax.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], c='red', marker='x', \n",
    "                  s=300, linewidths=4, label='Cluster Centroids', zorder=5)\n",
    "        #Add profile labels to centroids\n",
    "\n",
    "        for i, (x,y) in enumerate(cluster_centroids):\n",
    "            profile_name = profile_summary[i].get('Profile_Name', f'P{i+1}')\n",
    "            ax.annotate(profile_name, (x,y), xytext=(5,5), textcoords='offset points',\n",
    "                       fontsize=10, fontweight='bold', bbox=dict(boxstyle='round,pad=0.3',\n",
    "                       facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "        ax.set_xlabel('Latent Dimension 1', fontsize=12)\n",
    "        ax.set_ylabel('Latent Dimension 2', fontsize=12)\n",
    "        ax.set_title('Latent Space Visualization (Colored by Profile)', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=ax, label='Profile')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    elif best_latent_dim == 3:\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "        scatter = ax.scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], latent_vectors_all[:, 2],\n",
    "                            c=cluster_labels_all, cmap='viridis', alpha=0.6, s=30)\n",
    "        ax.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], cluster_centroids[:, 2],\n",
    "                  c='red', marker='x', s=300, linewidths=4, label='Centroids')\n",
    "    \n",
    "        ax.set_xlabel('Latent Dim 1', fontsize=12)\n",
    "        ax.set_ylabel('Latent Dim 2', fontsize=12)\n",
    "        ax.set_zlabel('Latent Dim 3', fontsize=12)\n",
    "        ax.set_title('Latent Space Visualization (3D)', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        plt.colorbar(scatter, ax=ax, label='Profile')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    #2. Profile Characteristics Heatmap\n",
    "    print(\"\\n2. Profile Characteristics Heatmap:\")\n",
    "    profile_matrix = pd.DataFrame(profile_summary)[['Profile', 'Depression', 'Anxiety', 'Stress', 'Burnout']].set_index('Profile')\n",
    "    profile_matrix_normalized = (profile_matrix - profile_matrix.min()) / (profile_matrix.max() - profile_matrix.min())\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(profile_matrix_normalized.T, annot=profile_matrix.T, fmt='.3f', cmap='RdYlGn_r',\n",
    "               cbar_kws={'label': 'Normalized Symptom Level'}, linewidths=0.5, linecolor='black')\n",
    "    plt.title('Profile Characteristics Heatmap\\n(Normalized Symptom Levels)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Profile', fontsize=12)\n",
    "    plt.ylabel('Symptom', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #Profile Bar Chart\n",
    "    print(\"\\n3. Profile Symptom Levels (Bar Chart):\")\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(len(profile_summary))\n",
    "    width = 0.2\n",
    "\n",
    "    for i, profile in enumerate(profile_summary):\n",
    "        ax.bar(x[i] - 1.5*width, profile['Depression'], width, label='Depression' if i == 0 else '', color='#ff6b6b')\n",
    "        ax.bar(x[i] - 0.5*width, profile['Anxiety'], width, label='Anxiety' if i == 0 else '', color='#4ecdc4')\n",
    "        ax.bar(x[i] + 0.5*width, profile['Stress'], width, label='Stress' if i == 0 else '', color='#45b7d1')\n",
    "        ax.bar(x[i] + 1.5*width, profile['Burnout'], width, label='Burnout' if i == 0 else '', color='#f9ca24')\n",
    "\n",
    "    ax.set_xlabel('Profile', fontsize=12)\n",
    "    ax.set_ylabel('Symptom Level', fontsize=12)\n",
    "    ax.set_title('Symptom Levels by Profile', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([p['Profile'] for p in profile_summary])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #Cluster Size Distribution\n",
    "    print(\"\\n4. Cluster Size Distribution:\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cluster_sizes = [p['N'] for p in profile_summary]\n",
    "    cluster_labels_viz = [p['Profile'] for p in profile_summary]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(cluster_sizes)))\n",
    "\n",
    "    bars = ax.bar(cluster_labels_viz, cluster_sizes, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    ax.set_xlabel('Profile', fontsize=12)\n",
    "    ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "    ax.set_title('Profile Size Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # H3 VALIDATION: Clinical Utility Testing\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Test if profile membership predicts therapy utilization\n",
    "    # Only available for D1-Swiss dataset\n",
    "    # ------------------------------------------------------------------------\n",
    "    if dataset_name == \"D1-Swiss\" and \"PSYT_Therapy_Use\" in all_data_df.columns:\n",
    "        print(\"\\nH3 VALIDATION: Testing Clinical Utility of Profiles\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Hypothesis H3: Profile membership is associated with therapy utilization\")\n",
    "        print(\"Using FULL dataset (train+val+test) for maximum statistical power\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        y_therapy = all_data_df[\"PSYT_Therapy_Use\"].values\n",
    "        train_val_therapy, test_therapy = train_test_split(\n",
    "            y_therapy,\n",
    "            test_size=0.2,\n",
    "            random_state=RANDOM_SEED,\n",
    "        )\n",
    "        y_therapy_aligned = np.concatenate([train_val_therapy, test_therapy])\n",
    "        all_cluster_labels = np.concatenate([cluster_labels_all, test_cluster_assignments])\n",
    "\n",
    "        assert len(y_therapy_aligned) == len(all_cluster_labels), \"Misalignment\"\n",
    "        print(f\"\\n✓ Data aligned: {len(all_cluster_labels)} samples\")\n",
    "        print(f\"  Therapy use rate: {y_therapy_aligned.mean():.2%} ({y_therapy_aligned.sum()}/{len(y_therapy_aligned)})\")\n",
    "\n",
    "        print(\"\\nChi-Square Test for Independence:\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        contingency = pd.crosstab(all_cluster_labels, y_therapy_aligned)\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "        print(\"   Contingency Table:\")\n",
    "        print(contingency)\n",
    "        print(f\"\\n   Chi-square statistic: χ² = {chi2:.4f}\")\n",
    "        print(f\"   Degrees of freedom: df = {dof}\")\n",
    "        print(f\"   p-value: p = {p:.6f}\")\n",
    "\n",
    "        alpha = 0.05\n",
    "        if p < alpha:\n",
    "            print(f\"\\n   ✓ SIGNIFICANT (p < {alpha}): Profile membership IS associated with therapy utilization\")\n",
    "            print(\"   → H3 VALIDATED: Profiles have clinical utility\")\n",
    "        else:\n",
    "            print(f\"\\n   ✗ NOT SIGNIFICANT (p >= {alpha}): No association detected\")\n",
    "            print(\"   → H3 NOT VALIDATED\")\n",
    "\n",
    "        print(\"\\nCramer V Effect Size:\")\n",
    "        print(\"=\"*70)\n",
    "        n = contingency.values.sum()\n",
    "        min_dim = min(contingency.shape)\n",
    "        cramers_v = np.sqrt(chi2 / (n * (min_dim - 1)))\n",
    "        print(f\"   Cramer's V: {cramers_v:.4f}\")\n",
    "\n",
    "        if cramers_v < 0.10:\n",
    "            effect_size = \"negligible\"\n",
    "        elif cramers_v < 0.30:\n",
    "            effect_size = \"small\"\n",
    "        elif cramers_v < 0.50:\n",
    "            effect_size = \"medium\"\n",
    "        else:\n",
    "            effect_size = \"large\"\n",
    "        print(f\"   Effect size: {effect_size}\")\n",
    "\n",
    "        print(\"\\n3. Post-Hoc Analysis: Standardized Residuals:\")\n",
    "        print(\"-\"*70)\n",
    "        print(\"   (Values > |2| indicate significant deviation from expected)\")\n",
    "        residuals = (contingency.values - expected) / np.sqrt(expected + 1e-10)\n",
    "        residuals_df = pd.DataFrame(\n",
    "            residuals,\n",
    "            index=[f'P{k+1}' for k in range(best_k)],\n",
    "            columns=['No Therapy', 'Therapy']\n",
    "        )\n",
    "        print(residuals_df.round(3))\n",
    "\n",
    "        print(\"\\n   Significant deviations:\")\n",
    "        for i in range(best_k):\n",
    "            for j in range(2):\n",
    "                if abs(residuals[i, j]) > 2:\n",
    "                    profile_name = profile_summary[i].get('Profile_Name', f'P{i+1}')\n",
    "                    therapy_status = 'Therapy' if j == 1 else 'No Therapy'\n",
    "                    direction = 'Higher' if residuals[i, j] > 0 else 'Lower'\n",
    "                    print(f\"      • {profile_name} - {therapy_status}: {direction} than expected (residual = {residuals[i, j]:.2f})\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"H3 VALIDATION SUMMARY:\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Dataset: {dataset_name} (N={len(all_cluster_labels)})\")\n",
    "        print(f\"Chi-square: χ² = {chi2:.4f}, p = {p:.6f}, df = {dof}\")\n",
    "        print(f\"Cramér's V = {cramers_v:.4f} ({effect_size} effect)\")\n",
    "        print(f\"H3 Status: {' VALIDATED' if p < alpha else '✗ NOT VALIDATED'}\")\n",
    "\n",
    "        if p < alpha:\n",
    "            print(\"\\nConclusion:\")\n",
    "            print(\"  Profiles demonstrate clinical utility by predicting therapy utilization.\")\n",
    "            print(\"  This supports the use of these profiles for targeted mental health interventions.\")\n",
    "\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "    else:\n",
    "        print(\"\\nSkipping H3 validation (only available for D1-Swiss with PSYT_Therapy_Use)\")\n",
    "\n",
    "    result = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'train_val_data': train_val_data,\n",
    "        'latent_vectors_all': latent_vectors_all,\n",
    "        'cluster_labels_all': cluster_labels_all,\n",
    "        'cluster_centroids': cluster_centroids,\n",
    "        'best_k': best_k,\n",
    "        'best_latent_dim': best_latent_dim,\n",
    "        'final_sil_score': final_sil_score,\n",
    "        'final_ch_score': final_ch_score,\n",
    "        'final_db_score': final_db_score,\n",
    "        'reconstruction_loss': test_recon_loss,  # Store test reconstruction loss for comparison\n",
    "        'profile_summary': profile_summary,  # Store profile characteristics (mean symptom levels per cluster)\n",
    "    }\n",
    "    PIPELINE_RESULTS[dataset_name] = result\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optuna Scout Phase-\n",
    "#The purpose of this phase is to intelligently explore the hyperparameter space\n",
    "#Uses 3fold cv, 15 epochs(expedited)\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "# train_and_validate_model is defined in Cell 3, no import needed\n",
    "\n",
    "def scout_objective(trial, dataset_name=\"D1-Swiss\"):\n",
    "    \"\"\"\n",
    "    Objective function for Scout hyperparameter optimization.\n",
    "    \"\"\"\n",
    "    #Suggest Hyperparameters\n",
    "    hidden_size = trial.suggest_int('hidden_size', 2, 12)  # Include 2 for 4-feature datasets\n",
    "    latent_dim = trial.suggest_int('latent_dim', 2, 4)\n",
    "    activation = trial.suggest_categorical('activation', ['ReLU', 'Tanh', 'Sigmoid'])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
    "\n",
    "\n",
    "\n",
    "    (df, feature_matrix, train_val_data, test_data,\n",
    "    train_val_tensor, test_tensor, kfold, dataset_path) = prepare_dataset(\n",
    "        dataset_name\n",
    "    )\n",
    "\n",
    "    kfold_scout = KFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    activation_map = {\n",
    "        'ReLU': nn.ReLU,\n",
    "        'Tanh': nn.Tanh,\n",
    "        'Sigmoid': nn.Sigmoid\n",
    "    }\n",
    "    \n",
    "    scores = []\n",
    "    criterion = nn.MSELoss()\n",
    "    fixed_lr = 1e-3\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kfold_scout.split(train_val_tensor)):\n",
    "        fold_seed = RANDOM_SEED + fold_idx\n",
    "        torch.manual_seed(fold_seed)\n",
    "        np.random.seed(fold_seed)\n",
    "\n",
    "\n",
    "        train_fold = train_val_tensor[train_idx]\n",
    "        val_fold = train_val_tensor[val_idx]\n",
    "\n",
    "        train_dataset = TensorDataset(train_fold.cpu())\n",
    "        val_dataset = TensorDataset(val_fold.cpu())\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        model = Autoencoder(\n",
    "            len(FEATURE_COLUMNS),\n",
    "            hidden_size,\n",
    "            latent_dim,\n",
    "            activation_map[activation]\n",
    "\n",
    "        ).to(device)\n",
    "\n",
    "        if optimizer_name == 'Adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=fixed_lr)\n",
    "        else:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=fixed_lr, momentum=0.9)\n",
    "        \n",
    "        # Train and Validate - 15 epochs only\n",
    "        scout_epochs = 15 \n",
    "\n",
    "        train_losses, val_losses, optimal_k, best_sil_score, latent_vectors, validation_metrics = \\\n",
    "            train_and_validate_model(model, train_loader, val_loader, optimizer, \n",
    "                                    criterion, scout_epochs, device)\n",
    "        \n",
    "        fold_score = best_sil_score\n",
    "        scores.append(fold_score)\n",
    "        \n",
    "        # Report intermediate value (enables per-fold pruning)\n",
    "        # Each fold = 1 step, so with 3-fold CV: 5 steps = 5 ÷ 3 = 1.67 folds\n",
    "        trial.report(fold_score, step=fold_idx)\n",
    "        \n",
    "        # Check if should prune after this fold\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return np.mean(scores) if scores else 0.0\n",
    "def run_scout_phase(dataset_name=\"D1-Swiss\", n_trials=150, save_path=None):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"SCOUT PHASE: Optuna Optimization on {dataset_name}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  - Trials: {n_trials}\")\n",
    "    print(f\"  - CV Folds: 3 (fast)\")\n",
    "    print(f\"  - Epochs: 15 (fast)\")\n",
    "    print(f\"  - Pruning: Enabled (stops bad trials early)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        pruner=MedianPruner(\n",
    "            n_startup_trials=20,  # Wait for 20 complete trials before pruning\n",
    "                              # (Ensures stable median baseline, prevents false positives)\n",
    "            n_warmup_steps=5      # Wait for 5 reporting steps before pruning\n",
    "                              # With 3-fold CV: 5 steps ÷ 3 folds = 1.67 folds\n",
    "                              # (Allows 1 complete fold + 2/3 of next fold before pruning)\n",
    "        )\n",
    "    )\n",
    "    start_time = time.time()\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: scout_objective(trial, dataset_name),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\n✓ Scout phase completed in {elapsed_time/60:.1f} minutes\")\n",
    "    print(f\"  Best trial: {study.best_trial.number}\")\n",
    "    print(f\"  Best value: {study.best_value:.6f}\")\n",
    "    print(f\"  Best params: {study.best_trial.params}\")\n",
    "\n",
    "    top_5_trials = sorted([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE], key=lambda x: x.value if x.value is not None else -1, reverse=True)[:5]\n",
    "    \n",
    "    print(f\"\\nTop 5 Trials:\")\n",
    "    for i, trial in enumerate(top_5_trials, 1):\n",
    "        print(f\"  {i}. Trial {trial.number}: Score={trial.value:.6f}, Params={trial.params}\")\n",
    "    \n",
    "    # Analyze patterns\n",
    "    print(f\"\\nPattern Analysis:\")\n",
    "    hidden_sizes = [t.params['hidden_size'] for t in top_5_trials]\n",
    "    latent_dims = [t.params['latent_dim'] for t in top_5_trials]\n",
    "    activations = [t.params['activation'] for t in top_5_trials]\n",
    "    optimizers = [t.params['optimizer'] for t in top_5_trials]\n",
    "    \n",
    "    print(f\"  Hidden sizes in top 5: {Counter(hidden_sizes)}\")\n",
    "    print(f\"  Latent dims in top 5: {Counter(latent_dims)}\")\n",
    "    print(f\"  Activations in top 5: {Counter(activations)}\")\n",
    "    print(f\"  Optimizers in top 5: {Counter(optimizers)}\")\n",
    "    \n",
    "    # Save results\n",
    "    scout_results = {\n",
    "        'dataset': dataset_name,\n",
    "        'n_trials': n_trials,\n",
    "        'best_trial': study.best_trial.number,\n",
    "        'best_value': study.best_value,\n",
    "        'best_params': study.best_trial.params,\n",
    "        'top_5_trials': [\n",
    "            {\n",
    "                'number': t.number,\n",
    "                'value': t.value,\n",
    "                'params': t.params\n",
    "            } for t in top_5_trials\n",
    "        ],\n",
    "        'patterns': {\n",
    "            'hidden_sizes': dict(Counter(hidden_sizes)),\n",
    "            'latent_dims': dict(Counter(latent_dims)),\n",
    "            'activations': dict(Counter(activations)),\n",
    "            'optimizers': dict(Counter(optimizers))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(scout_results, f, indent=2)\n",
    "        print(f\"\\n✓ Results saved to {save_path}\")\n",
    "    \n",
    "    return study, scout_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d5255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the narrowed ranges from the optuna exploration study\n",
    "#Analyse the patterns and trends in the results\n",
    "#Use these ranges to set the hyperparameters for the next phase\n",
    "\n",
    "def extract_narrowed_ranges(scout_results):\n",
    "    \"\"\"\n",
    "    Extract narrowed ranges from scout results\n",
    "    \"\"\"\n",
    "    top_5 = scout_results['top_5_trials']\n",
    "\n",
    "    #Extract ranges from top 5 trials\n",
    "    hidden_sizes = sorted(set([t['params']['hidden_size'] for t in top_5]))\n",
    "    latent_dims = sorted(set([t['params']['latent_dim'] for t in top_5]))\n",
    "    activations = sorted(set([t['params']['activation'] for t in top_5]))\n",
    "    optimizers = sorted(set([t['params']['optimizer'] for t in top_5]))\n",
    "\n",
    "            \n",
    "    hidden_sizes_expanded = sorted(set(\n",
    "        [max(3, h-1) for h in hidden_sizes] + \n",
    "        hidden_sizes + \n",
    "        [min(12, h+1) for h in hidden_sizes]\n",
    "    ))\n",
    "\n",
    "    if len(activations) == 3:\n",
    "        act_counts = Counter([t['params']['activation'] for t in top_5])\n",
    "        activations = [act for act in activations if act_counts[act] >= 2]\n",
    "\n",
    "    narrowed_params = {\n",
    "        'hidden_sizes': hidden_sizes_expanded,\n",
    "        'latent_dims': latent_dims,\n",
    "        'activations': activations,\n",
    "        'optimizers': optimizers,\n",
    "        'epochs': [50, 75, 100]  # Full epochs for focused grid\n",
    "    }\n",
    "\n",
    "    print(\"Narrowed Parameter Ranges for Focused Grid:\")\n",
    "    print(f\"  hidden_sizes: {narrowed_params['hidden_sizes']}\")\n",
    "    print(f\"  latent_dims: {narrowed_params['latent_dims']}\")\n",
    "    print(f\"  activations: {narrowed_params['activations']}\")\n",
    "    print(f\"  optimizers: {narrowed_params['optimizers']}\")\n",
    "    print(f\"  epochs: {narrowed_params['epochs']}\")\n",
    "    \n",
    "    grid_size = (len(narrowed_params['hidden_sizes']) * \n",
    "                 len(narrowed_params['latent_dims']) * \n",
    "                 len(narrowed_params['activations']) * \n",
    "                 len(narrowed_params['optimizers']) * \n",
    "                 len(narrowed_params['epochs']))\n",
    "    print(f\"\\n  Grid size: {grid_size} configs × 10 folds = {grid_size * 10} experiments\")\n",
    "    \n",
    "    return narrowed_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbceb83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DATASET = \"D1-Swiss\"\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "SAVE_DIR = '/content/drive/MyDrive/mental-health-profiling'  # Adjust path as needed\n",
    "\n",
    "import os \n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"OPTIMIZATION PIPELINE FOR {TARGET_DATASET}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SCOUT PHASE: Optuna Optimization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "study, scout_results = run_scout_phase(\n",
    "    dataset_name=TARGET_DATASET,\n",
    "    n_trials=150,\n",
    "    save_path=f'{SAVE_DIR}/scout_{TARGET_DATASET.lower().replace(\"-\", \"_\")}.json'\n",
    ")\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "study_db_path = f'{SAVE_DIR}/scout_{TARGET_DATASET.lower().replace(\"-\", \"_\")}.db'\n",
    "study_storage = optuna.storages.InMemoryStorage()\n",
    "# For simplicity, just save the study as pickle\n",
    "import pickle\n",
    "with open(study_db_path.replace('.db', '_study.pkl'), 'wb') as f:\n",
    "    pickle.dump(study, f)\n",
    "\n",
    "# Phase 2: Extract narrowed ranges\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: EXTRACT NARROWED RANGES\")\n",
    "print(\"=\"*80)\n",
    "narrowed_params = extract_narrowed_ranges(scout_results)\n",
    "\n",
    "with open(f'{SAVE_DIR}/narrowed_params_{TARGET_DATASET.lower().replace(\"-\", \"_\")}.json', 'w') as f:\n",
    "    json.dump(narrowed_params, f, indent=2)\n",
    "\n",
    "# Phase 3: Focused Grid\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3: FOCUSED GRID SEARCH\")\n",
    "print(\"=\"*80)\n",
    "best_result = run_autoencoder_pipeline(\n",
    "    dataset_name=TARGET_DATASET,\n",
    "    narrowed_params=narrowed_params\n",
    ")\n",
    "\n",
    "# Save final results\n",
    "with open(f'{SAVE_DIR}/best_config_{TARGET_DATASET.lower().replace(\"-\", \"_\")}.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'dataset': TARGET_DATASET,\n",
    "        'best_k': best_result['best_k'],\n",
    "        'best_latent_dim': best_result['best_latent_dim'],\n",
    "        'final_sil_score': best_result['final_sil_score'],\n",
    "        'final_ch_score': best_result['final_ch_score'],\n",
    "        'final_db_score': best_result['final_db_score'],\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ COMPLETE: {TARGET_DATASET} optimization finished\")\n",
    "print(f\"  Results saved to: {SAVE_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: TRAINING AND VALIDATION FUNCTION\n",
    "# ============================================================================\n",
    "# Purpose: Train autoencoder and evaluate clustering quality in latent space\n",
    "# Key feature: Model quality evaluated by clustering metrics, not just\n",
    "#              reconstruction loss, because goal is finding distinct profiles\n",
    "# ============================================================================\n",
    "\n",
    "def train_and_validate_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Train and validate the autoencoder model and evaluate reconstruction accuracy and latent quality.\n",
    "    \n",
    "    Uses multiple validation methods for K selection with consensus/voting approach:\n",
    "    - Silhouette score (primary, tiebreaker)\n",
    "    - Calinski-Harabasz index\n",
    "    - Davies-Bouldin index\n",
    "    - Elbow method (WCSS-based knee detection)\n",
    "    \n",
    "    K selection: Uses consensus voting - if 2+ methods agree on K, that K is selected.\n",
    "    If no consensus, falls back to silhouette score (most interpretable).\n",
    "    \n",
    "    Returns:\n",
    "        train_losses: list of training loss values\n",
    "        val_losses: list of validation loss values\n",
    "        optimal_k: optimal number of clusters (consensus or silhouette-based)\n",
    "        best_silhouette_score: best silhouette score achieved\n",
    "        latent_vectors: latent representations (train+val combined)\n",
    "        validation_metrics: dict with all K validation metrics and consensus info\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        epoch_training_loss = 0.0\n",
    "        for batch_data in train_loader:\n",
    "            batch_data = batch_data[0].to(device)  # Unpack tuple from DataLoader\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = model(batch_data)\n",
    "            loss = criterion(reconstructed, batch_data)  # MSE Reconstruction Loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_training_loss += loss.item()\n",
    "        avg_train_loss = epoch_training_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        epoch_validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_data in val_loader:\n",
    "                batch_data = batch_data[0].to(device)\n",
    "                reconstructed = model(batch_data)\n",
    "                loss = criterion(reconstructed, batch_data)\n",
    "                epoch_validation_loss += loss.item()\n",
    "        avg_val_loss = epoch_validation_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        model.train()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # EXTRACT LATENT VECTORS AND EVALUATE CLUSTERING QUALITY\n",
    "    # ------------------------------------------------------------------------\n",
    "    # After training, extract latent representations and evaluate clustering\n",
    "    # ------------------------------------------------------------------------\n",
    "    model.eval()\n",
    "    all_latent_vectors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Extract latent vectors from training data\n",
    "        for batch_data in train_loader:\n",
    "            data = batch_data[0].to(device)\n",
    "            latent = model.encoder(data)\n",
    "            all_latent_vectors.append(latent.cpu().numpy())\n",
    "        \n",
    "        # Extract latent vectors from validation data\n",
    "        for batch_data in val_loader:\n",
    "            data = batch_data[0].to(device)\n",
    "            latent = model.encoder(data)\n",
    "            all_latent_vectors.append(latent.cpu().numpy())\n",
    "    \n",
    "    # Combine all latent vectors\n",
    "    latent_vectors = np.vstack(all_latent_vectors)\n",
    "\n",
    "    # DIAGNOSTIC: Check if model collapsed\n",
    "    print(f\"\\n=== LATENT SPACE DIAGNOSTIC ===\")\n",
    "    print(f\"Shape: {latent_vectors.shape}\")\n",
    "    print(f\"Mean per dimension: {latent_vectors.mean(axis=0)}\")\n",
    "    print(f\"Std per dimension: {latent_vectors.std(axis=0)}\")\n",
    "    print(f\"Min per dimension: {latent_vectors.min(axis=0)}\")\n",
    "    print(f\"Max per dimension: {latent_vectors.max(axis=0)}\")\n",
    "    print(f\"Overall std: {latent_vectors.std():.6f}\")\n",
    "    if latent_vectors.std() < 0.01:\n",
    "        print(\"⚠️ MODEL COLLAPSED: All latent vectors are nearly identical!\")\n",
    "        print(\"   This means the encoder isn't learning useful representations.\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # =========================================================================\n",
    "    # K Selection via Multiple Validation Methods (Convergent Validity)\n",
    "    # =========================================================================\n",
    "    k_range = range(2, 7)  # K = 2, 3, 4, 5, 6\n",
    "    \n",
    "    # Initialize metric lists\n",
    "    silhouette_scores_list = []\n",
    "    calinski_harabasz_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "    wcss_values = []  # Within-cluster sum of squares (for elbow method)\n",
    "    \n",
    "    best_silhouette_score = -1\n",
    "    \n",
    "    print(f\"\\nTesting K values: {list(k_range)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for k in k_range:\n",
    "        print(f\"K={k}: Running K-means...\", end=\" \", flush=True)\n",
    "        kmeans = KMeans(n_clusters=k, random_state=RANDOM_SEED, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(latent_vectors)\n",
    "        print(\"Done. \", end=\"\", flush=True)\n",
    "        \n",
    "        # Check if we have at least 2 clusters (required for silhouette score)\n",
    "        n_unique_clusters = len(np.unique(cluster_labels))\n",
    "        if n_unique_clusters < 2:\n",
    "            print(f\"Warning: Only {n_unique_clusters} cluster(s). Skipping metrics.\")\n",
    "            # Assign dummy values that will be ignored\n",
    "            sil_score = -1\n",
    "            ch_score = 0\n",
    "            db_score = float('inf')\n",
    "        else:\n",
    "            print(\"Computing metrics...\", end=\" \", flush=True)\n",
    "            # Compute all validation metrics\n",
    "            sil_score = silhouette_score(latent_vectors, cluster_labels)\n",
    "            ch_score = calinski_harabasz_score(latent_vectors, cluster_labels)\n",
    "            db_score = davies_bouldin_score(latent_vectors, cluster_labels)\n",
    "            print(f\"Sil={sil_score:.4f}, CH={ch_score:.2f}, DB={db_score:.4f}\")\n",
    "        wcss = kmeans.inertia_  # Within-cluster sum of squares\n",
    "        \n",
    "        # Store metrics\n",
    "        silhouette_scores_list.append(sil_score)\n",
    "        calinski_harabasz_scores.append(ch_score)\n",
    "        davies_bouldin_scores.append(db_score)\n",
    "        wcss_values.append(wcss)\n",
    "        \n",
    "        # Track best silhouette score\n",
    "        if sil_score > best_silhouette_score:\n",
    "            best_silhouette_score = sil_score\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"K-means evaluation complete. Best silhouette score: {best_silhouette_score:.4f}\")\n",
    "    print(\"\\nDetermining optimal K using consensus voting...\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Consensus/Voting Approach for K Selection\n",
    "    # =========================================================================\n",
    "    # Determine optimal K from each method\n",
    "    optimal_k_silhouette = k_range[np.argmax(silhouette_scores_list)]\n",
    "    optimal_k_ch = k_range[np.argmax(calinski_harabasz_scores)]  # Highest CH = best\n",
    "    optimal_k_db = k_range[np.argmin(davies_bouldin_scores)]  # Lowest DB = best\n",
    "    \n",
    "    # Elbow Method: Find the \"knee\" where WCSS decrease rate slows down\n",
    "    # Compute percentage decrease in WCSS for each K\n",
    "    wcss_decreases = []\n",
    "    for i in range(1, len(wcss_values)):\n",
    "        if wcss_values[i-1] > 0:  # Avoid division by zero\n",
    "            pct_decrease = ((wcss_values[i-1] - wcss_values[i]) / wcss_values[i-1]) * 100\n",
    "            wcss_decreases.append(pct_decrease)\n",
    "        else:\n",
    "            wcss_decreases.append(0)\n",
    "    \n",
    "    # Find elbow: K where decrease rate drops most (knee point)\n",
    "    # The elbow is where adding more clusters doesn't significantly reduce WCSS\n",
    "    if len(wcss_decreases) > 0:\n",
    "        decrease_rates = np.array(wcss_decreases)\n",
    "        # Method: Find where the decrease rate drops most (elbow detection)\n",
    "        # Compute the rate of change of decrease rates (second derivative of WCSS)\n",
    "        if len(decrease_rates) > 1:\n",
    "            # Rate changes: how much the decrease rate changes between consecutive K\n",
    "            rate_changes = np.diff(decrease_rates)\n",
    "            # Elbow is where rate change is most negative (biggest drop in decrease rate)\n",
    "            # This means: decrease rate was high, then dropped significantly\n",
    "            elbow_idx = np.argmin(rate_changes) + 1  # +1 because diff reduces length by 1\n",
    "            # Ensure index is within valid range\n",
    "            elbow_idx = min(elbow_idx, len(k_range) - 1)\n",
    "            optimal_k_elbow = k_range[elbow_idx]\n",
    "        else:\n",
    "            # Fallback: use K with smallest decrease (conservative)\n",
    "            min_decrease_idx = np.argmin(wcss_decreases) + 1\n",
    "            min_decrease_idx = min(min_decrease_idx, len(k_range) - 1)\n",
    "            optimal_k_elbow = k_range[min_decrease_idx]\n",
    "    else:\n",
    "        # Fallback to silhouette if WCSS calculation fails\n",
    "        optimal_k_elbow = optimal_k_silhouette\n",
    "    \n",
    "    # Consensus voting: If 2+ methods agree, use that K\n",
    "    # Note: Multiple comparisons across 4 metrics and 5 K values (K=2-6) are exploratory\n",
    "    # We use consensus voting rather to find the optimal K\n",
    "  \n",
    "    k_votes = [optimal_k_silhouette, optimal_k_ch, optimal_k_db, optimal_k_elbow]\n",
    "    k_counts = Counter(k_votes)\n",
    "    most_common_k, consensus_count = k_counts.most_common(1)[0]\n",
    "    \n",
    "    if consensus_count >= 2:\n",
    "        # Consensus reached: 2+ methods agree\n",
    "        optimal_k = most_common_k\n",
    "        consensus_status = f\"Consensus: {consensus_count} methods agree on K={optimal_k}\"\n",
    "        consensus_reached = True\n",
    "    else:\n",
    "        # No consensus: fallback to silhouette (most interpretable)\n",
    "        optimal_k = optimal_k_silhouette\n",
    "        consensus_status = f\"No consensus (Sil={optimal_k_silhouette}, CH={optimal_k_ch}, DB={optimal_k_db}, Elbow={optimal_k_elbow}). Using silhouette K={optimal_k}\"\n",
    "        consensus_reached = False\n",
    "    \n",
    "    print(f\"✓ Optimal K selected: {optimal_k} ({consensus_status})\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Package all validation metrics for analysis\n",
    "    validation_metrics = {\n",
    "        'k_values': list(k_range),\n",
    "        'silhouette_scores': silhouette_scores_list,\n",
    "        'calinski_harabasz_scores': calinski_harabasz_scores,\n",
    "        'davies_bouldin_scores': davies_bouldin_scores,\n",
    "        'wcss_values': wcss_values,\n",
    "        'wcss_decreases': wcss_decreases if len(wcss_decreases) > 0 else [],\n",
    "        'optimal_k_silhouette': optimal_k_silhouette,\n",
    "        'optimal_k_ch': optimal_k_ch,\n",
    "        'optimal_k_db': optimal_k_db,\n",
    "        'optimal_k_elbow': optimal_k_elbow,\n",
    "        'optimal_k_consensus': optimal_k,\n",
    "        'consensus_reached': consensus_reached,\n",
    "        'consensus_status': consensus_status,\n",
    "        'k_votes': k_votes\n",
    "    }\n",
    "    \n",
    "    return train_losses, val_losses, optimal_k, best_silhouette_score, latent_vectors, validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef2e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NOTE: This cell previously contained duplicate hyperparameter tuning code\n",
    "# that was causing NameError because kfold and train_val_tensor are only\n",
    "# defined inside run_autoencoder_pipeline() function scope.\n",
    "#\n",
    "# The hyperparameter tuning code is now properly contained within \n",
    "# run_autoencoder_pipeline() function.\n",
    "#\n",
    "# To run the pipeline, use:\n",
    "#   run_autoencoder_pipeline(\"D1-Swiss\")\n",
    "# ============================================================================\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# PCA Comparison: Justify Autoencoder Choice\n",
    "# =========================================================================\n",
    "\n",
    "# LEGACY VERSION (kept for reference):\n",
    "\"\"\"\n",
    "SWISS_DATASET = \"D1-Swiss\"\n",
    "\n",
    "if SWISS_DATASET not in PIPELINE_RESULTS:\n",
    "    print(f\"{SWISS_DATASET} not yet processed. Running autoencoder pipeline once for PCA comparison...\")\n",
    "    run_autoencoder_pipeline(SWISS_DATASET)\n",
    "\n",
    "swiss_results = PIPELINE_RESULTS[SWISS_DATASET]\n",
    "train_val_data = swiss_results['train_val_data']\n",
    "latent_vectors_all = swiss_results['latent_vectors_all']\n",
    "cluster_labels_all = swiss_results['cluster_labels_all']\n",
    "cluster_centroids = swiss_results['cluster_centroids']\n",
    "best_k = swiss_results['best_k']\n",
    "best_latent_dim = swiss_results['best_latent_dim']\n",
    "final_sil_score = swiss_results['final_sil_score']\n",
    "final_ch_score = swiss_results['final_ch_score']\n",
    "final_db_score = swiss_results['final_db_score']\n",
    "\n",
    "print(\"METHOD VALIDATION: PCA vs Autoencoder Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing if autoencoder captures nonlinear patterns better than PCA\")\n",
    "print(\"Testing PCA with all possible dimensions (1-4) to find optimal PCA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "scaler = StandardScaler()\n",
    "train_val_scaled = scaler.fit_transform(train_val_data)\n",
    "\n",
    "# Test PCA with all possible dimensions (1, 2, 3, 4)\n",
    "print(\"\\n1. Testing PCA with all dimensions:\")\n",
    "pca_results = []\n",
    "\n",
    "for pca_dim in range(1, 5):  # 1, 2, 3, 4\n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    pca_latent = pca.fit_transform(train_val_scaled)\n",
    "    explained_var = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "    # Cluster with optimal K (same as autoencoder)\n",
    "    pca_kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_SEED, n_init=10)\n",
    "    pca_cluster_labels = pca_kmeans.fit_predict(pca_latent)\n",
    "\n",
    "    # Evaluate clustering quality\n",
    "    pca_sil_score = silhouette_score(pca_latent, pca_cluster_labels)\n",
    "    pca_ch_score = calinski_harabasz_score(pca_latent, pca_cluster_labels)\n",
    "    pca_db_score = davies_bouldin_score(pca_latent, pca_cluster_labels)\n",
    "\n",
    "    pca_results.append({\n",
    "        'dim': pca_dim,\n",
    "        'explained_var': explained_var,\n",
    "        'silhouette': pca_sil_score,\n",
    "        'calinski_harabasz': pca_ch_score,\n",
    "        'davies_bouldin': pca_db_score\n",
    "    })\n",
    "\n",
    "    print(f\"   PCA dim={pca_dim}: Sil={pca_sil_score:.6f}, CH={pca_ch_score:.6f}, DB={pca_db_score:.6f}, Var={explained_var:.4f}\")\n",
    "\n",
    "# Find best PCA configuration (by silhouette score)\n",
    "pca_results_df = pd.DataFrame(pca_results)\n",
    "best_pca_idx = pca_results_df['silhouette'].idxmax()\n",
    "best_pca_config = pca_results_df.iloc[best_pca_idx]\n",
    "\n",
    "print(f\"\\n   Best PCA: dim={int(best_pca_config['dim'])}, Sil={best_pca_config['silhouette']:.6f}\")\n",
    "\n",
    "# Get best PCA latent vectors for visualization\n",
    "best_pca_dim = int(best_pca_config['dim'])\n",
    "best_pca = PCA(n_components=best_pca_dim)\n",
    "pca_latent_best = best_pca.fit_transform(train_val_scaled)\n",
    "pca_kmeans_best = KMeans(n_clusters=best_k, random_state=RANDOM_SEED, n_init=10)\n",
    "pca_cluster_labels_best = pca_kmeans_best.fit_predict(pca_latent_best)\n",
    "pca_centroids_best = pca_kmeans_best.cluster_centers_\n",
    "\n",
    "# Autoencoder results (from Cell 7)\n",
    "print(\"\\n2. Autoencoder (Nonlinear Method):\")\n",
    "print(f\"   AE latent dim: {best_latent_dim}\")\n",
    "print(f\"   AE latent vectors shape: {latent_vectors_all.shape}\")\n",
    "print(f\"   AE Clustering Quality:\")\n",
    "print(f\"     Silhouette Score: {final_sil_score:.6f}\")\n",
    "print(f\"     Calinski-Harabasz: {final_ch_score:.6f}\")\n",
    "print(f\"     Davies-Bouldin: {final_db_score:.6f}\")\n",
    "\n",
    "# Comparison: Best PCA vs Autoencoder\n",
    "print(\"\\n3. Comparison (Best PCA vs Autoencoder):\")\n",
    "best_pca_sil = best_pca_config['silhouette']\n",
    "best_pca_ch = best_pca_config['calinski_harabasz']\n",
    "best_pca_db = best_pca_config['davies_bouldin']\n",
    "\n",
    "sil_improvement = ((final_sil_score - best_pca_sil) / best_pca_sil) * 100\n",
    "ch_improvement = ((final_ch_score - best_pca_ch) / best_pca_ch) * 100\n",
    "db_improvement = ((best_pca_db - final_db_score) / best_pca_db) * 100  # DB: lower is better\n",
    "\n",
    "print(f\"   Silhouette: AE {final_sil_score:.6f} vs Best PCA {best_pca_sil:.6f} ({sil_improvement:+.2f}%)\")\n",
    "print(f\"   Calinski-Harabasz: AE {final_ch_score:.6f} vs Best PCA {best_pca_ch:.6f} ({ch_improvement:+.2f}%)\")\n",
    "print(f\"   Davies-Bouldin: AE {final_db_score:.6f} vs Best PCA {best_pca_db:.6f} ({db_improvement:+.2f}% improvement)\")\n",
    "\n",
    "# Conclusion\n",
    "print(\"\\n4. Conclusion:\")\n",
    "if final_sil_score > best_pca_sil:\n",
    "    print(f\"   ✓ Autoencoder achieves {sil_improvement:.2f}% better silhouette score than best PCA\")\n",
    "    print(f\"   → Suggests latent structure contains nonlinear patterns\")\n",
    "    print(f\"   → Justifies use of autoencoder over linear PCA\")\n",
    "else:\n",
    "    print(f\"   Note: Best PCA (dim={best_pca_dim}) performs similarly ({best_pca_sil:.6f} vs {final_sil_score:.6f})\")\n",
    "    print(f\"   → Linear patterns may be sufficient, but AE provides flexibility\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Visualization: Compare latent spaces\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "if best_pca_dim == 2:\n",
    "    scatter1 = axes[0].scatter(pca_latent_best[:, 0], pca_latent_best[:, 1], c=pca_cluster_labels_best, \n",
    "                              cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[0].scatter(pca_centroids_best[:, 0], pca_centroids_best[:, 1], c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[0].set_xlabel('PC1', fontsize=12)\n",
    "    axes[0].set_ylabel('PC2', fontsize=12)\n",
    "    axes[0].set_title(f'Best PCA (dim={best_pca_dim}, Linear)\\nSilhouette: {best_pca_sil:.4f}', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    scatter2 = axes[1].scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], c=cluster_labels_all,\n",
    "                              cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[1].scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[1].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "    axes[1].set_ylabel('Latent Dim 2', fontsize=12)\n",
    "    axes[1].set_title(f'Autoencoder (dim={best_latent_dim}, Nonlinear)\\nSilhouette: {final_sil_score:.4f}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "elif best_pca_dim == 3 or best_latent_dim == 3:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "    if best_pca_dim >= 3:\n",
    "        ax1.scatter(pca_latent_best[:, 0], pca_latent_best[:, 1], pca_latent_best[:, 2], \n",
    "                   c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax1.scatter(pca_centroids_best[:, 0], pca_centroids_best[:, 1], pca_centroids_best[:, 2], \n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    else:\n",
    "        pca_3d = np.zeros((len(pca_latent_best), 3))\n",
    "        pca_3d[:, :best_pca_dim] = pca_latent_best\n",
    "        ax1.scatter(pca_3d[:, 0], pca_3d[:, 1], pca_3d[:, 2], \n",
    "                   c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "    ax1.set_title(f'Best PCA (dim={best_pca_dim})')\n",
    "\n",
    "    if best_latent_dim >= 3:\n",
    "        ax2.scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], latent_vectors_all[:, 2], \n",
    "                   c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax2.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], cluster_centroids[:, 2], \n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    else:\n",
    "        ae_3d = np.zeros((len(latent_vectors_all), 3))\n",
    "        ae_3d[:, :best_latent_dim] = latent_vectors_all\n",
    "        ax2.scatter(ae_3d[:, 0], ae_3d[:, 1], ae_3d[:, 2], \n",
    "                   c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax2.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], np.zeros(len(cluster_centroids)), \n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    ax2.set_xlabel('Latent Dim 1')\n",
    "    ax2.set_ylabel('Latent Dim 2')\n",
    "    ax2.set_zlabel('Latent Dim 3')\n",
    "    ax2.set_title(f'Autoencoder (dim={best_latent_dim})\\nSil: {final_sil_score:.4f} vs PCA: {best_pca_sil:.4f}')\n",
    "else:\n",
    "    scatter1 = axes[0].scatter(pca_latent_best[:, 0], np.zeros(len(pca_latent_best)), \n",
    "                              c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[0].scatter(pca_centroids_best[:, 0], np.zeros(len(pca_centroids_best)), \n",
    "                   c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[0].set_xlabel('PC1', fontsize=12)\n",
    "    axes[0].set_ylabel('(Projected)', fontsize=12)\n",
    "    axes[0].set_title(f'Best PCA (dim={best_pca_dim})', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    scatter2 = axes[1].scatter(latent_vectors_all[:, 0], np.zeros(len(latent_vectors_all)) if best_latent_dim == 1 else latent_vectors_all[:, 1], \n",
    "                              c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[1].scatter(cluster_centroids[:, 0], np.zeros(len(cluster_centroids)) if best_latent_dim == 1 else cluster_centroids[:, 1], \n",
    "                   c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[1].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "    axes[1].set_ylabel('Latent Dim 2' if best_latent_dim >= 2 else '(Projected)', fontsize=12)\n",
    "    axes[1].set_title(f'Autoencoder (dim={best_latent_dim})', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n5. Summary Table:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Best PCA', 'Autoencoder'],\n",
    "    'Latent_Dim': [best_pca_dim, best_latent_dim],\n",
    "    'Silhouette': [best_pca_sil, final_sil_score],\n",
    "    'Calinski_Harabasz': [best_pca_ch, final_ch_score],\n",
    "    'Davies_Bouldin': [best_pca_db, final_db_score]\n",
    "})\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "# ACTIVE VERSION (executes):\n",
    "SWISS_DATASET = \"D1-Swiss\"\n",
    "\n",
    "if SWISS_DATASET not in PIPELINE_RESULTS:\n",
    "    print(f\"{SWISS_DATASET} not yet processed. Running autoencoder pipeline once for PCA comparison...\")\n",
    "    run_autoencoder_pipeline(SWISS_DATASET)\n",
    "\n",
    "swiss_results = PIPELINE_RESULTS[SWISS_DATASET]\n",
    "train_val_data = swiss_results['train_val_data']\n",
    "latent_vectors_all = swiss_results['latent_vectors_all']\n",
    "cluster_labels_all = swiss_results['cluster_labels_all']\n",
    "cluster_centroids = swiss_results['cluster_centroids']\n",
    "best_k = swiss_results['best_k']\n",
    "best_latent_dim = swiss_results['best_latent_dim']\n",
    "final_sil_score = swiss_results['final_sil_score']\n",
    "final_ch_score = swiss_results['final_ch_score']\n",
    "final_db_score = swiss_results['final_db_score']\n",
    "\n",
    "print(\"METHOD VALIDATION: PCA vs Autoencoder Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing if autoencoder captures nonlinear patterns better than PCA\")\n",
    "print(\"Testing PCA with all possible dimensions (1-4) to find optimal PCA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "scaler = StandardScaler()\n",
    "train_val_scaled = scaler.fit_transform(train_val_data)\n",
    "\n",
    "# Test PCA with all possible dimensions (1, 2, 3, 4)\n",
    "print(\"\\n1. Testing PCA with all dimensions:\")\n",
    "pca_results = []\n",
    "\n",
    "for pca_dim in range(1, 5):  # 1, 2, 3, 4\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    pca_latent = pca.fit_transform(train_val_scaled)\n",
    "    explained_var = pca.explained_variance_ratio_.sum()\n",
    "\n",
    "    pca_kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_SEED, n_init=10)\n",
    "    pca_cluster_labels = pca_kmeans.fit_predict(pca_latent)\n",
    "\n",
    "    pca_sil_score = silhouette_score(pca_latent, pca_cluster_labels)\n",
    "    pca_ch_score = calinski_harabasz_score(pca_latent, pca_cluster_labels)\n",
    "    pca_db_score = davies_bouldin_score(pca_latent, pca_cluster_labels)\n",
    "\n",
    "    pca_results.append({\n",
    "        'dim': pca_dim,\n",
    "        'explained_var': explained_var,\n",
    "        'silhouette': pca_sil_score,\n",
    "        'calinski_harabasz': pca_ch_score,\n",
    "        'davies_bouldin': pca_db_score\n",
    "    })\n",
    "\n",
    "    print(f\"   PCA dim={pca_dim}: Sil={pca_sil_score:.6f}, CH={pca_ch_score:.6f}, DB={pca_db_score:.6f}, Var={explained_var:.4f}\")\n",
    "\n",
    "pca_results_df = pd.DataFrame(pca_results)\n",
    "best_pca_idx = pca_results_df['silhouette'].idxmax()\n",
    "best_pca_config = pca_results_df.iloc[best_pca_idx]\n",
    "\n",
    "print(f\"\\n   Best PCA: dim={int(best_pca_config['dim'])}, Sil={best_pca_config['silhouette']:.6f}\")\n",
    "\n",
    "best_pca_dim = int(best_pca_config['dim'])\n",
    "best_pca = PCA(n_components=best_pca_dim)\n",
    "pca_latent_best = best_pca.fit_transform(train_val_scaled)\n",
    "pca_kmeans_best = KMeans(n_clusters=best_k, random_state=RANDOM_SEED, n_init=10)\n",
    "pca_cluster_labels_best = pca_kmeans_best.fit_predict(pca_latent_best)\n",
    "pca_centroids_best = pca_kmeans_best.cluster_centers_\n",
    "\n",
    "print(\"\\n2. Autoencoder (Nonlinear Method):\")\n",
    "print(f\"   AE latent dim: {best_latent_dim}\")\n",
    "print(f\"   AE latent vectors shape: {latent_vectors_all.shape}\")\n",
    "print(\"   AE Clustering Quality:\")\n",
    "print(f\"     Silhouette Score: {final_sil_score:.6f}\")\n",
    "print(f\"     Calinski-Harabasz: {final_ch_score:.6f}\")\n",
    "print(f\"     Davies-Bouldin: {final_db_score:.6f}\")\n",
    "\n",
    "print(\"\\n3. Comparison (Best PCA vs Autoencoder):\")\n",
    "best_pca_sil = best_pca_config['silhouette']\n",
    "best_pca_ch = best_pca_config['calinski_harabasz']\n",
    "best_pca_db = best_pca_config['davies_bouldin']\n",
    "\n",
    "sil_improvement = ((final_sil_score - best_pca_sil) / best_pca_sil) * 100\n",
    "ch_improvement = ((final_ch_score - best_pca_ch) / best_pca_ch) * 100\n",
    "db_improvement = ((best_pca_db - final_db_score) / best_pca_db) * 100\n",
    "\n",
    "print(f\"   Silhouette: AE {final_sil_score:.6f} vs Best PCA {best_pca_sil:.6f} ({sil_improvement:+.2f}%)\")\n",
    "print(f\"   Calinski-Harabasz: AE {final_ch_score:.6f} vs Best PCA {best_pca_ch:.6f} ({ch_improvement:+.2f}%)\")\n",
    "print(f\"   Davies-Bouldin: AE {final_db_score:.6f} vs Best PCA {best_pca_db:.6f} ({db_improvement:+.2f}% improvement)\")\n",
    "\n",
    "print(\"\\n4. Conclusion:\")\n",
    "if final_sil_score > best_pca_sil:\n",
    "    print(f\"   ✓ Autoencoder achieves {sil_improvement:.2f}% better silhouette score than best PCA\")\n",
    "    print(f\"   → Suggests latent structure contains nonlinear patterns\")\n",
    "    print(f\"   → Justifies use of autoencoder over linear PCA\")\n",
    "else:\n",
    "    print(f\"   Note: Best PCA (dim={best_pca_dim}) performs similarly ({best_pca_sil:.6f} vs {final_sil_score:.6f})\")\n",
    "    print(f\"   → Linear patterns may be sufficient, but AE provides flexibility\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "if best_pca_dim == 2:\n",
    "    scatter1 = axes[0].scatter(pca_latent_best[:, 0], pca_latent_best[:, 1], c=pca_cluster_labels_best,\n",
    "                              cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[0].scatter(pca_centroids_best[:, 0], pca_centroids_best[:, 1], c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[0].set_xlabel('PC1', fontsize=12)\n",
    "    axes[0].set_ylabel('PC2', fontsize=12)\n",
    "    axes[0].set_title(f'Best PCA (dim={best_pca_dim}, Linear)\\nSilhouette: {best_pca_sil:.4f}', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    scatter2 = axes[1].scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], c=cluster_labels_all,\n",
    "                              cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[1].scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[1].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "    axes[1].set_ylabel('Latent Dim 2', fontsize=12)\n",
    "    axes[1].set_title(f'Autoencoder (dim={best_latent_dim}, Nonlinear)\\nSilhouette: {final_sil_score:.4f}', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "elif best_pca_dim == 3 or best_latent_dim == 3:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "    if best_pca_dim >= 3:\n",
    "        ax1.scatter(pca_latent_best[:, 0], pca_latent_best[:, 1], pca_latent_best[:, 2],\n",
    "                   c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax1.scatter(pca_centroids_best[:, 0], pca_centroids_best[:, 1], pca_centroids_best[:, 2],\n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    else:\n",
    "        pca_3d = np.zeros((len(pca_latent_best), 3))\n",
    "        pca_3d[:, :best_pca_dim] = pca_latent_best\n",
    "        ax1.scatter(pca_3d[:, 0], pca_3d[:, 1], pca_3d[:, 2],\n",
    "                   c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "    ax1.set_xlabel('PC1')\n",
    "    ax1.set_ylabel('PC2')\n",
    "    ax1.set_zlabel('PC3')\n",
    "    ax1.set_title(f'Best PCA (dim={best_pca_dim})')\n",
    "\n",
    "    if best_latent_dim >= 3:\n",
    "        ax2.scatter(latent_vectors_all[:, 0], latent_vectors_all[:, 1], latent_vectors_all[:, 2],\n",
    "                   c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax2.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], cluster_centroids[:, 2],\n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    else:\n",
    "        ae_3d = np.zeros((len(latent_vectors_all), 3))\n",
    "        ae_3d[:, :best_latent_dim] = latent_vectors_all\n",
    "        ax2.scatter(ae_3d[:, 0], ae_3d[:, 1], ae_3d[:, 2],\n",
    "                   c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax2.scatter(cluster_centroids[:, 0], cluster_centroids[:, 1], np.zeros(len(cluster_centroids)),\n",
    "                   c='red', marker='x', s=200, linewidths=3)\n",
    "    ax2.set_xlabel('Latent Dim 1')\n",
    "    ax2.set_ylabel('Latent Dim 2')\n",
    "    ax2.set_zlabel('Latent Dim 3')\n",
    "    ax2.set_title(f'Autoencoder (dim={best_latent_dim})\\nSil: {final_sil_score:.4f} vs PCA: {best_pca_sil:.4f}')\n",
    "else:\n",
    "    scatter1 = axes[0].scatter(pca_latent_best[:, 0], np.zeros(len(pca_latent_best)),\n",
    "                              c=pca_cluster_labels_best, cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[0].scatter(pca_centroids_best[:, 0], np.zeros(len(pca_centroids_best)),\n",
    "                   c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[0].set_xlabel('PC1', fontsize=12)\n",
    "    axes[0].set_ylabel('(Projected)', fontsize=12)\n",
    "    axes[0].set_title(f'Best PCA (dim={best_pca_dim})', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    scatter2 = axes[1].scatter(latent_vectors_all[:, 0], np.zeros(len(latent_vectors_all)) if best_latent_dim == 1 else latent_vectors_all[:, 1],\n",
    "                              c=cluster_labels_all, cmap='viridis', alpha=0.6, s=20)\n",
    "    axes[1].scatter(cluster_centroids[:, 0], np.zeros(len(cluster_centroids)) if best_latent_dim == 1 else cluster_centroids[:, 1],\n",
    "                   c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    axes[1].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "    axes[1].set_ylabel('Latent Dim 2' if best_latent_dim >= 2 else '(Projected)', fontsize=12)\n",
    "    axes[1].set_title(f'Autoencoder (dim={best_latent_dim})', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "    plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n5. Summary Table:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Best PCA', 'Autoencoder'],\n",
    "    'Latent_Dim': [best_pca_dim, best_latent_dim],\n",
    "    'Silhouette': [best_pca_sil, final_sil_score],\n",
    "    'Calinski_Harabasz': [best_pca_ch, final_ch_score],\n",
    "    'Davies_Bouldin': [best_pca_db, final_db_score]\n",
    "})\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c731bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# REPLICATION TESTING: H1/H2 Hypotheses (D1-Swiss as Reference)\n",
    "# =========================================================================\n",
    "# Only matches latent_dim (required for comparison), K can differ\n",
    "# Reuses existing run_autoencoder_pipeline() function\n",
    "# =========================================================================\n",
    "\n",
    "# Additional imports (already imported in Cell 0, but included for clarity)\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Configuration constants\n",
    "EXTERNAL_DATASETS = [\"D2-Cultural\", \"D3-Academic\", \"D4-Tech\"]\n",
    "REPLICATION_THRESHOLD_H1 = 0.70\n",
    "REPLICATION_THRESHOLD_H2 = 0.50\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REPLICATION TESTING: Testing Profile Generalizability\")\n",
    "print(\"=\"*80)\n",
    "print(\"Hypothesis H1: Universal profiles (r > 0.70)\")\n",
    "print(\"Hypothesis H2: Context-specific profiles (r < 0.50)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Reference: D1-Swiss (only latent_dim must match for comparison)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 1: PROCESS D1-SWISS (REFERENCE DATASET)\n",
    "# ========================================================================\n",
    "print(\"\\nStep 1: Processing D1-Swiss (reference dataset)...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if \"D1-Swiss\" not in PIPELINE_RESULTS:\n",
    "    print(\"Running pipeline on D1-Swiss...\")\n",
    "    run_autoencoder_pipeline(\"D1-Swiss\")\n",
    "else:\n",
    "    print(\"D1-Swiss already processed ✓\")\n",
    "\n",
    "d1_results = PIPELINE_RESULTS[\"D1-Swiss\"]\n",
    "d1_latent_dim = d1_results['best_latent_dim']\n",
    "d1_k = d1_results['best_k']\n",
    "d1_centroids = d1_results['cluster_centroids']\n",
    "\n",
    "print(f\"\\nD1-Swiss Reference Configuration:\")\n",
    "print(f\"  Optimal K: {d1_k}\")\n",
    "print(f\"  Latent dimension: {d1_latent_dim}D\")\n",
    "print(f\"  Centroids shape: {d1_centroids.shape}\")\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 2: PROCESS EXTERNAL DATASETS AND RETUNE IF NEEDED\n",
    "# ========================================================================\n",
    "print(f\"\\nStep 2: Processing external datasets and retuning to {d1_latent_dim}D if needed...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "standardized_results = {}\n",
    "\n",
    "for ext_dataset in EXTERNAL_DATASETS:\n",
    "    print(f\"\\nProcessing {ext_dataset}...\")\n",
    "    \n",
    "    # Run pipeline if not already processed\n",
    "    if ext_dataset not in PIPELINE_RESULTS:\n",
    "        print(f\"  Running pipeline on {ext_dataset}...\")\n",
    "        run_autoencoder_pipeline(ext_dataset)\n",
    "    \n",
    "    ext_result = PIPELINE_RESULTS[ext_dataset]\n",
    "    ext_latent_dim = ext_result['best_latent_dim']\n",
    "    \n",
    "    print(f\"  Original latent dimensions: {ext_latent_dim}D\")\n",
    "    \n",
    "    if ext_latent_dim == d1_latent_dim:\n",
    "        print(f\"  ✓ Latent dimension matches D1-Swiss ({d1_latent_dim}D)\")\n",
    "        standardized_results[ext_dataset] = ext_result.copy()\n",
    "    else:\n",
    "        print(f\"  ⚠ Dimension mismatch: {ext_latent_dim}D ≠ {d1_latent_dim}D\")\n",
    "        print(f\"  → Retuning to match D1-Swiss latent_dim ({d1_latent_dim}D)...\")\n",
    "        print(f\"    (K can differ - only latent_dim needs to match for comparison)\")\n",
    "        \n",
    "        retrained_result = run_autoencoder_pipeline(\n",
    "            ext_dataset, \n",
    "            force_latent_dim=d1_latent_dim\n",
    "        )\n",
    "        \n",
    "        original_recon_loss = ext_result.get('reconstruction_loss', None)\n",
    "        \n",
    "        if original_recon_loss is not None:\n",
    "            retrained_recon_loss = retrained_result.get('reconstruction_loss', None)\n",
    "            \n",
    "            if retrained_recon_loss is not None:\n",
    "                if original_recon_loss != 0:\n",
    "                    info_loss_pct = (\n",
    "                        (retrained_recon_loss - original_recon_loss) / original_recon_loss\n",
    "                    ) * 100\n",
    "                    \n",
    "                    print(f\"\\n  Information Loss Analysis ({ext_latent_dim}D → {d1_latent_dim}D):\")\n",
    "                    print(f\"    Original recon loss ({ext_latent_dim}D): {original_recon_loss:.6f}\")\n",
    "                    print(f\"    Retuned recon loss ({d1_latent_dim}D): {retrained_recon_loss:.6f}\")\n",
    "                    print(f\"    Increase: {info_loss_pct:+.2f}% (higher = more information lost)\")\n",
    "                    \n",
    "                    retrained_result['info_loss_pct'] = info_loss_pct\n",
    "                    retrained_result['original_recon_loss'] = original_recon_loss\n",
    "                else:\n",
    "                    print(f\"\\n  Information Loss Analysis ({ext_latent_dim}D → {d1_latent_dim}D):\")\n",
    "                    print(f\"    Original recon loss ({ext_latent_dim}D): {original_recon_loss:.6f}\")\n",
    "                    print(f\"    Retuned recon loss ({d1_latent_dim}D): {retrained_recon_loss:.6f}\")\n",
    "                    print(f\"    Note: Cannot calculate percentage change (original loss is 0)\")\n",
    "        \n",
    "        standardized_results[ext_dataset] = retrained_result\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 3: COMPARE PROFILE CHARACTERISTICS ACROSS DATASETS\n",
    "# ========================================================================\n",
    "# PRIMARY: Symptom pattern comparison (natural K - preserves structure)\n",
    "# SECONDARY: Forced K matching (robustness check - standardized comparison)\n",
    "# ========================================================================\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Step 3: Comparing profile characteristics across datasets\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"METHODOLOGICAL APPROACH:\")\n",
    "print(\"  PRIMARY: Symptom pattern comparison (natural K)\")\n",
    "print(\"    - Preserves each dataset's optimal K (natural structure)\")\n",
    "print(\"    - Compares actual symptom patterns (Depression, Anxiety, Stress, Burnout)\")\n",
    "print(\"    - Uses Hungarian algorithm to match best subset when K differs\")\n",
    "print(\"    - Tests: 'Are symptom patterns similar across datasets?'\")\n",
    "print(\"  SECONDARY: Forced K matching (robustness check)\")\n",
    "print(\"    - Re-clusters external datasets to match D1's K\")\n",
    "print(\"    - Standardized comparison with same structure\")\n",
    "print(\"    - Tests: 'Can we find matching profiles if we force same structure?'\")\n",
    "print(\"\\nNOTE: Different optimal K values are interpreted as evidence for H2\")\n",
    "print(\"      (context-specificity), but this is integrated into interpretation,\")\n",
    "print(\"      not a separate comparison method.\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Extract D1-Swiss profile characteristics\n",
    "d1_profile_summary = d1_results.get('profile_summary', [])\n",
    "if not d1_profile_summary:\n",
    "    print(\"⚠ WARNING: D1-Swiss profile_summary not found. Falling back to centroid comparison.\")\n",
    "    d1_profile_summary = None\n",
    "\n",
    "replication_results = []\n",
    "\n",
    "for ext_dataset in EXTERNAL_DATASETS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Comparing {ext_dataset} profiles to D1-Swiss\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    ext_result = standardized_results[ext_dataset]\n",
    "    ext_k = ext_result['best_k']\n",
    "    ext_profile_summary = ext_result.get('profile_summary', [])\n",
    "    \n",
    "    print(f\"  D1-Swiss K: {d1_k}\")\n",
    "    print(f\"  {ext_dataset} K: {ext_k}\")\n",
    "    \n",
    "    # Check if K differs (preliminary evidence for interpretation)\n",
    "    k_different = (ext_k != d1_k)\n",
    "    if k_different:\n",
    "        print(f\"     Note: Different optimal K ({d1_k} vs {ext_k}) suggests context-specificity\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # PRIMARY METHOD: Symptom Pattern Comparison (Natural K)\n",
    "    # ====================================================================\n",
    "    if d1_profile_summary and ext_profile_summary:\n",
    "        print(f\"\\n  PRIMARY: Symptom Pattern Comparison (Natural K)\")\n",
    "        print(f\"  {'-'*70}\")\n",
    "        print(f\"    Testing: 'Are symptom patterns similar across datasets?'\")\n",
    "        if ext_k != d1_k:\n",
    "            print(f\"    Note: Comparing {min(d1_k, ext_k)} of {max(d1_k, ext_k)} profiles\")\n",
    "            print(f\"    → Preserving natural structure (not forcing K matching)\")\n",
    "            print(f\"    → Extra profiles in larger dataset indicate additional structure\")\n",
    "        \n",
    "        # Extract symptom vectors (4 features: Depression, Anxiety, Stress, Burnout)\n",
    "        d1_symptom_matrix = np.array([\n",
    "            [p['Depression'], p['Anxiety'], p['Stress'], p['Burnout']]\n",
    "            for p in d1_profile_summary\n",
    "        ])\n",
    "        \n",
    "        ext_symptom_matrix = np.array([\n",
    "            [p['Depression'], p['Anxiety'], p['Stress'], p['Burnout']]\n",
    "            for p in ext_profile_summary\n",
    "        ])\n",
    "        \n",
    "        print(f\"    D1-Swiss symptom matrix: {d1_symptom_matrix.shape} (K={d1_k} × 4 features)\")\n",
    "        print(f\"    {ext_dataset} symptom matrix: {ext_symptom_matrix.shape} (K={ext_k} × 4 features)\")\n",
    "        \n",
    "        # Match profiles using Hungarian Algorithm on symptom patterns\n",
    "        # Distance = Euclidean distance in 4D symptom space\n",
    "        symptom_distance_matrix = cdist(d1_symptom_matrix, ext_symptom_matrix, metric='euclidean')\n",
    "        row_indices, col_indices = linear_sum_assignment(symptom_distance_matrix)\n",
    "        ext_symptom_matched = ext_symptom_matrix[col_indices]\n",
    "        \n",
    "        # Calculate feature-wise correlations\n",
    "        feature_correlations = []\n",
    "        feature_p_values = []\n",
    "        feature_names = ['Depression', 'Anxiety', 'Stress', 'Burnout']\n",
    "        \n",
    "        for feat_idx, feat_name in enumerate(feature_names):\n",
    "            d1_feat = d1_symptom_matrix[:, feat_idx]\n",
    "            ext_feat = ext_symptom_matched[:, feat_idx]\n",
    "            # Use minimum K for correlation (smaller sample size)\n",
    "            min_k = min(d1_k, ext_k)\n",
    "            correlation, p_value = pearsonr(d1_feat[:min_k], ext_feat[:min_k])\n",
    "            feature_correlations.append(correlation)\n",
    "            feature_p_values.append(p_value)\n",
    "        \n",
    "        # Calculate overall correlation on matched symptom profiles\n",
    "        # Flatten both matrices and compute correlation\n",
    "        d1_symptom_flat = d1_symptom_matrix.flatten()\n",
    "        ext_symptom_flat = ext_symptom_matched.flatten()\n",
    "        # Use minimum length to handle different K values\n",
    "        min_len = min(len(d1_symptom_flat), len(ext_symptom_flat))\n",
    "        overall_symptom_corr, overall_symptom_p = pearsonr(\n",
    "            d1_symptom_flat[:min_len], \n",
    "            ext_symptom_flat[:min_len]\n",
    "        )\n",
    "        \n",
    "        avg_feature_corr = np.mean(feature_correlations)\n",
    "        \n",
    "        print(f\"\\n    Feature-wise correlations (n={min(d1_k, ext_k)} profiles):\")\n",
    "        for feat_name, corr, p_val in zip(feature_names, feature_correlations, feature_p_values):\n",
    "            print(f\"      {feat_name}: r={corr:.4f} (p={p_val:.6f})\")\n",
    "        print(f\"    Average feature correlation: {avg_feature_corr:.4f}\")\n",
    "        print(f\"    Overall symptom pattern correlation: {overall_symptom_corr:.4f} (p={overall_symptom_p:.6f}, n={min_len})\")\n",
    "        \n",
    "        # Test hypotheses using symptom pattern correlation (PRIMARY)\n",
    "        h1_supported_primary = overall_symptom_corr > REPLICATION_THRESHOLD_H1\n",
    "        h2_supported_primary = overall_symptom_corr < REPLICATION_THRESHOLD_H2\n",
    "        \n",
    "        print(f\"\\n    Symptom Pattern Correlation (PRIMARY TEST):\")\n",
    "        print(f\"      Correlation: r={overall_symptom_corr:.4f}\")\n",
    "        print(f\"      H1 threshold (r > {REPLICATION_THRESHOLD_H1}): {'Met' if h1_supported_primary else 'Not Met'}\")\n",
    "        print(f\"      H2 threshold (r < {REPLICATION_THRESHOLD_H2}): {'Met' if h2_supported_primary else 'Not Met'}\")\n",
    "        \n",
    "        # Combined interpretation: K difference informs interpretation, symptom correlation is PRIMARY\n",
    "        print(f\"\\n    INTERPRETATION:\")\n",
    "        if k_different:\n",
    "            print(f\"      Preliminary: Different K ({d1_k} vs {ext_k}) → Suggests context-specificity\")\n",
    "            if h2_supported_primary:\n",
    "                print(f\"      PRIMARY: Low symptom correlation (r={overall_symptom_corr:.4f}) → CONFIRMS H2\")\n",
    "                print(f\"      → CONCLUSION: CONTEXT-SPECIFIC (both K and patterns differ)\")\n",
    "            elif h1_supported_primary:\n",
    "                print(f\"      PRIMARY: High symptom correlation (r={overall_symptom_corr:.4f}) → CONFLICTS with K evidence\")\n",
    "                print(f\"      → CONCLUSION: CONTEXT-SPECIFIC (K differs, but some patterns similar)\")\n",
    "                print(f\"      → Note: Different K is strong evidence for context-specificity\")\n",
    "            else:\n",
    "                print(f\"      PRIMARY: Moderate correlation (r={overall_symptom_corr:.4f}) → NEUTRAL\")\n",
    "                print(f\"      → CONCLUSION: CONTEXT-SPECIFIC (K difference is strong evidence)\")\n",
    "        else:\n",
    "            # Same K, so symptom correlation is the deciding factor\n",
    "            print(f\"      Preliminary: Same K ({d1_k}) → Neutral\")\n",
    "            if h1_supported_primary:\n",
    "                print(f\"      PRIMARY: High symptom correlation (r={overall_symptom_corr:.4f}) → SUPPORTS H1\")\n",
    "                print(f\"      → CONCLUSION: UNIVERSAL (same structure, similar patterns)\")\n",
    "            elif h2_supported_primary:\n",
    "                print(f\"      PRIMARY: Low symptom correlation (r={overall_symptom_corr:.4f}) → SUPPORTS H2\")\n",
    "                print(f\"      → CONCLUSION: CONTEXT-SPECIFIC (same structure, different patterns)\")\n",
    "            else:\n",
    "                print(f\"      PRIMARY: Moderate correlation (r={overall_symptom_corr:.4f}) → NEUTRAL\")\n",
    "                print(f\"      → CONCLUSION: MIXED (same structure, moderate pattern similarity)\")\n",
    "        \n",
    "        primary_correlation = overall_symptom_corr\n",
    "        primary_p_value = overall_symptom_p\n",
    "        h1_supported_primary = h1_supported_primary\n",
    "        h2_supported_primary = h2_supported_primary\n",
    "    else:\n",
    "        print(f\"\\n   Profile characteristics not available, skipping symptom pattern comparison\")\n",
    "        primary_correlation = None\n",
    "        primary_p_value = None\n",
    "        h1_supported_primary = False\n",
    "        h2_supported_primary = False\n",
    "    \n",
    "    # ====================================================================\n",
    "    # SECONDARY METHOD: Forced K Matching (Robustness Check)\n",
    "    # ====================================================================\n",
    "    print(f\"\\n  SECONDARY: Forced K Matching (Robustness Check)\")\n",
    "    print(f\"  {'-'*70}\")\n",
    "    \n",
    "    ext_centroids = ext_result['cluster_centroids']\n",
    "    \n",
    "    # Re-cluster if K differs\n",
    "    if ext_k != d1_k:\n",
    "        print(f\"    Re-clustering {ext_dataset} with K={d1_k} to match D1-Swiss...\")\n",
    "        ext_latent_vectors = ext_result['latent_vectors_all']\n",
    "        ext_kmeans = KMeans(n_clusters=d1_k, random_state=RANDOM_SEED, n_init=10)\n",
    "        ext_cluster_labels = ext_kmeans.fit_predict(ext_latent_vectors)\n",
    "        ext_centroids = ext_kmeans.cluster_centers_\n",
    "        ext_k_forced = d1_k\n",
    "    else:\n",
    "        ext_k_forced = ext_k\n",
    "    \n",
    "    # Match centroids using Hungarian Algorithm\n",
    "    d1_centroids = d1_results['cluster_centroids']\n",
    "    distance_matrix = cdist(d1_centroids, ext_centroids, metric='euclidean')\n",
    "    row_indices, col_indices = linear_sum_assignment(distance_matrix)\n",
    "    ext_centroids_matched = ext_centroids[col_indices]\n",
    "    \n",
    "    # Calculate dimension-wise correlations\n",
    "    dim_correlations = []\n",
    "    dim_p_values = []\n",
    "    \n",
    "    for dim in range(d1_latent_dim):\n",
    "        d1_dim = d1_centroids[:, dim]\n",
    "        ext_dim = ext_centroids_matched[:, dim]\n",
    "        correlation, p_value = pearsonr(d1_dim, ext_dim)\n",
    "        dim_correlations.append(correlation)\n",
    "        dim_p_values.append(p_value)\n",
    "    \n",
    "    avg_correlation = np.mean(dim_correlations)\n",
    "    \n",
    "    # Overall correlation on centroids\n",
    "    d1_flat = d1_centroids.flatten()\n",
    "    ext_flat = ext_centroids_matched.flatten()\n",
    "    overall_centroid_corr, overall_centroid_p = pearsonr(d1_flat, ext_flat)\n",
    "    \n",
    "    print(f\"    Centroid correlation (forced K={d1_k}): {overall_centroid_corr:.4f} (p={overall_centroid_p:.6f})\")\n",
    "    print(f\"    ⚠ Note: Forced K matching may mask true differences\")\n",
    "    \n",
    "    h1_supported_secondary = overall_centroid_corr > REPLICATION_THRESHOLD_H1\n",
    "    h2_supported_secondary = overall_centroid_corr < REPLICATION_THRESHOLD_H2\n",
    "    \n",
    "    # Store results\n",
    "    replication_results.append({\n",
    "        'dataset': ext_dataset,\n",
    "        'd1_k': d1_k,\n",
    "        'ext_k': ext_result['best_k'],  # Original K, not forced\n",
    "        'k_different': k_different,  # Preliminary evidence: Different K suggests H2\n",
    "        'primary_correlation': primary_correlation if primary_correlation is not None else None,\n",
    "        'primary_p_value': primary_p_value if primary_p_value is not None else None,\n",
    "        'avg_feature_correlation': avg_feature_corr if d1_profile_summary and ext_profile_summary else None,\n",
    "        'h1_supported_primary': h1_supported_primary if primary_correlation is not None else False,\n",
    "        'h2_supported_primary': h2_supported_primary if primary_correlation is not None else False,\n",
    "        'secondary_correlation': overall_centroid_corr,  # Robustness check\n",
    "        'secondary_p_value': overall_centroid_p,\n",
    "        'avg_dim_correlation': avg_correlation,\n",
    "        'h1_supported_secondary': h1_supported_secondary,\n",
    "        'h2_supported_secondary': h2_supported_secondary,\n",
    "        'info_loss_pct': ext_result.get('info_loss_pct', None),\n",
    "    })\n",
    "    \n",
    "    # ====================================================================\n",
    "    # VISUALIZATION\n",
    "    # ====================================================================\n",
    "    if d1_latent_dim == 2:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # D1-Swiss centroids\n",
    "        axes[0].scatter(\n",
    "            d1_centroids[:, 0],\n",
    "            d1_centroids[:, 1],\n",
    "            c=range(d1_k),\n",
    "            cmap='viridis',\n",
    "            s=200,\n",
    "            edgecolors='black',\n",
    "            linewidth=2,\n",
    "            marker='o',\n",
    "            label='D1-Swiss'\n",
    "        )\n",
    "        \n",
    "        for i, (x, y) in enumerate(d1_centroids):\n",
    "            axes[0].annotate(\n",
    "                f'P{i+1}',\n",
    "                (x, y),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontweight='bold'\n",
    "            )\n",
    "        \n",
    "        axes[0].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "        axes[0].set_ylabel('Latent Dim 2', fontsize=12)\n",
    "        axes[0].set_title('D1-Swiss Centroids (Reference)', fontsize=14, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # External dataset centroids\n",
    "        axes[1].scatter(\n",
    "            ext_centroids_matched[:, 0],\n",
    "            ext_centroids_matched[:, 1],\n",
    "            c=range(ext_k),\n",
    "            cmap='viridis',\n",
    "            s=200,\n",
    "            edgecolors='red',\n",
    "            linewidth=2,\n",
    "            marker='s',\n",
    "            label=ext_dataset\n",
    "        )\n",
    "        \n",
    "        for i, (x, y) in enumerate(ext_centroids_matched):\n",
    "            axes[1].annotate(\n",
    "                f'P{i+1}',\n",
    "                (x, y),\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points',\n",
    "                fontweight='bold'\n",
    "            )\n",
    "        \n",
    "        axes[1].set_xlabel('Latent Dim 1', fontsize=12)\n",
    "        axes[1].set_ylabel('Latent Dim 2', fontsize=12)\n",
    "        axes[1].set_title(\n",
    "            f'{ext_dataset} Centroids (Matched)\\nr = {overall_centroid_corr:.4f}',\n",
    "            fontsize=14,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    elif d1_latent_dim == 3:\n",
    "        fig = plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        ax1 = fig.add_subplot(121, projection='3d')\n",
    "        ax1.scatter(\n",
    "            d1_centroids[:, 0],\n",
    "            d1_centroids[:, 1],\n",
    "            d1_centroids[:, 2],\n",
    "            c=range(d1_k),\n",
    "            cmap='viridis',\n",
    "            s=200,\n",
    "            edgecolors='black',\n",
    "            linewidth=2\n",
    "        )\n",
    "        ax1.set_xlabel('Latent Dim 1')\n",
    "        ax1.set_ylabel('Latent Dim 2')\n",
    "        ax1.set_zlabel('Latent Dim 3')\n",
    "        ax1.set_title('D1-Swiss Centroids (Reference)')\n",
    "        \n",
    "        ax2 = fig.add_subplot(122, projection='3d')\n",
    "        ax2.scatter(\n",
    "            ext_centroids_matched[:, 0],\n",
    "            ext_centroids_matched[:, 1],\n",
    "            ext_centroids_matched[:, 2],\n",
    "            c=range(ext_k),\n",
    "            cmap='viridis',\n",
    "            s=200,\n",
    "            edgecolors='red',\n",
    "            linewidth=2\n",
    "        )\n",
    "        ax2.set_xlabel('Latent Dim 1')\n",
    "        ax2.set_ylabel('Latent Dim 2')\n",
    "        ax2.set_zlabel('Latent Dim 3')\n",
    "        ax2.set_title(f'{ext_dataset} Centroids\\nr = {overall_centroid_corr:.4f}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ========================================================================\n",
    "# SUMMARY AND CONCLUSIONS\n",
    "# ========================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"REPLICATION TESTING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nMETHODOLOGICAL APPROACH:\")\n",
    "print(\"  PRIMARY: Symptom pattern comparison (natural K)\")\n",
    "print(\"    - Preserves each dataset's optimal K (natural structure)\")\n",
    "print(\"    - Compares actual symptom patterns (Depression, Anxiety, Stress, Burnout)\")\n",
    "print(\"    - Uses Hungarian algorithm to match best subset when K differs\")\n",
    "print(\"    - Tests: 'Are symptom patterns similar across datasets?'\")\n",
    "print(\"  SECONDARY: Forced K matching (robustness check)\")\n",
    "print(\"    - Re-clusters external datasets to match D1's K\")\n",
    "print(\"    - Standardized comparison with same structure\")\n",
    "print(\"    - Compares centroids in latent space\")\n",
    "print(\"    - Tests: 'Can we find matching profiles if we force same structure?'\")\n",
    "print(\"\\nINTERPRETATION NOTES:\")\n",
    "print(\"  - Different optimal K values are interpreted as evidence for H2\")\n",
    "print(\"    (context-specificity), but this is integrated into interpretation,\")\n",
    "print(\"    not a separate comparison method\")\n",
    "print(\"  - Correlation computed on profiles (n=K, typically 2-6)\")\n",
    "print(\"  - P-values should be interpreted with caution due to small sample size\")\n",
    "print(\"  - Correlation coefficient (r) is the primary metric for hypothesis testing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "replication_df = pd.DataFrame(replication_results)\n",
    "\n",
    "# Display K comparison (preliminary evidence)\n",
    "display_cols_k = [\n",
    "    'dataset',\n",
    "    'd1_k',\n",
    "    'ext_k',\n",
    "    'k_different'\n",
    "]\n",
    "\n",
    "print(\"\\nPRELIMINARY: Optimal K Comparison (Informs Interpretation)\")\n",
    "print(\"=\"*80)\n",
    "k_display = replication_df[display_cols_k].copy()\n",
    "k_display.columns = ['Dataset', 'D1 K', 'Ext K', 'K Different (H2 Evidence)']\n",
    "k_display['K Different (H2 Evidence)'] = k_display['K Different (H2 Evidence)'].map({True: '✓ YES (H2)', False: '✗ NO (Neutral)'})\n",
    "print(k_display.to_string(index=False))\n",
    "\n",
    "# Display PRIMARY results (symptom patterns)\n",
    "display_cols_primary = [\n",
    "    'dataset',\n",
    "    'primary_correlation',\n",
    "    'avg_feature_correlation',\n",
    "    'h1_supported_primary',\n",
    "    'h2_supported_primary'\n",
    "]\n",
    "\n",
    "print(\"\\n\\nPRIMARY: Symptom Pattern Comparison (Natural K)\")\n",
    "print(\"=\"*80)\n",
    "primary_display = replication_df[display_cols_primary].copy()\n",
    "primary_display.columns = ['Dataset', 'Symptom Pattern r', 'Avg Feature r', 'H1 Supported', 'H2 Supported']\n",
    "print(primary_display.to_string(index=False))\n",
    "\n",
    "# Display SECONDARY results (forced K)\n",
    "display_cols_secondary = [\n",
    "    'dataset',\n",
    "    'secondary_correlation',\n",
    "    'avg_dim_correlation',\n",
    "    'h1_supported_secondary',\n",
    "    'h2_supported_secondary'\n",
    "]\n",
    "\n",
    "print(\"\\n\\nSECONDARY: Forced K Matching (Robustness Check)\")\n",
    "print(\"=\"*80)\n",
    "secondary_display = replication_df[display_cols_secondary].copy()\n",
    "secondary_display.columns = ['Dataset', 'Centroid r', 'Avg Dim r', 'H1 Supported', 'H2 Supported']\n",
    "print(secondary_display.to_string(index=False))\n",
    "\n",
    "# Count evidence\n",
    "k_different_count = sum(replication_df['k_different'] == True)  # Preliminary evidence\n",
    "h1_count_primary = sum(replication_df['h1_supported_primary'] == True)  # PRIMARY\n",
    "h2_count_primary = sum(replication_df['h2_supported_primary'] == True)  # PRIMARY\n",
    "h1_count_secondary = sum(replication_df['h1_supported_secondary'] == True)  # SECONDARY\n",
    "h2_count_secondary = sum(replication_df['h2_supported_secondary'] == True)  # SECONDARY\n",
    "total_tested = len(replication_df)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OVERALL CONCLUSION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Reference: D1-Swiss ({d1_latent_dim}D, K={d1_k})\")\n",
    "print(f\"All datasets standardized to: {d1_latent_dim}D latent space\")\n",
    "print(f\"Datasets tested: {total_tested}\")\n",
    "print(f\"\\nPRELIMINARY (Optimal K Comparison):\")\n",
    "print(f\"  Different K (H2 evidence): {k_different_count}/{total_tested} datasets\")\n",
    "print(f\"  Same K (Neutral): {total_tested - k_different_count}/{total_tested} datasets\")\n",
    "print(f\"\\nPRIMARY (Symptom Pattern Comparison - Natural K):\")\n",
    "print(f\"  H1 (Universal) supported: {h1_count_primary}/{total_tested}\")\n",
    "print(f\"  H2 (Context-Specific) supported: {h2_count_primary}/{total_tested}\")\n",
    "print(f\"\\nSECONDARY (Forced K Matching - Robustness Check):\")\n",
    "print(f\"  H1 (Universal) supported: {h1_count_secondary}/{total_tested}\")\n",
    "print(f\"  H2 (Context-Specific) supported: {h2_count_secondary}/{total_tested}\")\n",
    "\n",
    "# Final conclusion based on PRIMARY method (symptom pattern comparison)\n",
    "# K difference is integrated as preliminary evidence\n",
    "if k_different_count >= 2:\n",
    "    # Different K is strong preliminary evidence for H2\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✓ H2 VALIDATED: Profiles are CONTEXT-SPECIFIC\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  PRELIMINARY: {k_different_count}/{total_tested} datasets have different optimal K\")\n",
    "    print(f\"    → Strong evidence that profiles are context-specific\")\n",
    "    print(f\"    → Different populations have different profile structures\")\n",
    "    if h2_count_primary >= 2:\n",
    "        print(f\"  PRIMARY: {h2_count_primary}/{total_tested} datasets show low symptom pattern correlation\")\n",
    "        print(f\"    → Confirms context-specificity (both structure and patterns differ)\")\n",
    "    elif h1_count_primary >= 1:\n",
    "        print(f\"  PRIMARY: Some datasets show high symptom pattern correlation despite different K\")\n",
    "        print(f\"    → Note: Different K is strong evidence for context-specificity\")\n",
    "    print(f\"  → Conclusion: Mental health profiles vary by population/culture/context\")\n",
    "elif k_different_count == 1:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"→ MIXED EVIDENCE: Partial support for context-specificity\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  PRELIMINARY: 1/{total_tested} dataset has different optimal K\")\n",
    "    print(f\"    → Suggests some context-specificity, but not universal\")\n",
    "    if h2_count_primary >= 2:\n",
    "        print(f\"  PRIMARY: {h2_count_primary}/{total_tested} datasets show low symptom pattern correlation\")\n",
    "        print(f\"    → Additional evidence for context-specificity\")\n",
    "    print(f\"  → Conclusion: Profiles may be context-specific in some populations but not others\")\n",
    "else:\n",
    "    # All have same K, so PRIMARY (symptom correlation) is deciding factor\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    if h1_count_primary >= 2:\n",
    "        print(\"✓ H1 VALIDATED: Profiles appear to be UNIVERSAL\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  PRELIMINARY: All datasets have same optimal K ({d1_k}) - Neutral\")\n",
    "        print(f\"  PRIMARY: {h1_count_primary}/{total_tested} datasets show high symptom pattern correlation\")\n",
    "        print(f\"    → Conclusion: Mental health profiles are consistent across populations\")\n",
    "        print(f\"    → Similar mean levels of Depression, Anxiety, Stress, Burnout across datasets\")\n",
    "    elif h2_count_primary >= 2:\n",
    "        print(\"✓ H2 VALIDATED: Profiles appear to be CONTEXT-SPECIFIC\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  PRELIMINARY: All datasets have same optimal K ({d1_k}) - Neutral\")\n",
    "        print(f\"  PRIMARY: {h2_count_primary}/{total_tested} datasets show low symptom pattern correlation\")\n",
    "        print(f\"    → Conclusion: Same structure but different symptom patterns\")\n",
    "        print(f\"    → Profiles vary by population/culture/context\")\n",
    "    else:\n",
    "        print(\"→ MIXED RESULTS: Moderate replication\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"  PRELIMINARY: All datasets have same optimal K ({d1_k}) - Neutral\")\n",
    "        print(f\"  PRIMARY: Symptom patterns show moderate correlation (0.50 ≤ r ≤ 0.70)\")\n",
    "        print(f\"    → May depend on specific dataset characteristics\")\n",
    "        print(f\"    → Consider dataset-specific factors (culture, demographics, measurement tools)\")\n",
    "\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# OPTIONAL: Process All Datasets Automatically\n",
    "# =========================================================================\n",
    "# Uncomment the function call below to process all datasets through\n",
    "# the autoencoder pipeline automatically\n",
    "# =========================================================================\n",
    "\n",
    "def run_all_datasets():\n",
    "    \"\"\"\n",
    "    Process all datasets through the autoencoder pipeline.\n",
    "    \n",
    "    This function iterates through all datasets defined in DATASETS\n",
    "    and runs the complete pipeline (hyperparameter tuning + profile extraction)\n",
    "    for each one.\n",
    "    \"\"\"\n",
    "    for dataset_name in DATASETS:\n",
    "        print(f\"\\n{'#'*80}\\nProcessing {dataset_name}\\n{'#'*80}\\n\")\n",
    "        run_autoencoder_pipeline(dataset_name)\n",
    "\n",
    "# Uncomment the line below to run:\n",
    "# run_all_datasets()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h1/h2 = replication testing\n",
    "#Purpose : Identify which are universal vs context-specific\n",
    "#Two pronged approach : Primary(Natural K) vs Secondary(Forced K)\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ALL_DATASETS = [\"D1-Swiss\", \"D2-Cultural\", \"D3-Academic\", \"D4-Tech\"]\n",
    "REPLICATION_THRESHOLD_H1 = 0.70  # r > 0.70 = Universal\n",
    "REPLICATION_THRESHOLD_H2 = 0.50  # r < 0.50 = Context-specific\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PAIRWISE REPLICATION TESTING\")\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing all datasets to each other to identify universal vs context-specific profiles\")\n",
    "print(\"Two-pronged approach: Natural K (PRIMARY) + Forced K (SECONDARY)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#Process the datasets\n",
    "print(\"\\nProcessing datasets...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset_name in ALL_DATASETS:\n",
    "    if dataset_name not in PIPELINE_RESULTS:\n",
    "        print(f\"  Processing {dataset_name}...\")\n",
    "        run_autoencoder_pipeline(dataset_name)\n",
    "    else:\n",
    "        print(f\"  {dataset_name} already processed ✓\")\n",
    "\n",
    "#Standardize to common latent dim\n",
    "print(\"\\nStandardizing to common latent dimension...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "#Get all unique latent dimensions from results\n",
    "#use swiss dataset as reference\n",
    "swiss_latent_dim = PIPELINE_RESULTS[\"D1-Swiss\"][\"best_latent_dim\"]\n",
    "print(f\"Swiss dataset has latent dimension: {swiss_latent_dim}\")\n",
    "\n",
    "standardized_results = {}\n",
    "\n",
    "for dataset_name in ALL_DATASETS:\n",
    "    result = PIPELINE_RESULTS[dataset_name]\n",
    "\n",
    "    if result['best_latent_dim'] != swiss_latent_dim:\n",
    "        print(f\"Standardizing {dataset_name} to {swiss_latent_dim}D...\")\n",
    "        result = run_autoencoder_pipeline(dataset_name, force_latent_dim=swiss_latent_dim)\n",
    "    standardized_results[dataset_name] = result\n",
    "    print(f\"  ✓ {dataset_name}: K={result['best_k']}, latent_dim={result['best_latent_dim']}D\")\n",
    "\n",
    "#Pairwise comparison for all the pairs\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" Pairwise Profile Comparisons (All Unique Pairs)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing symptom patterns (Depression, Anxiety, Stress, Burnout)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def compare_profiles(dataset1_name, dataset2_name, results_dict, force_k=None):\n",
    "    \"\"\"\n",
    "    Compare profiles between two datasets using symptom patterns\n",
    "    \n",
    "    Args:\n",
    "        dataset1_name (str): Name of the first dataset\n",
    "        dataset2_name (str): Name of the second dataset\n",
    "        results_dict (dict): Dictionary containing all dataset results\n",
    "        force_k (int, optional): If provided, re-cluster both datasets to this K\n",
    "                                 If None, uses natural K from each dataset\n",
    "    \"\"\"\n",
    "    result1 = results_dict[dataset1_name]\n",
    "    result2 = results_dict[dataset2_name]\n",
    "\n",
    "    #If force k - re-cluster both datasets\n",
    "    if force_k is not None:\n",
    "        from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "         # Re-cluster dataset1\n",
    "        latent1 = result1['latent_vectors_all']\n",
    "        train_val_data1 = result1['train_val_data']\n",
    "        kmeans1 = KMeans(n_clusters=force_k, random_state=RANDOM_SEED, n_init=10)\n",
    "        labels1 = kmeans1.fit_predict(latent1)\n",
    "        \n",
    "        # Re-cluster dataset2\n",
    "        latent2 = result2['latent_vectors_all']\n",
    "        train_val_data2 = result2['train_val_data']\n",
    "        kmeans2 = KMeans(n_clusters=force_k, random_state=RANDOM_SEED, n_init=10)\n",
    "        labels2 = kmeans2.fit_predict(latent2)\n",
    "\n",
    "        #Recompute the profile summaries with the new Forced K\n",
    "\n",
    "        profile1 = []\n",
    "        profile2 = []\n",
    "\n",
    "        for k in range(force_k):\n",
    "            # Dataset 1\n",
    "            mask1 = labels1 == k\n",
    "            if np.sum(mask1) > 0:\n",
    "                cluster1 = train_val_data1[mask1]\n",
    "                profile1.append({\n",
    "                    'Depression': cluster1[:, 0].mean(),\n",
    "                    'Anxiety': cluster1[:, 1].mean(),\n",
    "                    'Stress': cluster1[:, 2].mean(),\n",
    "                    'Burnout': cluster1[:, 3].mean()\n",
    "                })\n",
    "            \n",
    "            # Dataset 2\n",
    "            mask2 = labels2 == k\n",
    "            if np.sum(mask2) > 0:\n",
    "                cluster2 = train_val_data2[mask2]\n",
    "                profile2.append({\n",
    "                    'Depression': cluster2[:, 0].mean(),\n",
    "                    'Anxiety': cluster2[:, 1].mean(),\n",
    "                    'Stress': cluster2[:, 2].mean(),\n",
    "                    'Burnout': cluster2[:, 3].mean()\n",
    "                })\n",
    "    else:\n",
    "        # Use natural K (original approach)\n",
    "        profile1 = result1.get('profile_summary', [])\n",
    "        profile2 = result2.get('profile_summary', [])\n",
    "        \n",
    "    if not profile1 or not profile2:\n",
    "        return None, None, False, False\n",
    "    \n",
    "    symptom_matrix1 = np.array([\n",
    "        [p['Depression'], p['Anxiety'], p['Stress'], p['Burnout']]\n",
    "        for p in profile1\n",
    "    ])\n",
    "\n",
    "    symptom_matrix2 = np.array([\n",
    "        [p['Depression'], p['Anxiety'], p['Stress'], p['Burnout']]\n",
    "        for p in profile2\n",
    "    ])\n",
    "\n",
    "    #match the profiles using the job assignment algorithm(hungarian algorithm)\n",
    "\n",
    "    distance_matrix = cdist(symptom_matrix1, symptom_matrix2, metric='euclidean')\n",
    "    row_ind, col_ind = linear_sum_assignment(distance_matrix)\n",
    "    symptom_matrix2_matched = symptom_matrix2[col_ind]\n",
    "\n",
    "    flat1 = symptom_matrix1.flatten()\n",
    "    flat2 = symptom_matrix2_matched.flatten()\n",
    "\n",
    "    #Handle different k (if not forced k)\n",
    "\n",
    "    if force_k is None:\n",
    "        min_len = min(len(flat1), len(flat2))\n",
    "        if min_len < 4:  # Need at least 1 profile (4 symptoms)\n",
    "            return None, None, False, False\n",
    "        correlation, p_value = pearsonr(flat1[:min_len], flat2[:min_len])\n",
    "    else:\n",
    "        # Same K, can use full arrays\n",
    "        if len(flat1) != len(flat2):\n",
    "            return None, None, False, False\n",
    "        correlation, p_value = pearsonr(flat1, flat2)\n",
    "    # Test hypotheses\n",
    "    h1_supported = correlation > REPLICATION_THRESHOLD_H1\n",
    "    h2_supported = correlation < REPLICATION_THRESHOLD_H2\n",
    "\n",
    "    return correlation, p_value, h1_supported, h2_supported\n",
    "\n",
    "pairwise_results = []\n",
    "for i in range(len(ALL_DATASETS)):\n",
    "    for j in range(i+1, len(ALL_DATASETS)):\n",
    "        dataset1_name = ALL_DATASETS[i]\n",
    "        dataset2_name = ALL_DATASETS[j]\n",
    "        print(f\"\\nComparing {dataset1_name} and {dataset2_name}...\")\n",
    "\n",
    "        result1 = standardized_results[dataset1_name]\n",
    "        result2 = standardized_results[dataset2_name]\n",
    "\n",
    "        k1 = result1['best_k']\n",
    "        k2 = result2['best_k']\n",
    "        k_different = k1 != k2    \n",
    "\n",
    "        correlation_natural, p_value_natural, h1_supported_natural, h2_supported_natural = compare_profiles(\n",
    "            dataset1_name, dataset2_name, standardized_results, force_k=None\n",
    "        )\n",
    "\n",
    "        #Secondary - forced k\n",
    "\n",
    "        reference_k = standardized_results[\"D1-Swiss\"][\"best_k\"]\n",
    "        if k_different:\n",
    "            correlation_forced, p_forced, h1_forced, h2_forced = compare_profiles(\n",
    "                dataset1_name, dataset2_name, standardized_results, force_k=reference_k\n",
    "            )\n",
    "        else:\n",
    "            # Same K, forced = natural\n",
    "            correlation_forced = correlation_natural\n",
    "            p_forced = p_value_natural\n",
    "            h1_forced = h1_supported_natural\n",
    "            h2_forced = h2_supported_natural\n",
    "\n",
    "        print(f\"    K: {k1} vs {k2} ({'Different' if k_different else 'Same'})\")\n",
    "\n",
    "        if correlation_natural is not None:\n",
    "            print(f\"    PRIMARY (Natural K):\")\n",
    "            print(f\"      Correlation: r={correlation_natural:.4f} (p={p_value_natural:.6f})\")\n",
    "            print(f\"      H1 (Universal): {'✔️' if h1_supported_natural else '✗'}\")\n",
    "            print(f\"      H2 (Context-specific): {'✔️' if h2_supported_natural else '✗'}\")            \n",
    "\n",
    "        else:\n",
    "            print(f\"    PRIMARY (Natural K): No valid profiles found\")\n",
    "        \n",
    "\n",
    "        if correlation_forced is not None:\n",
    "            print(f\"Secondary (Forced K):\")\n",
    "            print(f\"      Correlation: r={correlation_forced:.4f} (p={p_forced:.6f})\")\n",
    "            print(f\"      H1 (Universal): {'✔️' if h1_forced else '✗'}\")\n",
    "            print(f\"      H2 (Context-specific): {'✔️' if h2_forced else '✗'}\")\n",
    "        else:\n",
    "            print(f\"Secondary (Forced K): No valid profiles found\")\n",
    "        \n",
    "        pairwise_results.append({\n",
    "            'dataset1': dataset1_name,\n",
    "            'dataset2': dataset2_name,\n",
    "            'k1': k1,\n",
    "            'k2': k2,\n",
    "            'k_different': k_different,\n",
    "            'correlation_natural': correlation_natural,\n",
    "            'correlation_forced': correlation_forced,\n",
    "            'p_value_natural': p_value_natural,\n",
    "            'p_value_forced': p_forced,\n",
    "            'h1_supported_natural': h1_supported_natural if correlation_natural is not None else False,\n",
    "            'h2_supported_natural': h2_supported_natural if correlation_natural is not None else False,\n",
    "            'h1_supported_forced': h1_forced if correlation_forced is not None else False,\n",
    "            'h2_supported_forced': h2_forced if correlation_forced is not None else False,\n",
    "            'correlation': correlation_natural,\n",
    "            'p_value': p_value_natural,\n",
    "            'h1_supported': h1_supported_natural if correlation_natural is not None else False,\n",
    "            'h2_supported': h2_supported_natural if correlation_natural is not None else False,\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    #Create correlation matrix and analysis\n",
    "\n",
    "print(\"\\nCreating correlation matrix and analysis...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "pairwise_df = pd.DataFrame(pairwise_results)\n",
    "\n",
    "correlation_matrix_natural = np.ones((len(ALL_DATASETS), len(ALL_DATASETS))) * np.nan\n",
    "correlation_matrix_forced = np.ones((len(ALL_DATASETS), len(ALL_DATASETS))) * np.nan\n",
    "\n",
    "for index, row in pairwise_df.iterrows():\n",
    "    i = ALL_DATASETS.index(row['dataset1'])\n",
    "    j = ALL_DATASETS.index(row['dataset2'])\n",
    "    correlation_natural = row['correlation_natural'] if row['correlation_natural'] is not None else np.nan\n",
    "    correlation_forced = row['correlation_forced'] if row['correlation_forced'] is not None else np.nan\n",
    "\n",
    "    correlation_matrix_natural[i, j] = correlation_natural\n",
    "    correlation_matrix_forced[i, j] = correlation_forced\n",
    "\n",
    "    correlation_matrix_natural[j, i] = correlation_natural\n",
    "    correlation_matrix_forced[j, i] = correlation_forced\n",
    "\n",
    "correlation_df_natural = pd.DataFrame(\n",
    "    correlation_matrix_natural,\n",
    "    index=ALL_DATASETS,\n",
    "    columns=ALL_DATASETS\n",
    ")\n",
    "\n",
    "correlation_df_forced = pd.DataFrame(\n",
    "    correlation_matrix_forced,\n",
    "    index=ALL_DATASETS,\n",
    "    columns=ALL_DATASETS\n",
    ")\n",
    "\n",
    "print(\"\\nPRIMARY: Natural K Correlation Matrix (Preserves Original Structure):\")\n",
    "print(correlation_df_natural.to_string())\n",
    "\n",
    "print(\"\\nSECONDARY: Forced K Correlation Matrix (K=\" + str(standardized_results[\"D1-Swiss\"]['best_k']) + \", D1-Swiss Reference):\")\n",
    "print(correlation_df_forced.to_string())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Natural K\n",
    "sns.heatmap(correlation_df_natural, annot=True, fmt='.3f', cmap='RdYlGn', center=0.6,\n",
    "            vmin=0, vmax=1, square=True, linewidths=0.5, cbar_kws={'label': 'Correlation'}, ax=axes[0])\n",
    "axes[0].set_title('PRIMARY: Natural K (Preserves Original Structure)\\n(Higher = More Similar Profiles)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "\n",
    "# Forced K\n",
    "sns.heatmap(correlation_df_forced, annot=True, fmt='.3f', cmap='RdYlGn', center=0.6,\n",
    "            vmin=0, vmax=1, square=True, linewidths=0.5, cbar_kws={'label': 'Correlation'}, ax=axes[1])\n",
    "axes[1].set_title('SECONDARY: Forced K=' + str(standardized_results[\"D1-Swiss\"]['best_k']) + ' (D1-Swiss Reference)\\n(Higher = More Similar Profiles)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Identify universal and context-specific profiles\n",
    "print(\"\\nIdentifying Universal and Context-Specific Profiles:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "dataset_average_correlations = {}\n",
    "\n",
    "for dataset in ALL_DATASETS:\n",
    "    correlations = []\n",
    "    for other_dataset in ALL_DATASETS:\n",
    "        if dataset != other_dataset:\n",
    "            match = pairwise_df[\n",
    "                ((pairwise_df['dataset1'] == dataset) & (pairwise_df['dataset2'] == other_dataset)) |\n",
    "                ((pairwise_df['dataset1'] == other_dataset) & (pairwise_df['dataset2'] == dataset))\n",
    "            ]\n",
    "            if len(match) > 0 and match.iloc[0]['correlation_natural'] is not None:\n",
    "                correlations.append(match.iloc[0]['correlation_natural'])\n",
    "    \n",
    "    if correlations:\n",
    "        avg_correlation = np.mean(correlations)\n",
    "        dataset_average_correlations[dataset] = avg_correlation\n",
    "\n",
    "        if avg_correlation > REPLICATION_THRESHOLD_H1:\n",
    "            classification = \"Universal (high similarity to all others)\"\n",
    "        elif avg_correlation < REPLICATION_THRESHOLD_H2:\n",
    "            classification = \"Context-specific (low similarity to all others)\"\n",
    "        else:\n",
    "            classification = \"Mixed (intermediate similarity)\"\n",
    "        \n",
    "        print(f\"\\n{dataset}: {classification}\")\n",
    "        print(f\"  Average Correlation: {avg_correlation:.4f}\")\n",
    "        print(f\"  Correlations with others: {[f'{c:.3f}' for c in correlations]}\")\n",
    "\n",
    "#Final Print \n",
    "print(\"\\nFinal Classification:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "universal_datasets = [d for d, corr in dataset_average_correlations.items() \n",
    "                      if corr > REPLICATION_THRESHOLD_H1]\n",
    "context_specific_datasets = [d for d, corr in dataset_average_correlations.items() \n",
    "                            if corr < REPLICATION_THRESHOLD_H2]\n",
    "mixed_datasets = [d for d, corr in dataset_average_correlations.items() \n",
    "                 if REPLICATION_THRESHOLD_H2 <= corr <= REPLICATION_THRESHOLD_H1]\n",
    "\n",
    "print(f\"\\nUniversal Profiles (r > {REPLICATION_THRESHOLD_H1}): {universal_datasets}\")\n",
    "print(f\"Context-Specific Profiles (r < {REPLICATION_THRESHOLD_H2}): {context_specific_datasets}\")\n",
    "print(f\"Mixed Profiles ({REPLICATION_THRESHOLD_H2} ≤ r ≤ {REPLICATION_THRESHOLD_H1}): {mixed_datasets}\")\n",
    "\n",
    "h1_pairs_natural = sum(pairwise_df['h1_supported_natural'])\n",
    "h2_pairs_natural = sum(pairwise_df['h2_supported_natural'])\n",
    "h1_pairs_forced = sum(pairwise_df['h1_supported_forced'])\n",
    "h2_pairs_forced = sum(pairwise_df['h2_supported_forced'])\n",
    "total_pairs = len(pairwise_df)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PRIMARY: Natural K Comparisons (Preserves Original Structure)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Total pairs: {total_pairs}\")\n",
    "print(f\"  H1 (Universal) supported: {h1_pairs_natural}/{total_pairs} pairs\")\n",
    "print(f\"  H2 (Context-specific) supported: {h2_pairs_natural}/{total_pairs} pairs\")\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SECONDARY: Forced K Comparisons (K=\" + str(standardized_results[\"D1-Swiss\"]['best_k']) + \", D1-Swiss Reference)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Total pairs: {total_pairs}\")\n",
    "print(f\"  H1 (Universal) supported: {h1_pairs_forced}/{total_pairs} pairs\")\n",
    "print(f\"  H2 (Context-specific) supported: {h2_pairs_forced}/{total_pairs} pairs\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OVERALL CONCLUSION (Based on PRIMARY: Natural K)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "\n",
    "if h1_pairs_natural >= total_pairs * 0.5:\n",
    "    print(f\"\\n→ H1 VALIDATED: Profiles appear UNIVERSAL across most datasets\")\n",
    "    print(f\"  → {len(universal_datasets)}/{len(ALL_DATASETS)} datasets show high average correlation\")\n",
    "    print(f\"  → Mental health profiles are consistent across populations\")\n",
    "elif h2_pairs_natural >= total_pairs * 0.5:\n",
    "    print(f\"\\n→ H2 VALIDATED: Profiles appear CONTEXT-SPECIFIC across most datasets\")\n",
    "    print(f\"  → {len(context_specific_datasets)}/{len(ALL_DATASETS)} datasets show low average correlation\")\n",
    "    print(f\"  → Mental health profiles vary by population/culture/context\")\n",
    "else:\n",
    "    print(f\"\\n→ MIXED EVIDENCE: Profiles show partial replication\")\n",
    "    print(f\"  → Some datasets universal, some context-specific\")\n",
    "    print(f\"  → May depend on specific dataset characteristics\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520441ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
